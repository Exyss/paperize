Contents
Foreword 1
Part I Mathematical Foundations 9
1 Introdu
ii Contents
4.2 Eigenvalues and Eigenvectors 105
4.3 Cholesky Decomposition 114
4.4 Eigendecomposition and Diagonalization 115
4.5 Singular Value Decomposition 119
4.6 Matrix Approximation 129
4.7 Matrix Phylogeny 134
4.8 Further Reading 135
Exercises 137
5 Vector Calculus 139
5.1 Differentiation of Univariate Functions 141
5.2 Partial Differentiation and Gradients 146
5.3 Gradients of Vector-Valued Functions 149
5.4 Gradients of Matrices 155
5.5 Useful Identities for Computing Gradients 158
5.6 Backpropagation and Automatic Differentiation 159
5.7 Higher-Order Derivatives 164
5.8 Linearization and Multivariate Taylor Series 165
5.9 Further Reading 170
Exercises 170
6 Probability and Distributions 172
6.1 Construction of a Probability Space 172
6.2 Discrete and Continuous Probabilities 178
6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183
6.4 Summary Statistics and Independence 186
6.5 Gaussian Distribution 197
6.6 Conjugacy and the Exponential Family 205
6.7 Change of Variables/Inverse Transform 214
6.8 Further Reading 221
Exercises 221
7 Continuous Optimization 225
7.1 Optimization Using Gradient Descent 227
7.2 Constrained Optimization and Lagrange Multipliers 233
7.3 Convex Optimization 236
7.4 Further Reading 246
Exercises 247
Part II Central Machine Learning Problems 249
8 When Models Meet Data 251
8.1 Data, Models, and Learning 251
8.2 Empirical Risk Minimization 258
8.3 Parameter Estimation 265
8.4 Probabilistic Modeling and Inference 272
8.5 Directed Graphical Models 278
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Contents iii
8.6 Model Selection 283
9 Linear Regression 289
9.1 Problem Formulation 291
9.2 Parameter Estimation 292
9.3 Bayesian Linear Regression 303
9.4 Maximum Likelihood as Orthogonal Projection 313
9.5 Further Reading 315
10 Dimensionality Reduction with Principal Component Analysis 317
10.1 Problem Setting 318
10.2 Maximum Variance Perspective 320
10.3 Projection Perspective 325
10.4 Eigenvector Computation and Low-Rank Approximations 333
10.5 PCA in High Dimensions 335
10.6 Key Steps of PCA in Practice 336
10.7 Latent Variable Perspective 339
10.8 Further Reading 343
11 Density Estimation with Gaussian Mixture Models 348
11.1 Gaussian Mixture Model 349
11.2 Parameter Learning via Maximum Likelihood 350
11.3 EM Algorithm 360
11.4 Latent-Variable Perspective 363
11.5 Further Reading 368
12 Classification with Support Vector Machines 370
12.1 Separating Hyperplanes 372
12.2 Primal Support Vector Machine 374
12.3 Dual Support Vector Machine 383
12.4 Kernels 388
12.5 Numerical Solution 390
12.6 Further Reading 392
References 395
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).

Foreword
Machine learning is the latest in a long line of attempts to distill human
knowledge and reasoning into a form that is suitable for constructing machines and engineering automated systems. As machine learning becomes
more ubiquitous and its software packages become easier to use, it is natural and desirable that the low-level technical details are abstracted away
and hidden from the practitioner. However, this brings with it the danger
that a practitioner becomes unaware of the design decisions and, hence,
the limits of machine learning algorithms.
The enthusiastic practitioner who is interested to learn more about the
magic behind successful machine learning algorithms currently faces a
daunting set of pre-requisite knowledge:
Programming languages and data analysis tools
Large-scale computation and the associated frameworks
Mathematics and statistics and how machine learning builds on it
At universities, introductory courses on machine learning tend to spend
early parts of the course covering some of these pre-requisites. For historical reasons, courses in machine learning tend to be taught in the computer
science department, where students are often trained in the first two areas
of knowledge, but not so much in mathematics and statistics.
Current machine learning textbooks primarily focus on machine learning algorithms and methodologies and assume that the reader is competent in mathematics and statistics. Therefore, these books only spend
one or two chapters on background mathematics, either at the beginning
of the book or as appendices. We have found many people who want to
delve into the foundations of basic machine learning methods who struggle with the mathematical knowledge required to read a machine learning
textbook. Having taught undergraduate and graduate courses at universities, we find that the gap between high school mathematics and the mathematics level required to read a standard machine learning textbook is too
big for many people.
This book brings the mathematical foundations of basic machine learning concepts to the fore and collects the information in a single place so
that this skills gap is narrowed or even closed.
1
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com.
2 Foreword
Why Another Book on Machine Learning?
Machine learning builds upon the language of mathematics to express
concepts that seem intuitively obvious but that are surprisingly difficult
to formalize. Once formalized properly, we can gain insights into the task
we want to solve. One common complaint of students of mathematics
around the globe is that the topics covered seem to have little relevance
to practical problems. We believe that machine learning is an obvious and
direct motivation for people to learn mathematics.
This book is intended to be a guidebook to the vast mathematical lit-
“Math is linked in erature that forms the foundations of modern machine learning. We mothe popular mind
with phobia and
anxiety. You’d think
we’re discussing
spiders.” (Strogatz,
2014, page 281)
tivate the need for mathematical concepts by directly pointing out their
usefulness in the context of fundamental machine learning problems. In
the interest of keeping the book short, many details and more advanced
concepts have been left out. Equipped with the basic concepts presented
here, and how they fit into the larger context of machine learning, the
reader can find numerous resources for further study, which we provide at
the end of the respective chapters. For readers with a mathematical background, this book provides a brief but precisely stated glimpse of machine
learning. In contrast to other books that focus on methods and models
of machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Barber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers
and Girolami, 2016) or programmatic aspects of machine learning (Muller ¨
and Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),
we provide only four representative examples of machine learning algorithms. Instead, we focus on the mathematical concepts behind the models
themselves. We hope that readers will be able to gain a deeper understanding of the basic questions in machine learning and connect practical questions arising from the use of machine learning with fundamental choices
in the mathematical model.
We do not aim to write a classical machine learning book. Instead, our
intention is to provide the mathematical background, applied to four central machine learning problems, to make it easier to read other machine
learning textbooks.
Who Is the Target Audience?
As applications of machine learning become widespread in society, we
believe that everybody should have some understanding of its underlying
principles. This book is written in an academic mathematical style, which
enables us to be precise about the concepts behind machine learning. We
encourage readers unfamiliar with this seemingly terse style to persevere
and to keep the goals of each topic in mind. We sprinkle comments and
remarks throughout the text, in the hope that it provides useful guidance
with respect to the big picture.
The book assumes the reader to have mathematical knowledge commonly
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Foreword 3
covered in high school mathematics and physics. For example, the reader
should have seen derivatives and integrals before, and geometric vectors
in two or three dimensions. Starting from there, we generalize these concepts. Therefore, the target audience of the book includes undergraduate
university students, evening learners and learners participating in online
machine learning courses.
In analogy to music, there are three types of interaction that people
have with machine learning:
Astute Listener The democratization of machine learning by the provision of open-source software, online tutorials and cloud-based tools allows users to not worry about the specifics of pipelines. Users can focus on
extracting insights from data using off-the-shelf tools. This enables nontech-savvy domain experts to benefit from machine learning. This is similar to listening to music; the user is able to choose and discern between
different types of machine learning, and benefits from it. More experienced users are like music critics, asking important questions about the
application of machine learning in society such as ethics, fairness, and privacy of the individual. We hope that this book provides a foundation for
thinking about the certification and risk management of machine learning
systems, and allows them to use their domain expertise to build better
machine learning systems.
Experienced Artist Skilled practitioners of machine learning can plug
and play different tools and libraries into an analysis pipeline. The stereotypical practitioner would be a data scientist or engineer who understands
machine learning interfaces and their use cases, and is able to perform
wonderful feats of prediction from data. This is similar to a virtuoso playing music, where highly skilled practitioners can bring existing instruments to life and bring enjoyment to their audience. Using the mathematics presented here as a primer, practitioners would be able to understand the benefits and limits of their favorite method, and to extend and
generalize existing machine learning algorithms. We hope that this book
provides the impetus for more rigorous and principled development of
machine learning methods.
Fledgling Composer As machine learning is applied to new domains,
developers of machine learning need to develop new methods and extend
existing algorithms. They are often researchers who need to understand
the mathematical basis of machine learning and uncover relationships between different tasks. This is similar to composers of music who, within
the rules and structure of musical theory, create new and amazing pieces.
We hope this book provides a high-level overview of other technical books
for people who want to become composers of machine learning. There is
a great need in society for new researchers who are able to propose and
explore novel approaches for attacking the many challenges of learning
from data.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
4 Foreword
Acknowledgments
We are grateful to many people who looked at early drafts of the book
and suffered through painful expositions of concepts. We tried to implement their ideas that we did not vehemently disagree with. We would
like to especially acknowledge Christfried Webers for his careful reading
of many parts of the book, and his detailed suggestions on structure and
presentation. Many friends and colleagues have also been kind enough
to provide their time and energy on different versions of each chapter.
We have been lucky to benefit from the generosity of the online community, who have suggested improvements via https://github.com, which
greatly improved the book.
The following people have found bugs, proposed clarifications and suggested relevant literature, either via https://github.com or personal
communication. Their names are sorted alphabetically.
Abdul-Ganiy Usman
Adam Gaier
Adele Jackson
Aditya Menon
Alasdair Tran
Aleksandar Krnjaic
Alexander Makrigiorgos
Alfredo Canziani
Ali Shafti
Amr Khalifa
Andrew Tanggara
Angus Gruen
Antal A. Buss
Antoine Toisoul Le Cann
Areg Sarvazyan
Artem Artemev
Artyom Stepanov
Bill Kromydas
Bob Williamson
Boon Ping Lim
Chao Qu
Cheng Li
Chris Sherlock
Christopher Gray
Daniel McNamara
Daniel Wood
Darren Siegel
David Johnston
Dawei Chen
Ellen Broad
Fengkuangtian Zhu
Fiona Condon
Georgios Theodorou
He Xin
Irene Raissa Kameni
Jakub Nabaglo
James Hensman
Jamie Liu
Jean Kaddour
Jean-Paul Ebejer
Jerry Qiang
Jitesh Sindhare
John Lloyd
Jonas Ngnawe
Jon Martin
Justin Hsi
Kai Arulkumaran
Kamil Dreczkowski
Lily Wang
Lionel Tondji Ngoupeyou
Lydia Knufing ¨
Mahmoud Aslan
Mark Hartenstein
Mark van der Wilk
Markus Hegland
Martin Hewing
Matthew Alger
Matthew Lee
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Foreword 5
Maximus McCann
Mengyan Zhang
Michael Bennett
Michael Pedersen
Minjeong Shin
Mohammad Malekzadeh
Naveen Kumar
Nico Montali
Oscar Armas
Patrick Henriksen
Patrick Wieschollek
Pattarawat Chormai
Paul Kelly
Petros Christodoulou
Piotr Januszewski
Pranav Subramani
Quyu Kong
Ragib Zaman
Rui Zhang
Ryan-Rhys Griffiths
Salomon Kabongo
Samuel Ogunmola
Sandeep Mavadia
Sarvesh Nikumbh
Sebastian Raschka
Senanayak Sesh Kumar Karri
Seung-Heon Baek
Shahbaz Chaudhary
Shakir Mohamed
Shawn Berry
Sheikh Abdul Raheem Ali
Sheng Xue
Sridhar Thiagarajan
Syed Nouman Hasany
Szymon Brych
Thomas Buhler ¨
Timur Sharapov
Tom Melamed
Vincent Adam
Vincent Dutordoir
Vu Minh
Wasim Aftab
Wen Zhi
Wojciech Stokowiec
Xiaonan Chong
Xiaowei Zhang
Yazhou Hao
Yicheng Luo
Young Lee
Yu Lu
Yun Cheng
Yuxiao Huang
Zac Cranko
Zijian Cao
Zoe Nolan
Contributors through GitHub, whose real names were not listed on their
GitHub profile, are:
SamDataMad
bumptiousmonkey
idoamihai
deepakiim
insad
HorizonP
cs-maillist
kudo23
empet
victorBigand
17SKYE
jessjing1995
We are also very grateful to Parameswaran Raman and the many anonymous reviewers, organized by Cambridge University Press, who read one
or more chapters of earlier versions of the manuscript, and provided constructive criticism that led to considerable improvements. A special mention goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt
advice about LATEX-related issues. Last but not least, we are very grateful
to our editor Lauren Cowles, who has been patiently guiding us through
the gestation process of this book.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
6 Foreword
Table of Symbols
Symbol Typical meaning
a, b, c, α, β, γ Scalars are lowercase
x, y, z Vectors are bold lowercase
A, B, C Matrices are bold uppercase
x
⊤, A
⊤
Transpose of a vector or matrix
A
−1
Inverse of a matrix
⟨x, y⟩ Inner product of x and y
x
⊤y Dot product of x and y
B = (b1, b2, b3) (Ordered) tuple
B = [b1, b2, b3] Matrix of column vectors stacked horizontally
B = {b1, b2, b3} Set of vectors (unordered)
Z, N Integers and natural numbers, respectively
R, C Real and complex numbers, respectively
Rn n-dimensional vector space of real numbers
∀x Universal quantifier: for all x
∃x Existential quantifier: there exists x
a := b a is defined as b
a =: b b is defined as a
a ∝ b a is proportional to b, i.e., a = constant · b
g ◦ f Function composition: “g after f”
⇐⇒ If and only if
=⇒ Implies
A, C Sets
a ∈ A a is an element of set A
∅ Empty set
A\B A without B: the set of elements in A but not in B
D Number of dimensions; indexed by d = 1, . . . , D
N Number of data points; indexed by n = 1, . . . , N
Im Identity matrix of size m × m
0m,n Matrix of zeros of size m × n
1m,n Matrix of ones of size m × n
ei Standard/canonical vector (where i is the component that is 1)
dim Dimensionality of vector space
rk(A) Rank of matrix A
Im(Φ) Image of linear mapping Φ
ker(Φ) Kernel (null space) of a linear mapping Φ
span[b1] Span (generating set) of b1
tr(A) Trace of A
det(A) Determinant of A
| · | Absolute value or determinant (depending on context)
∥·∥ Norm; Euclidean, unless specified
λ Eigenvalue or Lagrange multiplier
Eλ Eigenspace corresponding to eigenvalue λ
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Foreword 7
Symbol Typical meaning
x ⊥ y Vectors x and y are orthogonal
V Vector space
V
⊥ Orthogonal complement of vector space V
PN
Qn=1 xn Sum of the xn: x1 + . . . + xN
N
n=1 xn Product of the xn: x1 · . . . · xN
θ Parameter vector
∂f
∂x Partial derivative of f with respect to x
df
dx
Total derivative of f with respect to x
∇ Gradient
f∗ = minx f(x) The smallest function value of f
x∗ ∈ arg minx f(x) The value x∗ that minimizes f (note: arg min returns a set of values)
L Lagrangian
L Negative log-likelihood

n
k

Binomial coefficient, n choose k
VX[x] Variance of x with respect to the random variable X
EX[x] Expectation of x with respect to the random variable X
CovX,Y [x, y] Covariance between x and y.
X ⊥⊥ Y |Z X is conditionally independent of Y given Z
X ∼ p Random variable X is distributed according to p
N

µ, Σ

Gaussian distribution with mean µ and covariance Σ
Ber(µ) Bernoulli distribution with parameter µ
Bin(N, µ) Binomial distribution with parameters N, µ
Beta(α, β) Beta distribution with parameters α, β
Table of Abbreviations and Acronyms
Acronym Meaning
e.g. Exempli gratia (Latin: for example)
GMM Gaussian mixture model
i.e. Id est (Latin: this means)
i.i.d. Independent, identically distributed
MAP Maximum a posteriori
MLE Maximum likelihood estimation/estimator
ONB Orthonormal basis
PCA Principal component analysis
PPCA Probabilistic principal component analysis
REF Row-echelon form
SPD Symmetric, positive definite
SVM Support vector machine
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020

Part I
Mathematical Foundations
9
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com.

1
Introduction and Motivation
Machine learning is about designing algorithms that automatically extract
valuable information from data. The emphasis here is on “automatic”, i.e.,
machine learning is concerned about general-purpose methodologies that
can be applied to many datasets, while producing something that is meaningful. There are three concepts that are at the core of machine learning:
data, a model, and learning.
Since machine learning is inherently data driven, data is at the core data
of machine learning. The goal of machine learning is to design generalpurpose methodologies to extract valuable patterns from data, ideally
without much domain-specific expertise. For example, given a large corpus
of documents (e.g., books in many libraries), machine learning methods
can be used to automatically find relevant topics that are shared across
documents (Hoffman et al., 2010). To achieve this goal, we design models that are typically related to the process that generates data, similar to model
the dataset we are given. For example, in a regression setting, the model
would describe a function that maps inputs to real-valued outputs. To
paraphrase Mitchell (1997): A model is said to learn from data if its performance on a given task improves after the data is taken into account.
The goal is to find good models that generalize well to yet unseen data,
which we may care about in the future. Learning can be understood as a learning
way to automatically find patterns and structure in data by optimizing the
parameters of the model.
While machine learning has seen many success stories, and software is
readily available to design and train rich and flexible machine learning
systems, we believe that the mathematical foundations of machine learning are important in order to understand fundamental principles upon
which more complicated machine learning systems are built. Understanding these principles can facilitate creating new machine learning solutions,
understanding and debugging existing approaches, and learning about the
inherent assumptions and limitations of the methodologies we are working with.
11
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com.
12 Introduction and Motivation
1.1 Finding Words for Intuitions
A challenge we face regularly in machine learning is that concepts and
words are slippery, and a particular component of the machine learning
system can be abstracted to different mathematical concepts. For example,
the word “algorithm” is used in at least two different senses in the context of machine learning. In the first sense, we use the phrase “machine
learning algorithm” to mean a system that makes predictions based on inpredictor put data. We refer to these algorithms as predictors. In the second sense,
we use the exact same phrase “machine learning algorithm” to mean a
system that adapts some internal parameters of the predictor so that it
performs well on future unseen input data. Here we refer to this adaptatraining tion as training a system.
This book will not resolve the issue of ambiguity, but we want to highlight upfront that, depending on the context, the same expressions can
mean different things. However, we attempt to make the context sufficiently clear to reduce the level of ambiguity.
The first part of this book introduces the mathematical concepts and
foundations needed to talk about the three main components of a machine
learning system: data, models, and learning. We will briefly outline these
components here, and we will revisit them again in Chapter 8 once we
have discussed the necessary mathematical concepts.
While not all data is numerical, it is often useful to consider data in
a number format. In this book, we assume that data has already been
appropriately converted into a numerical representation suitable for readdata as vectors ing into a computer program. Therefore, we think of data as vectors. As
another illustration of how subtle words are, there are (at least) three
different ways to think about vectors: a vector as an array of numbers (a
computer science view), a vector as an arrow with a direction and magnitude (a physics view), and a vector as an object that obeys addition and
scaling (a mathematical view).
model A model is typically used to describe a process for generating data, similar to the dataset at hand. Therefore, good models can also be thought
of as simplified versions of the real (unknown) data-generating process,
capturing aspects that are relevant for modeling the data and extracting
hidden patterns from it. A good model can then be used to predict what
would happen in the real world without performing real-world experiments.
learning We now come to the crux of the matter, the learning component of
machine learning. Assume we are given a dataset and a suitable model.
Training the model means to use the data available to optimize some parameters of the model with respect to a utility function that evaluates how
well the model predicts the training data. Most training methods can be
thought of as an approach analogous to climbing a hill to reach its peak.
In this analogy, the peak of the hill corresponds to a maximum of some
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
1.2 Two Ways to Read This Book 13
desired performance measure. However, in practice, we are interested in
the model to perform well on unseen data. Performing well on data that
we have already seen (training data) may only mean that we found a
good way to memorize the data. However, this may not generalize well to
unseen data, and, in practical applications, we often need to expose our
machine learning system to situations that it has not encountered before.
Let us summarize the main concepts of machine learning that we cover
in this book:
We represent data as vectors.
We choose an appropriate model, either using the probabilistic or optimization view.
We learn from available data by using numerical optimization methods
with the aim that the model performs well on data not used for training.
1.2 Two Ways to Read This Book
We can consider two strategies for understanding the mathematics for
machine learning:
Bottom-up: Building up the concepts from foundational to more advanced. This is often the preferred approach in more technical fields,
such as mathematics. This strategy has the advantage that the reader
at all times is able to rely on their previously learned concepts. Unfortunately, for a practitioner many of the foundational concepts are not
particularly interesting by themselves, and the lack of motivation means
that most foundational definitions are quickly forgotten.
Top-down: Drilling down from practical needs to more basic requirements. This goal-driven approach has the advantage that the readers
know at all times why they need to work on a particular concept, and
there is a clear path of required knowledge. The downside of this strategy is that the knowledge is built on potentially shaky foundations, and
the readers have to remember a set of words that they do not have any
way of understanding.
We decided to write this book in a modular way to separate foundational
(mathematical) concepts from applications so that this book can be read
in both ways. The book is split into two parts, where Part I lays the mathematical foundations and Part II applies the concepts from Part I to a set
of fundamental machine learning problems, which form four pillars of
machine learning as illustrated in Figure 1.2: regression, dimensionality
reduction, density estimation, and classification. Chapters in Part I mostly
build upon the previous ones, but it is possible to skip a chapter and work
backward if necessary. Chapters in Part II are only loosely coupled and
can be read in any order. There are many pointers forward and backward
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
14 Introduction and Motivation
Figure 1.2 The
foundations and
four pillars of
machine learning.
Classification
Density
Regression
Dimensionality
Estimation
Reduction
Machine Learning
Vector Calculus Probability & Distributions Optimization
Linear Algebra Analytic Geometry Matrix Decomposition
between the two parts of the book to link mathematical concepts with
machine learning algorithms.
Of course there are more than two ways to read this book. Most readers
learn using a combination of top-down and bottom-up approaches, sometimes building up basic mathematical skills before attempting more complex concepts, but also choosing topics based on applications of machine
learning.
Part I Is about Mathematics
The four pillars of machine learning we cover in this book (see Figure 1.2)
require a solid mathematical foundation, which is laid out in Part I.
We represent numerical data as vectors and represent a table of such
data as a matrix. The study of vectors and matrices is called linear algebra,
linear algebra which we introduce in Chapter 2. The collection of vectors as a matrix is
also described there.
Given two vectors representing two objects in the real world, we want
to make statements about their similarity. The idea is that vectors that
are similar should be predicted to have similar outputs by our machine
learning algorithm (our predictor). To formalize the idea of similarity between vectors, we need to introduce operations that take two vectors as
input and return a numerical value representing their similarity. The conanalytic geometry struction of similarity and distances is central to analytic geometry and is
discussed in Chapter 3.
In Chapter 4, we introduce some fundamental concepts about matrimatrix ces and matrix decomposition. Some operations on matrices are extremely
decomposition useful in machine learning, and they allow for an intuitive interpretation
of the data and more efficient learning.
We often consider data to be noisy observations of some true underlying signal. We hope that by applying machine learning we can identify the
signal from the noise. This requires us to have a language for quantifying what “noise” means. We often would also like to have predictors that
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
1.2 Two Ways to Read This Book 15
allow us to express some sort of uncertainty, e.g., to quantify the confidence we have about the value of the prediction at a particular test data
point. Quantification of uncertainty is the realm of probability theory and probability theory
is covered in Chapter 6.
To train machine learning models, we typically find parameters that
maximize some performance measure. Many optimization techniques require the concept of a gradient, which tells us the direction in which to
search for a solution. Chapter 5 is about vector calculus and details the vector calculus
concept of gradients, which we subsequently use in Chapter 7, where we
talk about optimization to find maxima/minima of functions. optimization
Part II Is about Machine Learning
The second part of the book introduces four pillars of machine learning
as shown in Figure 1.2. We illustrate how the mathematical concepts introduced in the first part of the book are the foundation for each pillar.
Broadly speaking, chapters are ordered by difficulty (in ascending order).
In Chapter 8, we restate the three components of machine learning
(data, models, and parameter estimation) in a mathematical fashion. In
addition, we provide some guidelines for building experimental set-ups
that guard against overly optimistic evaluations of machine learning systems. Recall that the goal is to build a predictor that performs well on
unseen data.
In Chapter 9, we will have a close look at linear regression, where our linear regression
objective is to find functions that map inputs x ∈ RD to corresponding observed function values y ∈ R, which we can interpret as the labels of their
respective inputs. We will discuss classical model fitting (parameter estimation) via maximum likelihood and maximum a posteriori estimation,
as well as Bayesian linear regression, where we integrate the parameters
out instead of optimizing them.
Chapter 10 focuses on dimensionality reduction, the second pillar in Fig- dimensionality
ure 1.2 reduction , using principal component analysis. The key objective of dimensionality reduction is to find a compact, lower-dimensional representation
of high-dimensional data x ∈ RD, which is often easier to analyze than
the original data. Unlike regression, dimensionality reduction is only concerned about modeling the data – there are no labels associated with a
data point x.
In Chapter 11, we will move to our third pillar: density estimation. The density estimation
objective of density estimation is to find a probability distribution that describes a given dataset. We will focus on Gaussian mixture models for this
purpose, and we will discuss an iterative scheme to find the parameters of
this model. As in dimensionality reduction, there are no labels associated
with the data points x ∈ RD. However, we do not seek a low-dimensional
representation of the data. Instead, we are interested in a density model
that describes the data.
Chapter 12 concludes the book with an in-depth discussion of the fourth
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
16 Introduction and Motivation
classification pillar: classification. We will discuss classification in the context of support
vector machines. Similar to regression (Chapter 9), we have inputs x and
corresponding labels y. However, unlike regression, where the labels were
real-valued, the labels in classification are integers, which requires special
care.
1.3 Exercises and Feedback
We provide some exercises in Part I, which can be done mostly by pen and
paper. For Part II, we provide programming tutorials (jupyter notebooks)
to explore some properties of the machine learning algorithms we discuss
in this book.
We appreciate that Cambridge University Press strongly supports our
aim to democratize education and learning by making this book freely
available for download at
https://mml-book.com
where tutorials, errata, and additional materials can be found. Mistakes
can be reported and feedback provided using the preceding URL.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2
Linear Algebra
When formalizing intuitive concepts, a common approach is to construct a
set of objects (symbols) and a set of rules to manipulate these objects. This
is known as an algebra. Linear algebra is the study of vectors and certain algebra
rules to manipulate vectors. The vectors many of us know from school are
called “geometric vectors”, which are usually denoted by a small arrow
above the letter, e.g., −→x and −→y . In this book, we discuss more general
concepts of vectors and use a bold letter to represent them, e.g., x and y.
In general, vectors are special objects that can be added together and
multiplied by scalars to produce another object of the same kind. From
an abstract mathematical viewpoint, any object that satisfies these two
properties can be considered a vector. Here are some examples of such
vector objects:
1. Geometric vectors. This example of a vector may be familiar from high
school mathematics and physics. Geometric vectors – see Figure 2.1(a)
– are directed segments, which can be drawn (at least in two dimensions). Two geometric vectors →
x,
→
y can be added, such that →
x+
→
y =
→
z
is another geometric vector. Furthermore, multiplication by a scalar
λ
→
x, λ ∈ R, is also a geometric vector. In fact, it is the original vector
scaled by λ. Therefore, geometric vectors are instances of the vector
concepts introduced previously. Interpreting vectors as geometric vectors enables us to use our intuitions about direction and magnitude to
reason about mathematical operations.
2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can
Figure 2.1
Different types of
vectors. Vectors can
be surprising
objects, including
(a) geometric
vectors
and (b) polynomials.
→
x →
y
→
x +
→
y
(a) Geometric vectors.
−2 0 2
x
−6
−4
−2
0
2
4
y
(b) Polynomials.
17
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com.
18 Linear Algebra
be added together, which results in another polynomial; and they can
be multiplied by a scalar λ ∈ R, and the result is a polynomial as
well. Therefore, polynomials are (rather unusual) instances of vectors.
Note that polynomials are very different from geometric vectors. While
geometric vectors are concrete “drawings”, polynomials are abstract
concepts. However, they are both vectors in the sense previously described.
3. Audio signals are vectors. Audio signals are represented as a series of
numbers. We can add audio signals together, and their sum is a new
audio signal. If we scale an audio signal, we also obtain an audio signal.
Therefore, audio signals are a type of vector, too.
4. Elements of Rn
(tuples of n real numbers) are vectors. Rn
is more
abstract than polynomials, and it is the concept we focus on in this
book. For instance,
a =


1
2
3

 ∈ R
3
(2.1)
is an example of a triplet of numbers. Adding two vectors a, b ∈ Rn
component-wise results in another vector: a + b = c ∈ Rn
. Moreover,
multiplying a ∈ Rn by λ ∈ R results in a scaled vector λa ∈ Rn
.
Considering vectors as elements of Rn
Be careful to check has an additional benefit that
whether array
operations actually
perform vector
operations when
implementing on a
computer.
it loosely corresponds to arrays of real numbers on a computer. Many
programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.
Linear algebra focuses on the similarities between these vector concepts.
We can add them together and multiply them by scalars. We will largely Pavel Grinfeld’s
series on linear
algebra:
http://tinyurl.
com/nahclwm
Gilbert Strang’s
course on linear
algebra:
http://tinyurl.
com/29p5q8j
3Blue1Brown series
on linear algebra:
https://tinyurl.
com/h5g4kps
focus on vectors in Rn
since most algorithms in linear algebra are formulated in Rn
. We will see in Chapter 8 that we often consider data to
be represented as vectors in Rn
. In this book, we will focus on finitedimensional vector spaces, in which case there is a 1:1 correspondence
between any kind of vector and Rn
. When it is convenient, we will use
intuitions about geometric vectors and consider array-based algorithms.
One major idea in mathematics is the idea of “closure”. This is the question: What is the set of all things that can result from my proposed operations? In the case of vectors: What is the set of vectors that can result by
starting with a small set of vectors, and adding them to each other and
scaling them? This results in a vector space (Section 2.4). The concept of
a vector space and its properties underlie much of machine learning. The
concepts introduced in this chapter are summarized in Figure 2.2.
This chapter is mostly based on the lecture notes and books by Drumm
and Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann
(2015), as well as Pavel Grinfeld’s Linear Algebra series. Other excellent
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.1 Systems of Linear Equations 19
Figure 2.2 A mind
map of the concepts
introduced in this
chapter, along with
where they are used
in other parts of the
book.
Vector
Vector space
Matrix Chapter 5
Vector calculus
Group
System of
linear equations
Matrix
inverse
Gaussian
elimination
Linear/affine
mapping
Linear
independence
Basis
Chapter 10
Dimensionality
reduction
Chapter 12
Classification
Chapter 3
Analytic geometry
composes
closure
Abelian
with +
represents
represents
solved by
solves
property of
maximal set
resources are Gilbert Strang’s Linear Algebra course at MIT and the Linear
Algebra Series by 3Blue1Brown.
Linear algebra plays an important role in machine learning and general mathematics. The concepts introduced in this chapter are further expanded to include the idea of geometry in Chapter 3. In Chapter 5, we
will discuss vector calculus, where a principled knowledge of matrix operations is essential. In Chapter 10, we will use projections (to be introduced in Section 3.8) for dimensionality reduction with principal component analysis (PCA). In Chapter 9, we will discuss linear regression, where
linear algebra plays a central role for solving least-squares problems.
2.1 Systems of Linear Equations
Systems of linear equations play a central part of linear algebra. Many
problems can be formulated as systems of linear equations, and linear
algebra gives us the tools for solving them.
Example 2.1
A company produces products N1, . . . , Nn for which resources
R1, . . . , Rm are required. To produce a unit of product Nj , aij units of
resource Ri are needed, where i = 1, . . . , m and j = 1, . . . , n.
The objective is to find an optimal production plan, i.e., a plan of how
many units xj of product Nj should be produced if a total of bi units of
resource Ri are available and (ideally) no resources are left over.
If we produce x1, . . . , xn units of the corresponding products, we need
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
20 Linear Algebra
a total of
ai1x1 + · · · + ainxn (2.2)
many units of resource Ri
. An optimal production plan (x1, . . . , xn) ∈ Rn
,
therefore, has to satisfy the following system of equations:
a11x1 + · · · + a1nxn = b1
.
.
.
am1x1 + · · · + amnxn = bm
, (2.3)
where aij ∈ R and bi ∈ R.
system of linear Equation (2.3) is the general form of a system of linear equations, and
equations x1, . . . , xn are the unknowns of this system. Every n-tuple (x1, . . . , xn) ∈
Rn
solution that satisfies (2.3) is a solution of the linear equation system.
Example 2.2
The system of linear equations
x1 + x2 + x3 = 3 (1)
x1 − x2 + 2x3 = 2 (2)
2x1 + 3x3 = 1 (3)
(2.4)
has no solution: Adding the first two equations yields 2x1+3x3 = 5, which
contradicts the third equation (3).
Let us have a look at the system of linear equations
x1 + x2 + x3 = 3 (1)
x1 − x2 + 2x3 = 2 (2)
x2 + x3 = 2 (3)
. (2.5)
From the first and third equation, it follows that x1 = 1. From (1)+(2),
we get 2x1 + 3x3 = 5, i.e., x3 = 1. From (3), we then get that x2 = 1.
Therefore, (1, 1, 1) is the only possible and unique solution (verify that
(1, 1, 1) is a solution by plugging in).
As a third example, we consider
x1 + x2 + x3 = 3 (1)
x1 − x2 + 2x3 = 2 (2)
2x1 + 3x3 = 5 (3)
. (2.6)
Since (1)+(2)=(3), we can omit the third equation (redundancy). From
(1) and (2), we get 2x1 = 5−3x3 and 2x2 = 1+x3. We define x3 = a ∈ R
as a free variable, such that any triplet

5
2
−
3
2
a,
1
2
+
1
2
a, a
, a ∈ R (2.7)
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.1 Systems of Linear Equations 21
Figure 2.1 The
solution space of a
system of two linear
equations with two
variables can be
geometrically
interpreted as the
intersection of two
lines. Every linear
equation represents
a line.
2x1 − 4x2 = 1 4x1 + 4x2 = 5 x1
x2
is a solution of the system of linear equations, i.e., we obtain a solution
set that contains infinitely many solutions.
In general, for a real-valued system of linear equations we obtain either
no, exactly one, or infinitely many solutions. Linear regression (Chapter 9)
solves a version of Example 2.1 when we cannot solve the system of linear
equations.
Remark (Geometric Interpretation of Systems of Linear Equations). In a
system of linear equations with two variables x1, x2, each linear equation
defines a line on the x1x2-plane. Since a solution to a system of linear
equations must satisfy all equations simultaneously, the solution set is the
intersection of these lines. This intersection set can be a line (if the linear
equations describe the same line), a point, or empty (when the lines are
parallel). An illustration is given in Figure 2.1 for the system
4x1 + 4x2 = 5
2x1 − 4x2 = 1
(2.8)
where the solution space is the point (x1, x2) = (1,
1
4
). Similarly, for three
variables, each linear equation determines a plane in three-dimensional
space. When we intersect these planes, i.e., satisfy all linear equations at
the same time, we can obtain a solution set that is a plane, a line, a point
or empty (when the planes have no common intersection). ♢
For a systematic approach to solving systems of linear equations, we
will introduce a useful compact notation. We collect the coefficients aij
into vectors and collect the vectors into matrices. In other words, we write
the system from (2.3) in the following form:



a11
.
.
.
am1



x1 +



a12
.
.
.
am2



x2 + · · · +



a1n
.
.
.
amn



xn =



b1
.
.
.
bm


 (2.9)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
22 Linear Algebra
⇐⇒



a11 · · · a1n
.
.
.
.
.
.
am1 · · · amn






x1
.
.
.
xn


 =



b1
.
.
.
bm


 . (2.10)
In the following, we will have a close look at these matrices and define computation rules. We will return to solving linear equations in Section 2.3.
2.2 Matrices
Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations, but they also represent linear
functions (linear mappings) as we will see later in Section 2.7. Before we
discuss some of these interesting topics, let us first define what a matrix
is and what kind of operations we can do with matrices. We will see more
properties of matrices in Chapter 4.
matrix Definition 2.1 (Matrix). With m, n ∈ N a real-valued (m, n) matrix A is
an m·n-tuple of elements aij , i = 1, . . . , m, j = 1, . . . , n, which is ordered
according to a rectangular scheme consisting of m rows and n columns:
A =





a11 a12 · · · a1n
a21 a22 · · · a2n
.
.
.
.
.
.
.
.
.
am1 am2 · · · amn





, aij ∈ R . (2.11)
row By convention (1, n)-matrices are called rows and (m, 1)-matrices are called
column columns. These special matrices are also called row/column vectors.
row vector
column vector
Figure 2.2 By
stacking its
columns, a matrix A
can be represented
as a long vector a.
re-shape
A ∈ R
4×2 a ∈ R
8
Rm×n
is the set of all real-valued (m, n)-matrices. A ∈ Rm×n
can be
equivalently represented as a ∈ Rmn by stacking all n columns of the
matrix into a long vector; see Figure 2.2.
2.2.1 Matrix Addition and Multiplication
The sum of two matrices A ∈ Rm×n
, B ∈ Rm×n
is defined as the elementwise sum, i.e.,
A + B :=



a11 + b11 · · · a1n + b1n
.
.
.
.
.
.
am1 + bm1 · · · amn + bmn


 ∈ R
m×n
. (2.12)
For matrices A ∈ Rm×n
, B ∈ Rn×k Note the size of the , the elements cij of the product
matrices. C = AB ∈ Rm×k are computed as
C =
np.einsum(’il,
lj’, A, B) cij =
Xn
l=1
ailblj , i = 1, . . . , m, j = 1, . . . , k. (2.13)
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.2 Matrices 23
This means, to compute element cij we multiply the elements of the ith There are n columns
in A and n rows in
B so that we can
compute ailblj for
l = 1, . . . , n.
Commonly, the dot
product between
two vectors a, b is
denoted by a⊤b or
⟨a, b⟩.
row of A with the jth column of B and sum them up. Later in Section 3.2,
we will call this the dot product of the corresponding row and column. In
cases, where we need to be explicit that we are performing multiplication,
we use the notation A · B to denote multiplication (explicitly showing
“·”).
Remark. Matrices can only be multiplied if their “neighboring” dimensions
match. For instance, an n × k-matrix A can be multiplied with a k × mmatrix B, but only from the left side:
|{z}
A
n×k
|{z}
B
k×m
= |{z}
C
n×m
(2.14)
The product BA is not defined if m ̸= n since the neighboring dimensions
do not match. ♢
Remark. Matrix multiplication is not defined as an element-wise operation
on matrix elements, i.e., cij ̸= aij bij (even if the size of A, B was chosen appropriately). This kind of element-wise multiplication often appears
in programming languages when we multiply (multi-dimensional) arrays
with each other, and is called a Hadamard product. ♢ Hadamard product
Example 2.3
For A =

1 2 3
3 2 1
∈ R2×3
, B =


0 2
1 −1
0 1

 ∈ R3×2
, we obtain
AB =

1 2 3
3 2 1


0 2
1 −1
0 1

 =

2 3
2 5
∈ R
2×2
, (2.15)
BA =


0 2
1 −1
0 1



1 2 3
3 2 1
=


6 4 2
−2 0 2
3 2 1

 ∈ R
3×3
. (2.16)
Figure 2.3 Even if
both matrix
multiplications AB
and BA are
defined, the
dimensions of the
results can be
different.
From this example, we can already see that matrix multiplication is not
commutative, i.e., AB ̸= BA; see also Figure 2.3 for an illustration.
Definition 2.2 (Identity Matrix). In Rn×n
, we define the identity matrix
identity matrix
In :=










1 0 · · · 0 · · · 0
0 1 · · · 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 · · · 1










∈ R
n×n
(2.17)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
24 Linear Algebra
as the n × n-matrix containing 1 on the diagonal and 0 everywhere else.
Now that we defined matrix multiplication, matrix addition and the
identity matrix, let us have a look at some properties of matrices:
associativity
Associativity:
∀A ∈ R
m×n
, B ∈ R
n×p
, C ∈ R
p×q
: (AB)C = A(BC) (2.18)
distributivity
Distributivity:
∀A, B ∈ R
m×n
, C, D ∈ R
n×p
: (A + B)C = AC + BC (2.19a)
A(C + D) = AC + AD (2.19b)
Multiplication with the identity matrix:
∀A ∈ R
m×n
: ImA = AIn = A (2.20)
Note that Im ̸= In for m ̸= n.
2.2.2 Inverse and Transpose
Definition 2.3 (Inverse). Consider a square matrix A ∈ Rn×n
A square matrix . Let matrix
possesses the same
number of columns
and rows.
B ∈ Rn×n have the property that AB = In = BA. B is called the
inverse of A and denoted by A
−1
.
inverse Unfortunately, not every matrix A possesses an inverse A
−1
. If this
regular inverse does exist, A is called regular/invertible/nonsingular, otherwise
invertible
nonsingular
singular/noninvertible. When the matrix inverse exists, it is unique. In Secsingular
noninvertible
tion 2.3, we will discuss a general way to compute the inverse of a matrix
by solving a system of linear equations.
Remark (Existence of the Inverse of a 2 × 2-matrix). Consider a matrix
A := 
a11 a12
a21 a22
∈ R
2×2
. (2.21)
If we multiply A with
A
′
:= 
a22 −a12
−a21 a11 
(2.22)
we obtain
AA′ =

a11a22 − a12a21 0
0 a11a22 − a12a21
= (a11a22 − a12a21)I .
(2.23)
Therefore,
A
−1 =
1
a11a22 − a12a21 
a22 −a12
−a21 a11 
(2.24)
if and only if a11a22 −a12a21 ̸= 0. In Section 4.1, we will see that a11a22 −
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.2 Matrices 25
a12a21 is the determinant of a 2×2-matrix. Furthermore, we can generally
use the determinant to check whether a matrix is invertible. ♢
Example 2.4 (Inverse Matrix)
The matrices
A =


1 2 1
4 4 5
6 7 7

 , B =


−7 −7 6
2 1 −1
4 5 −4

 (2.25)
are inverse to each other since AB = I = BA.
Definition 2.4 (Transpose). For A ∈ Rm×n
the matrix B ∈ Rn×m with
bij = aji is called the transpose of A. We write B = A
⊤
. transpose
The main diagonal
(sometimes called
“principal diagonal”,
“primary diagonal”,
“leading diagonal”,
or “major diagonal”)
of a matrix A is the
collection of entries
Aij where i = j.
In general, A
⊤
can be obtained by writing the columns of A as the rows
of A
⊤
. The following are important properties of inverses and transposes:
The scalar case of
(2.28) is
1
2+4 = 1
6
̸= 1
2 + 1
4
.
AA−1 = I = A
−1A (2.26)
(AB)
−1 = B
−1A
−1
(2.27)
(A + B)
−1
̸= A
−1 + B
−1
(2.28)
(A
⊤
)
⊤ = A (2.29)
(A + B)
⊤ = A
⊤ + B
⊤
(2.30)
(AB)
⊤ = B
⊤A
⊤
(2.31)
Definition 2.5 (Symmetric Matrix). A matrix A ∈ Rn×n
is symmetric if symmetric matrix
A = A
⊤
.
Note that only (n, n)-matrices can be symmetric. Generally, we call
(n, n)-matrices also square matrices because they possess the same num- square matrix
ber of rows and columns. Moreover, if A is invertible, then so is A
⊤
, and
(A
−1
)
⊤ = (A
⊤
)
−1 =: A
−⊤
.
Remark (Sum and Product of Symmetric Matrices). The sum of symmetric matrices A, B ∈ Rn×n
is always symmetric. However, although their
product is always defined, it is generally not symmetric:

1 0
0 0 1 1
1 1
=

1 1
0 0
. (2.32)
♢
2.2.3 Multiplication by a Scalar
Let us look at what happens to matrices when they are multiplied by a
scalar λ ∈ R. Let A ∈ Rm×n and λ ∈ R. Then λA = K, Kij = λ aij .
Practically, λ scales each element of A. For λ, ψ ∈ R, the following holds:
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
26 Linear Algebra
associativity Associativity:
(λψ)C = λ(ψC), C ∈ Rm×n
λ(BC) = (λB)C = B(λC) = (BC)λ, B ∈ Rm×n
, C ∈ Rn×k
.
Note that this allows us to move scalar values around.
(λC)
⊤ = C
⊤
λ
⊤ = C
⊤
λ = λC
⊤
since λ = λ
⊤ for all λ ∈ R. distributivity
Distributivity:
(λ + ψ)C = λC + ψC, C ∈ Rm×n
λ(B + C) = λB + λC, B, C ∈ Rm×n
Example 2.5 (Distributivity)
If we define
C := 
1 2
3 4
, (2.33)
then for any λ, ψ ∈ R we obtain
(λ + ψ)C =

(λ + ψ)1 (λ + ψ)2
(λ + ψ)3 (λ + ψ)4
=

λ + ψ 2λ + 2ψ
3λ + 3ψ 4λ + 4ψ

(2.34a)
=

λ 2λ
3λ 4λ

+

ψ 2ψ
3ψ 4ψ

= λC + ψC . (2.34b)
2.2.4 Compact Representations of Systems of Linear Equations
If we consider the system of linear equations
2x1 + 3x2 + 5x3 = 1
4x1 − 2x2 − 7x3 = 8
9x1 + 5x2 − 3x3 = 2
(2.35)
and use the rules for matrix multiplication, we can write this equation
system in a more compact form as


2 3 5
4 −2 −7
9 5 −3




x1
x2
x3

 =


1
8
2

 . (2.36)
Note that x1 scales the first column, x2 the second one, and x3 the third
one.
Generally, a system of linear equations can be compactly represented in
their matrix form as Ax = b; see (2.3), and the product Ax is a (linear)
combination of the columns of A. We will discuss linear combinations in
more detail in Section 2.5.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.3 Solving Systems of Linear Equations 27
2.3 Solving Systems of Linear Equations
In (2.3), we introduced the general form of an equation system, i.e.,
a11x1 + · · · + a1nxn = b1
.
.
.
am1x1 + · · · + amnxn = bm ,
(2.37)
where aij ∈ R and bi ∈ R are known constants and xj are unknowns,
i = 1, . . . , m, j = 1, . . . , n. Thus far, we saw that matrices can be used as
a compact way of formulating systems of linear equations so that we can
write Ax = b, see (2.10). Moreover, we defined basic matrix operations,
such as addition and multiplication of matrices. In the following, we will
focus on solving systems of linear equations and provide an algorithm for
finding the inverse of a matrix.
2.3.1 Particular and General Solution
Before discussing how to generally solve systems of linear equations, let
us have a look at an example. Consider the system of equations

1 0 8 −4
0 1 2 12 




x1
x2
x3
x4



 =

42
8

. (2.38)
The system has two equations and four unknowns. Therefore, in general
we would expect infinitely many solutions. This system of equations is
in a particularly easy form, where the first two columns consist of a 1
P
and a 0. Remember that we want to find scalars x1, . . . , x4, such that
4
i=1 xici = b, where we define ci to be the ith column of the matrix and
b the right-hand-side of (2.38). A solution to the problem in (2.38) can
be found immediately by taking 42 times the first column and 8 times the
second column so that
b =

42
8

= 42 
1
0

+ 8 
0
1

. (2.39)
Therefore, a solution is [42, 8, 0, 0]⊤. This solution is called a particular particular solution
solution or special solution. However, this is not the only solution of this special solution
system of linear equations. To capture all the other solutions, we need
to be creative in generating 0 in a non-trivial way using the columns of
the matrix: Adding 0 to our special solution does not change the special
solution. To do so, we express the third column using the first two columns
(which are of this very simple form)

8
2

= 8 
1
0

+ 2 
0
1

(2.40)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
28 Linear Algebra
so that 0 = 8c1 + 2c2 − 1c3 + 0c4 and (x1, x2, x3, x4) = (8, 2, −1, 0). In
fact, any scaling of this solution by λ1 ∈ R produces the 0 vector, i.e.,

1 0 8 −4
0 1 2 12 


λ1




8
2
−1
0





 = λ1(8c1 + 2c2 − c3) = 0 . (2.41)
Following the same line of reasoning, we express the fourth column of the
matrix in (2.38) using the first two columns and generate another set of
non-trivial versions of 0 as

1 0 8 −4
0 1 2 12 


λ2




−4
12
0
−1





 = λ2(−4c1 + 12c2 − c4) = 0 (2.42)
for any λ2 ∈ R. Putting everything together, we obtain all solutions of the
general solution equation system in (2.38), which is called the general solution, as the set



x ∈ R
4
: x =




42
8
0
0




+ λ1




8
2
−1
0




+ λ2




−4
12
0
−1




, λ1, λ2 ∈ R



. (2.43)
Remark. The general approach we followed consisted of the following
three steps:
1. Find a particular solution to Ax = b.
2. Find all solutions to Ax = 0.
3. Combine the solutions from steps 1. and 2. to the general solution.
Neither the general nor the particular solution is unique. ♢
The system of linear equations in the preceding example was easy to
solve because the matrix in (2.38) has this particularly convenient form,
which allowed us to find the particular and the general solution by inspection. However, general equation systems are not of this simple form.
Fortunately, there exists a constructive algorithmic way of transforming
any system of linear equations into this particularly simple form: Gaussian
elimination. Key to Gaussian elimination are elementary transformations
of systems of linear equations, which transform the equation system into
a simple form. Then, we can apply the three steps to the simple form that
we just discussed in the context of the example in (2.38).
2.3.2 Elementary Transformations
elementary Key to solving a system of linear equations are elementary transformations
transformations that keep the solution set the same, but that transform the equation system
into a simpler form:
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.3 Solving Systems of Linear Equations 29
Exchange of two equations (rows in the matrix representing the system
of equations)
Multiplication of an equation (row) with a constant λ ∈ R\{0}
Addition of two equations (rows)
Example 2.6
For a ∈ R, we seek all solutions of the following system of equations:
−2x1 + 4x2 − 2x3 − x4 + 4x5 = −3
4x1 − 8x2 + 3x3 − 3x4 + x5 = 2
x1 − 2x2 + x3 − x4 + x5 = 0
x1 − 2x2 − 3x4 + 4x5 = a
. (2.44)
We start by converting this system of equations into the compact matrix
notation Ax = b. We no longer mention the variables x explicitly and
build the augmented matrix (in the form
A | b

) augmented matrix




−2 4 −2 −1 4 −3
4 −8 3 −3 1 2
1 −2 1 −1 1 0
1 −2 0 −3 4 a




Swap with R3
Swap with R1
where we used the vertical line to separate the left-hand side from the
right-hand side in (2.44). We use ⇝ to indicate a transformation of the
augmented matrix using elementary transformations. The augmented
matrix
A | b

compactly
represents the
system of linear
equations Ax = b.
Swapping Rows 1 and 3 leads to




1 −2 1 −1 1 0
4 −8 3 −3 1 2
−2 4 −2 −1 4 −3
1 −2 0 −3 4 a




−4R1
+2R1
−R1
When we now apply the indicated transformations (e.g., subtract Row 1
four times from Row 2), we obtain




1 −2 1 −1 1 0
0 0 −1 1 −3 2
0 0 0 −3 6 −3
0 0 −1 −2 3 a




−R2 − R3
⇝




1 −2 1 −1 1 0
0 0 −1 1 −3 2
0 0 0 −3 6 −3
0 0 0 0 0 a+ 1




·(−1)
·(−
1
3
)
⇝




1 −2 1 −1 1 0
0 0 1 −1 3 −2
0 0 0 1 −2 1
0 0 0 0 0 a+ 1




©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
30 Linear Algebra
row-echelon form This (augmented) matrix is in a convenient form, the row-echelon form
(REF). Reverting this compact notation back into the explicit notation with
the variables we seek, we obtain
x1 − 2x2 + x3 − x4 + x5 = 0
x3 − x4 + 3x5 = −2
x4 − 2x5 = 1
0 = a + 1
. (2.45)
particular solution Only for a = −1 this system can be solved. A particular solution is






x1
x2
x3
x4
x5






=






2
0
−1
1
0






. (2.46)
general solution The general solution, which captures the set of all possible solutions, is



x ∈ R
5
: x =






2
0
−1
1
0






+ λ1






2
1
0
0
0






+ λ2






2
0
−1
2
1






, λ1, λ2 ∈ R



. (2.47)
In the following, we will detail a constructive way to obtain a particular
and general solution of a system of linear equations.
Remark (Pivots and Staircase Structure). The leading coefficient of a row
pivot (first nonzero number from the left) is called the pivot and is always
strictly to the right of the pivot of the row above it. Therefore, any equation system in row-echelon form always has a “staircase” structure. ♢
row-echelon form Definition 2.6 (Row-Echelon Form). A matrix is in row-echelon form if
All rows that contain only zeros are at the bottom of the matrix; correspondingly, all rows that contain at least one nonzero element are on
top of rows that contain only zeros.
Looking at nonzero rows only, the first nonzero number from the left
pivot (also called the pivot or the leading coefficient) is always strictly to the
leading coefficient right of the pivot of the row above it.
In other texts, it is
sometimes required
that the pivot is 1.
Remark (Basic and Free Variables). The variables corresponding to the
pivots in the row-echelon form are called basic variables and the other
basic variable variables are free variables. For example, in (2.45), x1, x3, x4 are basic
free variable variables, whereas x2, x5 are free variables. ♢
Remark (Obtaining a Particular Solution). The row-echelon form makes
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.3 Solving Systems of Linear Equations 31
our lives easier when we need to determine a particular solution. To do
this, we express the right-hand side of the equation system using the pivot
columns, such that b =
PP
i=1 λipi
, where pi
, i = 1, . . . , P, are the pivot
columns. The λi are determined easiest if we start with the rightmost pivot
column and work our way to the left.
In the previous example, we would try to find λ1, λ2, λ3 so that
λ1




1
0
0
0




+ λ2




1
1
0
0




+ λ3




−1
−1
1
0



 =




0
−2
1
0




. (2.48)
From here, we find relatively directly that λ3 = 1, λ2 = −1, λ1 = 2. When
we put everything together, we must not forget the non-pivot columns
for which we set the coefficients implicitly to 0. Therefore, we get the
particular solution x = [2, 0, −1, 1, 0]⊤. ♢
Remark (Reduced Row Echelon Form). An equation system is in reduced reduced
row-echelon form (also: row-reduced echelon form or row canonical form) if row-echelon form
It is in row-echelon form.
Every pivot is 1.
The pivot is the only nonzero entry in its column.
♢
The reduced row-echelon form will play an important role later in Section 2.3.3 because it allows us to determine the general solution of a system of linear equations in a straightforward way.
Gaussian
Remark (Gaussian Elimination). Gaussian elimination is an algorithm that elimination
performs elementary transformations to bring a system of linear equations
into reduced row-echelon form. ♢
Example 2.7 (Reduced Row Echelon Form)
Verify that the following matrix is in reduced row-echelon form (the pivots
are in bold):
A =


1 3 0 0 3
0 0 1 0 9
0 0 0 1 −4

 . (2.49)
The key idea for finding the solutions of Ax = 0 is to look at the nonpivot columns, which we will need to express as a (linear) combination of
the pivot columns. The reduced row echelon form makes this relatively
straightforward, and we express the non-pivot columns in terms of sums
and multiples of the pivot columns that are on their left: The second column is 3 times the first column (we can ignore the pivot columns on the
right of the second column). Therefore, to obtain 0, we need to subtract
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
32 Linear Algebra
the second column from three times the first column. Now, we look at the
fifth column, which is our second non-pivot column. The fifth column can
be expressed as 3 times the first pivot column, 9 times the second pivot
column, and −4 times the third pivot column. We need to keep track of
the indices of the pivot columns and translate this into 3 times the first column, 0 times the second column (which is a non-pivot column), 9 times
the third column (which is our second pivot column), and −4 times the
fourth column (which is the third pivot column). Then we need to subtract
the fifth column to obtain 0. In the end, we are still solving a homogeneous
equation system.
To summarize, all solutions of Ax = 0, x ∈ R5 are given by



x ∈ R
5
: x = λ1






3
−1
0
0
0






+ λ2






3
0
9
−4
−1






, λ1, λ2 ∈ R



. (2.50)
2.3.3 The Minus-1 Trick
In the following, we introduce a practical trick for reading out the solutions x of a homogeneous system of linear equations Ax = 0, where
A ∈ Rk×n
, x ∈ Rn
.
To start, we assume that A is in reduced row-echelon form without any
rows that just contain zeros, i.e.,
A =









0 · · · 0 1 ∗ · · · ∗ 0 ∗ · · · ∗ 0 ∗ · · · ∗
.
.
.
.
.
. 0 0 · · · 0 1 ∗ · · · ∗
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 0
.
.
.
.
.
.
0 · · · 0 0 0 · · · 0 0 0 · · · 0 1 ∗ · · · ∗









,
(2.51)
where ∗ can be an arbitrary real number, with the constraints that the first
nonzero entry per row must be 1 and all other entries in the corresponding
column must be 0. The columns j1, . . . , jk with the pivots (marked in
bold) are the standard unit vectors e1, . . . , ek ∈ Rk
. We extend this matrix
to an n × n-matrix A˜ by adding n − k rows of the form

0 · · · 0 −1 0 · · · 0

(2.52)
so that the diagonal of the augmented matrix A˜ contains either 1 or −1.
Then, the columns of A˜ that contain the −1 as pivots are solutions of
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.3 Solving Systems of Linear Equations 33
the homogeneous equation system Ax = 0. To be more precise, these
columns form a basis (Section 2.6.1) of the solution space of Ax = 0,
which we will later call the kernel or null space (see Section 2.7.3). kernel
null space
Example 2.8 (Minus-1 Trick)
Let us revisit the matrix in (2.49), which is already in reduced REF:
A =


1 3 0 0 3
0 0 1 0 9
0 0 0 1 −4

 . (2.53)
We now augment this matrix to a 5 × 5 matrix by adding rows of the
form (2.52) at the places where the pivots on the diagonal are missing
and obtain
A˜ =






1 3 0 0 3
0 −1 0 0 0
0 0 1 0 9
0 0 0 1 −4
0 0 0 0 −1






. (2.54)
From this form, we can immediately read out the solutions of Ax = 0 by
taking the columns of A˜ , which contain −1 on the diagonal:



x ∈ R
5
: x = λ1






3
−1
0
0
0






+ λ2






3
0
9
−4
−1






, λ1, λ2 ∈ R



, (2.55)
which is identical to the solution in (2.50) that we obtained by “insight”.
Calculating the Inverse
To compute the inverse A
−1
of A ∈ Rn×n
, we need to find a matrix X
that satisfies AX = In. Then, X = A
−1
. We can write this down as
a set of simultaneous linear equations AX = In, where we solve for
X = [x1| · · · |xn]. We use the augmented matrix notation for a compact
representation of this set of systems of linear equations and obtain

A|In

⇝ · · · ⇝

In|A
−1

. (2.56)
This means that if we bring the augmented equation system into reduced
row-echelon form, we can read out the inverse on the right-hand side of
the equation system. Hence, determining the inverse of a matrix is equivalent to solving systems of linear equations.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
34 Linear Algebra
Example 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)
To determine the inverse of
A =




1 0 2 0
1 1 0 0
1 2 0 1
1 1 1 1




(2.57)
we write down the augmented matrix




1 0 2 0 1 0 0 0
1 1 0 0 0 1 0 0
1 2 0 1 0 0 1 0
1 1 1 1 0 0 0 1




and use Gaussian elimination to bring it into reduced row-echelon form




1 0 0 0 −1 2 −2 2
0 1 0 0 1 −1 2 −2
0 0 1 0 1 −1 1 −1
0 0 0 1 −1 0 −1 2




,
such that the desired inverse is given as its right-hand side:
A
−1 =




−1 2 −2 2
1 −1 2 −2
1 −1 1 −1
−1 0 −1 2




. (2.58)
We can verify that (2.58) is indeed the inverse by performing the multiplication AA−1
and observing that we recover I4.
2.3.4 Algorithms for Solving a System of Linear Equations
In the following, we briefly discuss approaches to solving a system of linear equations of the form Ax = b. We make the assumption that a solution exists. Should there be no solution, we need to resort to approximate
solutions, which we do not cover in this chapter. One way to solve the approximate problem is using the approach of linear regression, which we
discuss in detail in Chapter 9.
In special cases, we may be able to determine the inverse A
−1
, such
that the solution of Ax = b is given as x = A
−1
b. However, this is
only possible if A is a square matrix and invertible, which is often not the
case. Otherwise, under mild assumptions (i.e., A needs to have linearly
independent columns) we can use the transformation
Ax = b ⇐⇒ A
⊤Ax = A
⊤
b ⇐⇒ x = (A
⊤A)
−1A
⊤
b (2.59)
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.4 Vector Spaces 35
and use the Moore-Penrose pseudo-inverse (A
⊤A)
−1A
⊤
to determine the Moore-Penrose
pseudo-inverse solution (2.59) that solves Ax = b, which also corresponds to the minimum norm least-squares solution. A disadvantage of this approach is that
it requires many computations for the matrix-matrix product and computing the inverse of A
⊤A. Moreover, for reasons of numerical precision it
is generally not recommended to compute the inverse or pseudo-inverse.
In the following, we therefore briefly discuss alternative approaches to
solving systems of linear equations.
Gaussian elimination plays an important role when computing determinants (Section 4.1), checking whether a set of vectors is linearly independent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),
computing the rank of a matrix (Section 2.6.2), and determining a basis
of a vector space (Section 2.6.1). Gaussian elimination is an intuitive and
constructive way to solve a system of linear equations with thousands of
variables. However, for systems with millions of variables, it is impractical as the required number of arithmetic operations scales cubically in the
number of simultaneous equations.
In practice, systems of many linear equations are solved indirectly, by either stationary iterative methods, such as the Richardson method, the Jacobi method, the Gauß-Seidel method, and the successive over-relaxation
method, or Krylov subspace methods, such as conjugate gradients, generalized minimal residual, or biconjugate gradients. We refer to the books
by Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann
(2015) for further details.
Let x∗ be a solution of Ax = b. The key idea of these iterative methods
is to set up an iteration of the form
x
(k+1) = Cx(k) + d (2.60)
for suitable C and d that reduces the residual error ∥x
(k+1)−x∗∥ in every
iteration and converges to x∗. We will introduce norms ∥ · ∥, which allow
us to compute similarities between vectors, in Section 3.1.
2.4 Vector Spaces
Thus far, we have looked at systems of linear equations and how to solve
them (Section 2.3). We saw that systems of linear equations can be compactly represented using matrix-vector notation (2.10). In the following,
we will have a closer look at vector spaces, i.e., a structured space in which
vectors live.
In the beginning of this chapter, we informally characterized vectors as
objects that can be added together and multiplied by a scalar, and they
remain objects of the same type. Now, we are ready to formalize this,
and we will start by introducing the concept of a group, which is a set
of elements and an operation defined on these elements that keeps some
structure of the set intact.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
36 Linear Algebra
2.4.1 Groups
Groups play an important role in computer science. Besides providing a
fundamental framework for operations on sets, they are heavily used in
cryptography, coding theory, and graphics.
Definition 2.7 (Group). Consider a set G and an operation ⊗ : G ×G → G
group defined on G. Then G := (G, ⊗) is called a group if the following hold:
closure
1. Closure of G under ⊗: ∀x, y ∈ G : x ⊗ y ∈ G associativity
2. Associativity: ∀x, y, z ∈ G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)
neutral element
inverse element 3. Neutral element: ∃e ∈ G ∀x ∈ G : x ⊗ e = x and e ⊗ x = x
4. Inverse element: ∀x ∈ G ∃y ∈ G : x ⊗ y = e and y ⊗ x = e, where e is
the neutral element. We often write x
−1
to denote the inverse element
of x.
Remark. The inverse element is defined with respect to the operation ⊗
and does not necessarily mean 1
x
. ♢
Abelian group If additionally ∀x, y ∈ G : x ⊗ y = y ⊗ x, then G = (G, ⊗) is an Abelian
group (commutative).
Example 2.10 (Groups)
Let us have a look at some examples of sets with associated operations
and see whether they are groups:
(Z, +) is an Abelian group.
N (N0, +) is not a group: Although (N0, +) possesses a neutral element 0 := N ∪ {0}
(0), the inverse elements are missing.
(Z, ·) is not a group: Although (Z, ·) contains a neutral element (1), the
inverse elements for any z ∈ Z, z ̸= ±1, are missing.
(R, ·) is not a group since 0 does not possess an inverse element.
(R\{0}, ·) is Abelian.
(Rn
, +),(Z
n
, +), n ∈ N are Abelian if + is defined componentwise, i.e.,
(x1, · · · , xn) + (y1, · · · , yn) = (x1 + y1, · · · , xn + yn). (2.61)
Then, (x1, · · · , xn)
−1
:= (−x1, · · · , −xn) is the inverse element and
e = (0, · · · , 0) is the neutral element.
(Rm×n
, +), the set of m × n-matrices is Abelian (with componentwise
addition as defined in (2.61)).
Let us have a closer look at (Rn×n
, ·), i.e., the set of n×n-matrices with
matrix multiplication as defined in (2.13).
– Closure and associativity follow directly from the definition of matrix
multiplication.
– Neutral element: The identity matrix In is the neutral element with
respect to matrix multiplication “·” in (Rn×n
, ·).
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.4 Vector Spaces 37
– Inverse element: If the inverse exists (A is regular), then A
−1
is the
inverse element of A ∈ Rn×n
, and in exactly this case (Rn×n
, ·) is a
group, called the general linear group.
Definition 2.8 (General Linear Group). The set of regular (invertible)
matrices A ∈ Rn×n
is a group with respect to matrix multiplication as
defined in (2.13) and is called general linear group GL(n, R). However, general linear group
since matrix multiplication is not commutative, the group is not Abelian.
2.4.2 Vector Spaces
When we discussed groups, we looked at sets G and inner operations on
G, i.e., mappings G × G → G that only operate on elements in G. In the
following, we will consider sets that in addition to an inner operation +
also contain an outer operation ·, the multiplication of a vector x ∈ G by
a scalar λ ∈ R. We can think of the inner operation as a form of addition,
and the outer operation as a form of scaling. Note that the inner/outer
operations have nothing to do with inner/outer products.
Definition 2.9 (Vector Space). A real-valued vector space V = (V, +, ·) is vector space
a set V with two operations
+ : V × V → V (2.62)
· : R × V → V (2.63)
where
1. (V, +) is an Abelian group
2. Distributivity:
1. ∀λ ∈ R, x, y ∈ V : λ · (x + y) = λ · x + λ · y
2. ∀λ, ψ ∈ R, x ∈ V : (λ + ψ) · x = λ · x + ψ · x
3. Associativity (outer operation): ∀λ, ψ ∈ R, x ∈ V : λ·(ψ·x) = (λψ)·x
4. Neutral element with respect to the outer operation: ∀x ∈ V : 1·x = x
The elements x ∈ V are called vectors. The neutral element of (V, +) is vector
the zero vector 0 = [0, . . . , 0]⊤, and the inner operation + is called vector vector addition
addition. The elements λ ∈ R are called scalars and the outer operation scalar
· is a multiplication by scalars. Note that a scalar product is something multiplication by
different, and we will get to this in Section 3.2 scalars
.
Remark. A “vector multiplication” ab, a, b ∈ Rn
, is not defined. Theoretically, we could define an element-wise multiplication, such that c = ab
with cj = aj bj . This “array multiplication” is common to many programming languages but makes mathematically limited sense using the standard rules for matrix multiplication: By treating vectors as n × 1 matrices
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
38 Linear Algebra
(which we usually do), we can use the matrix multiplication as defined
in (2.13). However, then the dimensions of the vectors do not match. Only
the following multiplications for vectors are defined: ab⊤ ∈ Rn×n
outer product (outer
product), a
⊤b ∈ R (inner/scalar/dot product). ♢
Example 2.11 (Vector Spaces)
Let us have a look at some important examples:
V = Rn
, n ∈ N is a vector space with operations defined as follows:
– Addition: x+y = (x1, . . . , xn)+(y1, . . . , yn) = (x1+y1, . . . , xn+yn)
for all x, y ∈ Rn
– Multiplication by scalars: λx = λ(x1, . . . , xn) = (λx1, . . . , λxn) for
all λ ∈ R, x ∈ Rn
V = Rm×n
, m, n ∈ N is a vector space with
– Addition: A + B =



a11 + b11 · · · a1n + b1n
.
.
.
.
.
.
am1 + bm1 · · · amn + bmn



is defined elementwise for all A, B ∈ V
– Multiplication by scalars: λA =



λa11 · · · λa1n
.
.
.
.
.
.
λam1 · · · λamn



as defined in
Section 2.2. Remember that Rm×n
is equivalent to Rmn
.
V = C, with the standard definition of addition of complex numbers.
Remark. In the following, we will denote a vector space (V, +, ·) by V
when + and · are the standard vector addition and scalar multiplication.
Moreover, we will use the notation x ∈ V for vectors in V to simplify
notation. ♢
Remark. The vector spaces Rn
, Rn×1
, R1×n are only different in the way
we write vectors. In the following, we will not make a distinction between
Rn and Rn×1
column vector , which allows us to write n-tuples as column vectors
x =



x1
.
.
.
xn


 . (2.64)
This simplifies the notation regarding vector space operations. However,
we do distinguish between Rn×1 and R1×n
row vector (the row vectors) to avoid confusion with matrix multiplication. By default, we write x to denote a column vector, and a row vector is denoted by x
⊤ transpose , the transpose of x. ♢
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.4 Vector Spaces 39
2.4.3 Vector Subspaces
In the following, we will introduce vector subspaces. Intuitively, they are
sets contained in the original vector space with the property that when
we perform vector space operations on elements within this subspace, we
will never leave it. In this sense, they are “closed”. Vector subspaces are a
key idea in machine learning. For example, Chapter 10 demonstrates how
to use vector subspaces for dimensionality reduction.
Definition 2.10 (Vector Subspace). Let V = (V, +, ·) be a vector space
and U ⊆ V, U ̸= ∅. Then U = (U, +, ·) is called vector subspace of V (or vector subspace
linear subspace) if U is a vector space with the vector space operations + linear subspace
and · restricted to U ×U and R×U. We write U ⊆ V to denote a subspace
U of V .
If U ⊆ V and V is a vector space, then U naturally inherits many properties directly from V because they hold for all x ∈ V, and in particular for
all x ∈ U ⊆ V. This includes the Abelian group properties, the distributivity, the associativity and the neutral element. To determine whether
(U, +, ·) is a subspace of V we still do need to show
1. U ̸= ∅, in particular: 0 ∈ U
2. Closure of U:
a. With respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx ∈ U.
b. With respect to the inner operation: ∀x, y ∈ U : x + y ∈ U.
Example 2.12 (Vector Subspaces)
Let us have a look at some examples:
For every vector space V , the trivial subspaces are V itself and {0}.
Only example D in Figure 2.1 is a subspace of R2
(with the usual inner/
outer operations). In A and C, the closure property is violated; B does
not contain 0.
The solution set of a homogeneous system of linear equations Ax = 0
with n unknowns x = [x1, . . . , xn]
⊤ is a subspace of Rn
.
The solution of an inhomogeneous system of linear equations Ax =
b, b ̸= 0 is not a subspace of Rn
.
The intersection of arbitrarily many subspaces is a subspace itself.
Figure 2.1 Not all
subsets of R2 are
subspaces. In A and
C, the closure
property is violated;
B does not contain
0. Only D is a
subspace.
0 0 0 0
A
B
C
D
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
40 Linear Algebra
Remark. Every subspace U ⊆ (Rn
, +, ·) is the solution space of a homogeneous system of linear equations Ax = 0 for x ∈ Rn
. ♢
2.5 Linear Independence
In the following, we will have a close look at what we can do with vectors
(elements of the vector space). In particular, we can add vectors together
and multiply them with scalars. The closure property guarantees that we
end up with another vector in the same vector space. It is possible to find
a set of vectors with which we can represent every vector in the vector
space by adding them together and scaling them. This set of vectors is
a basis, and we will discuss them in Section 2.6.1. Before we get there,
we will need to introduce the concepts of linear combinations and linear
independence.
Definition 2.11 (Linear Combination). Consider a vector space V and a
finite number of vectors x1, . . . , xk ∈ V . Then, every v ∈ V of the form
v = λ1x1 + · · · + λkxk =
X
k
i=1
λixi ∈ V (2.65)
linear combination with λ1, . . . , λk ∈ R is a linear combination of the vectors x1, . . . , xk.
The 0-vector can always be written as the linear combination of k vectors x1, . . . , xk because 0 =
Pk
i=1 0xi
is always true. In the following,
we are interested in non-trivial linear combinations of a set of vectors to
represent 0, i.e., linear combinations of vectors x1, . . . , xk, where not all
coefficients λi
in (2.65) are 0.
Definition 2.12 (Linear (In)dependence). Let us consider a vector space
V with k ∈ N and x1, . . . , xk ∈ V . If there is a non-trivial linear combination, such that 0 =
Pk
i=1 λixi with at least one λi ̸= 0, the vectors
linearly dependent x1, . . . , xk are linearly dependent. If only the trivial solution exists, i.e.,
linearly λ1 = . . . = λk = 0 the vectors x1, . . . , xk are linearly independent.
independent
Linear independence is one of the most important concepts in linear
algebra. Intuitively, a set of linearly independent vectors consists of vectors
that have no redundancy, i.e., if we remove any of those vectors from
the set, we will lose something. Throughout the next sections, we will
formalize this intuition more.
Example 2.13 (Linearly Dependent Vectors)
A geographic example may help to clarify the concept of linear independence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is
might say ,“You can get to Kigali by first going 506 km Northwest to Kampala (Uganda) and then 374 km Southwest.”. This is sufficient information
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.5 Linear Independence 41
to describe the location of Kigali because the geographic coordinate system may be considered a two-dimensional vector space (ignoring altitude
and the Earth’s curved surface). The person may add, “It is about 751 km
West of here.” Although this last statement is true, it is not necessary to
find Kigali given the previous information (see Figure 2.2 for an illustration). In this example, the “506 km Northwest” vector (blue) and the
“374 km Southwest” vector (purple) are linearly independent. This means
the Southwest vector cannot be described in terms of the Northwest vector, and vice versa. However, the third “751 km West” vector (black) is a
linear combination of the other two vectors, and it makes the set of vectors linearly dependent. Equivalently, given “751 km West” and “374 km
Southwest” can be linearly combined to obtain “506 km Northwest”.
Figure 2.2
Geographic example
(with crude
approximations to
cardinal directions)
of linearly
dependent vectors
in a
two-dimensional
space (plane).
506 km Northwest 751 km West 374 km Southwest 374 km SouthwestKampala Nairobi Kigali
Remark. The following properties are useful to find out whether vectors
are linearly independent:
k vectors are either linearly dependent or linearly independent. There
is no third option.
If at least one of the vectors x1, . . . , xk is 0 then they are linearly dependent. The same holds if two vectors are identical.
The vectors {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k}, k ⩾ 2, are linearly
dependent if and only if (at least) one of them is a linear combination
of the others. In particular, if one vector is a multiple of another vector,
i.e., xi = λxj , λ ∈ R then the set {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k}
is linearly dependent.
A practical way of checking whether vectors x1, . . . , xk ∈ V are linearly
independent is to use Gaussian elimination: Write all vectors as columns
of a matrix A and perform Gaussian elimination until the matrix is in
row echelon form (the reduced row-echelon form is unnecessary here):
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
42 Linear Algebra
– The pivot columns indicate the vectors, which are linearly independent of the vectors on the left. Note that there is an ordering of vectors when the matrix is built.
– The non-pivot columns can be expressed as linear combinations of
the pivot columns on their left. For instance, the row-echelon form

1 3 0
0 0 2
(2.66)
tells us that the first and third columns are pivot columns. The second column is a non-pivot column because it is three times the first
column.
All column vectors are linearly independent if and only if all columns
are pivot columns. If there is at least one non-pivot column, the columns
(and, therefore, the corresponding vectors) are linearly dependent.
♢
Example 2.14
Consider R4 with
x1 =




1
2
−3
4




, x2 =




1
1
0
2




, x3 =




−1
−2
1
1




. (2.67)
To check whether they are linearly dependent, we follow the general approach and solve
λ1x1 + λ2x2 + λ3x3 = λ1




1
2
−3
4




+ λ2




1
1
0
2




+ λ3




−1
−2
1
1



 = 0 (2.68)
for λ1, . . . , λ3. We write the vectors xi
, i = 1, 2, 3, as the columns of a
matrix and apply elementary row operations until we identify the pivot
columns:




1 1 −1
2 1 −2
−3 0 1
4 2 1



 ⇝ · · · ⇝




1 1 −1
0 1 0
0 0 1
0 0 0




. (2.69)
Here, every column of the matrix is a pivot column. Therefore, there is no
non-trivial solution, and we require λ1 = 0, λ2 = 0, λ3 = 0 to solve the
equation system. Hence, the vectors x1, x2, x3 are linearly independent.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.5 Linear Independence 43
Remark. Consider a vector space V with k linearly independent vectors
b1, . . . , bk and m linear combinations
x1 =
X
k
i=1
λi1bi
,
.
.
.
xm =
X
k
i=1
λimbi
.
(2.70)
Defining B = [b1, . . . , bk] as the matrix whose columns are the linearly
independent vectors b1, . . . , bk, we can write
xj = Bλj , λj =



λ1j
.
.
.
λkj


 , j = 1, . . . , m , (2.71)
in a more compact form.
We want to test whether x1, . . . , xm are linearly independent. For this
purpose, we follow the general approach of testing when Pm
j=1 ψjxj = 0.
With (2.71), we obtain
Xm
j=1
ψjxj =
Xm
j=1
ψjBλj = B
Xm
j=1
ψjλj . (2.72)
This means that {x1, . . . , xm} are linearly independent if and only if the
column vectors {λ1, . . . , λm} are linearly independent.
♢
Remark. In a vector space V , m linear combinations of k vectors x1, . . . , xk
are linearly dependent if m > k. ♢
Example 2.15
Consider a set of linearly independent vectors b1, b2, b3, b4 ∈ Rn and
x1 = b1 − 2b2 + b3 − b4
x2 = −4b1 − 2b2 + 4b4
x3 = 2b1 + 3b2 − b3 − 3b4
x4 = 17b1 − 10b2 + 11b3 + b4
. (2.73)
Are the vectors x1, . . . , x4 ∈ Rn
linearly independent? To answer this
question, we investigate whether the column vectors







1
−2
1
−1




,




−4
−2
0
4




,




2
3
−1
−3




,




17
−10
11
1







(2.74)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
44 Linear Algebra
are linearly independent. The reduced row-echelon form of the corresponding linear equation system with coefficient matrix
A =




1 −4 2 17
−2 −2 3 −10
1 0 −1 11
−1 4 −3 1




(2.75)
is given as




1 0 0 −7
0 1 0 −15
0 0 1 −18
0 0 0 0




. (2.76)
We see that the corresponding linear equation system is non-trivially solvable: The last column is not a pivot column, and x4 = −7x1−15x2−18x3.
Therefore, x1, . . . , x4 are linearly dependent as x4 can be expressed as a
linear combination of x1, . . . , x3.
2.6 Basis and Rank
In a vector space V , we are particularly interested in sets of vectors A that
possess the property that any vector v ∈ V can be obtained by a linear
combination of vectors in A. These vectors are special vectors, and in the
following, we will characterize them.
2.6.1 Generating Set and Basis
Definition 2.13 (Generating Set and Span). Consider a vector space V =
(V, +, ·) and set of vectors A = {x1, . . . , xk} ⊆ V. If every vector v ∈
V can be expressed as a linear combination of x1, . . . , xk, A is called a
generating set generating set of V . The set of all linear combinations of vectors in A is
span called the span of A. If A spans the vector space V , we write V = span[A]
or V = span[x1, . . . , xk].
Generating sets are sets of vectors that span vector (sub)spaces, i.e.,
every vector can be represented as a linear combination of the vectors
in the generating set. Now, we will be more specific and characterize the
smallest generating set that spans a vector (sub)space.
Definition 2.14 (Basis). Consider a vector space V = (V, +, ·) and A ⊆
minimal V. A generating set A of V is called minimal if there exists no smaller set
A˜ ⊊ A ⊆ V that spans V . Every linearly independent generating set of V
basis is minimal and is called a basis of V .
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.6 Basis and Rank 45
Let V = (V, +, ·) be a vector space and B ⊆ V, B ̸= ∅. Then, the
following statements are equivalent: A basis is a minimal
generating set and a
maximal linearly
independent set of
vectors.
B is a basis of V .
B is a minimal generating set.
B is a maximal linearly independent set of vectors in V , i.e., adding any
other vector to this set will make it linearly dependent.
Every vector x ∈ V is a linear combination of vectors from B, and every
linear combination is unique, i.e., with
x =
X
k
i=1
λibi =
X
k
i=1
ψibi (2.77)
and λi
, ψi ∈ R, bi ∈ B it follows that λi = ψi
, i = 1, . . . , k.
Example 2.16
In R3
, the canonical/standard basis is canonical basis
B =





1
0
0

 ,


0
1
0

 ,


0
0
1





. (2.78)
Different bases in R3 are
B1 =





1
0
0

 ,


1
1
0

 ,


1
1
1





, B2 =





0.5
0.8
0.4

 ,


1.8
0.3
0.3

 ,


−2.2
−1.3
3.5





. (2.79)
The set
A =







1
2
3
4




,




2
−1
0
2




,




1
1
0
−4







(2.80)
is linearly independent, but not a generating set (and no basis) of R4
:
For instance, the vector [1, 0, 0, 0]⊤ cannot be obtained by a linear combination of elements in A.
Remark. Every vector space V possesses a basis B. The preceding examples show that there can be many bases of a vector space V , i.e., there is
no unique basis. However, all bases possess the same number of elements,
the basis vectors. ♢ basis vector
We only consider finite-dimensional vector spaces V . In this case, the
dimension of V is the number of basis vectors of V , and we write dim(V ). dimension
If U ⊆ V is a subspace of V , then dim(U) ⩽ dim(V ) and dim(U) =
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
46 Linear Algebra
dim(V ) if and only if U = V . Intuitively, the dimension of a vector space
can be thought of as the number of independent directions in this vector
The dimension of a space.
vector space
corresponds to the
number of its basis
vectors.
Remark. The dimension of a vector space is not necessarily the number
of elements in a vector. For instance, the vector space V = span[
0
1

] is
one-dimensional, although the basis vector possesses two elements. ♢
Remark. A basis of a subspace U = span[x1, . . . , xm] ⊆ Rn
can be found
by executing the following steps:
1. Write the spanning vectors as columns of a matrix A
2. Determine the row-echelon form of A.
3. The spanning vectors associated with the pivot columns are a basis of
U.
♢
Example 2.17 (Determining a Basis)
For a vector subspace U ⊆ R5
, spanned by the vectors
x1 =






1
2
−1
−1
−1






, x2 =






2
−1
1
2
−2






, x3 =






3
−4
3
5
−3






, x4 =






−1
8
−5
−6
1






∈ R
5
, (2.81)
we are interested in finding out which vectors x1, . . . , x4 are a basis for U.
For this, we need to check whether x1, . . . , x4 are linearly independent.
Therefore, we need to solve
X
4
i=1
λixi = 0 , (2.82)
which leads to a homogeneous system of equations with matrix

x1, x2, x3, x4

=






1 2 3 −1
2 −1 −4 8
−1 1 3 −5
−1 2 5 −6
−1 −2 −3 1






. (2.83)
With the basic transformation rules for systems of linear equations, we
obtain the row-echelon form






1 2 3 −1
2 −1 −4 8
−1 1 3 −5
−1 2 5 −6
−1 −2 −3 1






⇝ · · · ⇝






1 2 3 −1
0 1 2 −2
0 0 0 1
0 0 0 0
0 0 0 0






.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.6 Basis and Rank 47
Since the pivot columns indicate which set of vectors is linearly independent, we see from the row-echelon form that x1, x2, x4 are linearly independent (because the system of linear equations λ1x1 + λ2x2 + λ4x4 = 0
can only be solved with λ1 = λ2 = λ4 = 0). Therefore, {x1, x2, x4} is a
basis of U.
2.6.2 Rank
The number of linearly independent columns of a matrix A ∈ Rm×n
equals the number of linearly independent rows and is called the rank rank
of A and is denoted by rk(A).
Remark. The rank of a matrix has some important properties:
rk(A) = rk(A
⊤
), i.e., the column rank equals the row rank.
The columns of A ∈ Rm×n
span a subspace U ⊆ Rm with dim(U) =
rk(A). Later we will call this subspace the image or range. A basis of
U can be found by applying Gaussian elimination to A to identify the
pivot columns.
The rows of A ∈ Rm×n
span a subspace W ⊆ Rn with dim(W) =
rk(A). A basis of W can be found by applying Gaussian elimination to
A
⊤
.
For all A ∈ Rn×n
it holds that A is regular (invertible) if and only if
rk(A) = n.
For all A ∈ Rm×n and all b ∈ Rm it holds that the linear equation
system Ax = b can be solved if and only if rk(A) = rk(A|b), where
A|b denotes the augmented system.
For A ∈ Rm×n
the subspace of solutions for Ax = 0 possesses dimension n − rk(A). Later, we will call this subspace the kernel or the null kernel
null space space.
A matrix A ∈ Rm×n has full rank if its rank equals the largest possible full rank
rank for a matrix of the same dimensions. This means that the rank of
a full-rank matrix is the lesser of the number of rows and columns, i.e.,
rk(A) = min(m, n). A matrix is said to be rank deficient if it does not rank deficient
have full rank.
♢
Example 2.18 (Rank)
A =


1 0 1
0 1 1
0 0 0

.
A has two linearly independent rows/columns so that rk(A) = 2.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
48 Linear Algebra
A =


1 2 1
−2 −3 1
3 5 0

 .
We use Gaussian elimination to determine the rank:


1 2 1
−2 −3 1
3 5 0

 ⇝ · · · ⇝


1 2 1
0 1 3
0 0 0

 . (2.84)
Here, we see that the number of linearly independent rows and columns
is 2, such that rk(A) = 2.
2.7 Linear Mappings
In the following, we will study mappings on vector spaces that preserve
their structure, which will allow us to define the concept of a coordinate.
In the beginning of the chapter, we said that vectors are objects that can be
added together and multiplied by a scalar, and the resulting object is still
a vector. We wish to preserve this property when applying the mapping:
Consider two real vector spaces V, W. A mapping Φ : V → W preserves
the structure of the vector space if
Φ(x + y) = Φ(x) + Φ(y) (2.85)
Φ(λx) = λΦ(x) (2.86)
for all x, y ∈ V and λ ∈ R. We can summarize this in the following
definition:
Definition 2.15 (Linear Mapping). For vector spaces V, W, a mapping
linear mapping Φ : V → W is called a linear mapping (or vector space homomorphism/
vector space
homomorphism
linear transformation) if
linear
transformation
∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y). (2.87)
It turns out that we can represent linear mappings as matrices (Section 2.7.1). Recall that we can also collect a set of vectors as columns of a
matrix. When working with matrices, we have to keep in mind what the
matrix represents: a linear mapping or a collection of vectors. We will see
more about linear mappings in Chapter 4. Before we continue, we will
briefly introduce special mappings.
Definition 2.16 (Injective, Surjective, Bijective). Consider a mapping Φ :
V → W, where V, W can be arbitrary sets. Then Φ is called
injective
Injective if ∀x, y ∈ V : Φ(x) = Φ(y) =⇒ x = y.
surjective
Surjective if Φ(V) = W. bijective
Bijective if it is injective and surjective.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 49
If Φ is surjective, then every element in W can be “reached” from V
using Φ. A bijective Φ can be “undone”, i.e., there exists a mapping Ψ :
W → V so that Ψ ◦ Φ(x) = x. This mapping Ψ is then called the inverse
of Φ and normally denoted by Φ
−1
.
With these definitions, we introduce the following special cases of linear
mappings between vector spaces V and W:
isomorphism
Isomorphism: Φ : V → W linear and bijective endomorphism
Endomorphism: Φ : V → V linear automorphism
Automorphism: Φ : V → V linear and bijective
We define idV : V → V , x 7→ x as the identity mapping or identity identity mapping
identity
automorphism
automorphism in V .
Example 2.19 (Homomorphism)
The mapping Φ : R2 → C, Φ(x) = x1 + ix2, is a homomorphism:
Φ
x1
x2

+

y1
y2
 = (x1 + y1) + i(x2 + y2) = x1 + ix2 + y1 + iy2
= Φ x1
x2
 + Φ y1
y2

Φ

λ

x1
x2
 = λx1 + λix2 = λ(x1 + ix2) = λΦ
x1
x2
 .
(2.88)
This also justifies why complex numbers can be represented as tuples in
R2
: There is a bijective linear mapping that converts the elementwise addition of tuples in R2
into the set of complex numbers with the corresponding addition. Note that we only showed linearity, but not the bijection.
Theorem 2.17 (Theorem 3.59 in Axler (2015)). Finite-dimensional vector
spaces V and W are isomorphic if and only if dim(V ) = dim(W).
Theorem 2.17 states that there exists a linear, bijective mapping between two vector spaces of the same dimension. Intuitively, this means
that vector spaces of the same dimension are kind of the same thing, as
they can be transformed into each other without incurring any loss.
Theorem 2.17 also gives us the justification to treat Rm×n
(the vector
space of m × n-matrices) and Rmn (the vector space of vectors of length
mn) the same, as their dimensions are mn, and there exists a linear, bijective mapping that transforms one into the other.
Remark. Consider vector spaces V, W, X. Then:
For linear mappings Φ : V → W and Ψ : W → X, the mapping
Ψ ◦ Φ : V → X is also linear.
If Φ : V → W is an isomorphism, then Φ
−1
: W → V is an isomorphism, too.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
50 Linear Algebra
Figure 2.1 Two
different coordinate
systems defined by
two sets of basis
vectors. A vector x
has different
coordinate
representations
depending on which
coordinate system is
chosen.
x x
e1
e2
b1
b2
If Φ : V → W, Ψ : V → W are linear, then Φ + Ψ and λΦ, λ ∈ R, are
linear, too.
♢
2.7.1 Matrix Representation of Linear Mappings
Any n-dimensional vector space is isomorphic to Rn
(Theorem 2.17). We
consider a basis {b1, . . . , bn} of an n-dimensional vector space V . In the
following, the order of the basis vectors will be important. Therefore, we
write
B = (b1, . . . , bn) (2.89)
ordered basis and call this n-tuple an ordered basis of V .
Remark (Notation). We are at the point where notation gets a bit tricky.
Therefore, we summarize some parts here. B = (b1, . . . , bn) is an ordered
basis, B = {b1, . . . , bn} is an (unordered) basis, and B = [b1, . . . , bn] is a
matrix whose columns are the vectors b1, . . . , bn. ♢
Definition 2.18 (Coordinates). Consider a vector space V and an ordered
basis B = (b1, . . . , bn) of V . For any x ∈ V we obtain a unique representation (linear combination)
x = α1b1 + . . . + αnbn (2.90)
coordinate of x with respect to B. Then α1, . . . , αn are the coordinates of x with
respect to B, and the vector
α =



α1
.
.
.
αn


 ∈ R
n
(2.91)
coordinate vector is the coordinate vector/coordinate representation of x with respect to the
coordinate
representation
ordered basis B.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 51
A basis effectively defines a coordinate system. We are familiar with the
Cartesian coordinate system in two dimensions, which is spanned by the
canonical basis vectors e1, e2. In this coordinate system, a vector x ∈ R2
has a representation that tells us how to linearly combine e1 and e2 to
obtain x. However, any basis of R2 defines a valid coordinate system,
and the same vector x from before may have a different coordinate representation in the (b1, b2) basis. In Figure 2.1, the coordinates of x with
respect to the standard basis (e1, e2) is [2, 2]⊤. However, with respect to
the basis (b1, b2) the same vector x is represented as [1.09, 0.72]⊤, i.e.,
x = 1.09b1 + 0.72b2. In the following sections, we will discover how to
obtain this representation.
Example 2.20
Let us have a look at a geometric vector x ∈ R2 with coordinates [2, 3]⊤ Figure 2.2
Different coordinate
representations of a
vector x, depending
on the choice of
basis.
e1
e2 b2
b1
x = −
1
2b1 +
5
2b2
x = 2e1 + 3e2
with respect to the standard basis (e1, e2) of R2
. This means, we can write
x = 2e1 + 3e2. However, we do not have to choose the standard basis to
represent this vector. If we use the basis vectors b1 = [1, −1]⊤, b2 = [1, 1]⊤
we will obtain the coordinates 1
2
[−1, 5]⊤ to represent the same vector with
respect to (b1, b2) (see Figure 2.2).
Remark. For an n-dimensional vector space V and an ordered basis B
of V , the mapping Φ : Rn → V , Φ(ei) = bi
, i = 1, . . . , n, is linear
(and because of Theorem 2.17 an isomorphism), where (e1, . . . , en) is
the standard basis of Rn
.
♢
Now we are ready to make an explicit connection between matrices and
linear mappings between finite-dimensional vector spaces.
Definition 2.19 (Transformation Matrix). Consider vector spaces V, W
with corresponding (ordered) bases B = (b1, . . . , bn) and C = (c1, . . . , cm).
Moreover, we consider a linear mapping Φ : V → W. For j ∈ {1, . . . , n},
Φ(bj ) = α1jc1 + · · · + αmjcm =
Xm
i=1
αijci (2.92)
is the unique representation of Φ(bj ) with respect to C. Then, we call the
m × n-matrix AΦ, whose elements are given by
AΦ(i, j) = αij , (2.93)
the transformation matrix of Φ (with respect to the ordered bases B of V transformation
and C of W matrix ).
The coordinates of Φ(bj ) with respect to the ordered basis C of W
are the j-th column of AΦ. Consider (finite-dimensional) vector spaces
V, W with ordered bases B, C and a linear mapping Φ : V → W with
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
52 Linear Algebra
transformation matrix AΦ. If xˆ is the coordinate vector of x ∈ V with
respect to B and yˆ the coordinate vector of y = Φ(x) ∈ W with respect
to C, then
yˆ = AΦxˆ . (2.94)
This means that the transformation matrix can be used to map coordinates
with respect to an ordered basis in V to coordinates with respect to an
ordered basis in W.
Example 2.21 (Transformation Matrix)
Consider a homomorphism Φ : V → W and ordered bases B =
(b1, . . . , b3) of V and C = (c1, . . . , c4) of W. With
Φ(b1) = c1 − c2 + 3c3 − c4
Φ(b2) = 2c1 + c2 + 7c3 + 2c4
Φ(b3) = 3c2 + c3 + 4c4
(2.95)
the transformation matrix
P
AΦ with respect to B and C satisfies Φ(bk) =
4
i=1 αikci for k = 1, . . . , 3 and is given as
AΦ = [α1, α2, α3] =




1 2 0
−1 1 3
3 7 1
−1 2 4




, (2.96)
where the αj , j = 1, 2, 3, are the coordinate vectors of Φ(bj ) with respect
to C.
Example 2.22 (Linear Transformations of Vectors)
Figure 2.3 Three
examples of linear
transformations of
the vectors shown
as dots in (a);
(b) Rotation by 45◦;
(c) Stretching of the
horizontal
coordinates by 2;
(d) Combination of
reflection, rotation
and stretching.
(a) Original data. (b) Rotation by 45◦. (c) Stretch along the
horizontal axis.
(d) General linear
mapping.
We consider three linear transformations of a set of vectors in R2 with
the transformation matrices
A1 =

cos( π
4
) − sin( π
4
)
sin( π
4
) cos( π
4
)

, A2 =

2 0
0 1
, A3 =
1
2

3 −1
1 −1

. (2.97)
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 53
Figure 2.3 gives three examples of linear transformations of a set of vectors. Figure 2.3(a) shows 400 vectors in R2
, each of which is represented
by a dot at the corresponding (x1, x2)-coordinates. The vectors are arranged in a square. When we use matrix A1 in (2.97) to linearly transform
each of these vectors, we obtain the rotated square in Figure 2.3(b). If we
apply the linear mapping represented by A2, we obtain the rectangle in
Figure 2.3(c) where each x1-coordinate is stretched by 2. Figure 2.3(d)
shows the original square from Figure 2.3(a) when linearly transformed
using A3, which is a combination of a reflection, a rotation, and a stretch.
2.7.2 Basis Change
In the following, we will have a closer look at how transformation matrices
of a linear mapping Φ : V → W change if we change the bases in V and
W. Consider two ordered bases
B = (b1, . . . , bn), B˜ = (˜b1, . . . ,
˜bn) (2.98)
of V and two ordered bases
C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.99)
of W. Moreover, AΦ ∈ Rm×n
is the transformation matrix of the linear
mapping Φ : V → W with respect to the bases B and C, and A˜
Φ ∈ Rm×n
is the corresponding transformation mapping with respect to B˜ and C˜.
In the following, we will investigate how A and A˜ are related, i.e., how/
whether we can transform AΦ into A˜
Φ if we choose to perform a basis
change from B, C to B, ˜ C˜.
Remark. We effectively get different coordinate representations of the
identity mapping idV . In the context of Figure 2.2, this would mean to
map coordinates with respect to (e1, e2) onto coordinates with respect to
(b1, b2) without changing the vector x. By changing the basis and correspondingly the representation of vectors, the transformation matrix with
respect to this new basis can have a particularly simple form that allows
for straightforward computation. ♢
Example 2.23 (Basis Change)
Consider a transformation matrix
A =

2 1
1 2
(2.100)
with respect to the canonical basis in R2
. If we define a new basis
B = (
1
1

,

1
−1

) (2.101)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
54 Linear Algebra
we obtain a diagonal transformation matrix
A˜ =

3 0
0 1
(2.102)
with respect to B, which is easier to work with than A.
In the following, we will look at mappings that transform coordinate
vectors with respect to one basis into coordinate vectors with respect to
a different basis. We will state our main result first and then provide an
explanation.
Theorem 2.20 (Basis Change). For a linear mapping Φ : V → W, ordered
bases
B = (b1, . . . , bn), B˜ = (˜b1, . . . ,
˜bn) (2.103)
of V and
C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.104)
of W, and a transformation matrix AΦ of Φ with respect to B and C, the
corresponding transformation matrix A˜
Φ with respect to the bases B˜ and C˜
is given as
A˜
Φ = T
−1AΦS . (2.105)
Here, S ∈ Rn×n
is the transformation matrix of idV that maps coordinates
with respect to B˜ onto coordinates with respect to B, and T ∈ Rm×m is the
transformation matrix of idW that maps coordinates with respect to C˜ onto
coordinates with respect to C.
Proof Following Drumm and Weil (2001), we can write the vectors of
the new basis B˜ of V as a linear combination of the basis vectors of B,
such that
˜bj = s1jb1 + · · · + snjbn =
Xn
i=1
sijbi
, j = 1, . . . , n . (2.106)
Similarly, we write the new basis vectors C˜ of W as a linear combination
of the basis vectors of C, which yields
c˜k = t1kc1 + · · · + tmkcm =
Xm
l=1
tlkcl
, k = 1, . . . , m . (2.107)
We define S = ((sij )) ∈ Rn×n as the transformation matrix that maps
coordinates with respect to B˜ onto coordinates with respect to B and
T = ((tlk)) ∈ Rm×m as the transformation matrix that maps coordinates
with respect to C˜ onto coordinates with respect to C. In particular, the jth
column of S is the coordinate representation of ˜bj with respect to B and
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 55
the kth column of T is the coordinate representation of c˜k with respect to
C. Note that both S and T are regular.
We are going to look at Φ(˜bj ) from two perspectives. First, applying the
mapping Φ, we get that for all j = 1, . . . , n
Φ(˜bj ) = Xm
k=1
a˜kjc˜k
| {z }
∈W
(2.107) =
Xm
k=1
a˜kjXm
l=1
tlkcl =
Xm
l=1 Xm
k=1
tlka˜kj!
cl
, (2.108)
where we first expressed the new basis vectors c˜k ∈ W as linear combinations of the basis vectors cl ∈ W and then swapped the order of
summation.
Alternatively, when we express the ˜bj ∈ V as linear combinations of
bj ∈ V , we arrive at
Φ(˜bj )
(2.106) = Φ Xn
i=1
sijbi
!
=
Xn
i=1
sijΦ(bi) =
Xn
i=1
sijXm
l=1
alicl (2.109a)
=
Xm
l=1 Xn
i=1
alisij!
cl
, j = 1, . . . , n , (2.109b)
where we exploited the linearity of Φ. Comparing (2.108) and (2.109b),
it follows for all j = 1, . . . , n and l = 1, . . . , m that
Xm
k=1
tlka˜kj =
Xn
i=1
alisij (2.110)
and, therefore,
T A˜
Φ = AΦS ∈ R
m×n
, (2.111)
such that
A˜
Φ = T
−1AΦS , (2.112)
which proves Theorem 2.20.
Theorem 2.20 tells us that with a basis change in V (B is replaced with
B˜) and W (C is replaced with C˜), the transformation matrix AΦ of a
linear mapping Φ : V → W is replaced by an equivalent matrix A˜
Φ with
A˜
Φ = T
−1AΦS. (2.113)
Figure 2.2 illustrates this relation: Consider a homomorphism Φ : V → W
and ordered bases B, B˜ of V and C, C˜ of W. The mapping ΦCB is an instantiation of Φ and maps basis vectors of B onto linear combinations of
basis vectors of C. Assume that we know the transformation matrix AΦ
of ΦCB with respect to the ordered bases B, C. When we perform a basis
change from B to B˜ in V and from C to C˜ in W, we can determine the
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
56 Linear Algebra
Figure 2.2 For a
homomorphism
Φ : V → W and
ordered bases B, B˜
of V and C, C˜ of W
(marked in blue),
we can express the
mapping ΦC˜B˜ with
respect to the bases
B, ˜ C˜ equivalently as
a composition of the
homomorphisms
ΦC˜B˜ =
ΞCC˜ ◦ ΦCB ◦ ΨBB˜
with respect to the
bases in the
subscripts. The
corresponding
transformation
matrices are in red.
V W
B
B˜ C˜
C
Φ
ΦCB
ΦC˜B˜
ΨBB˜ S T ΞCC˜
A˜ Φ
AΦ
V W
B
B˜ C˜
C
Φ
ΦCB
ΦC˜B˜
ΨBB˜ ΞCC˜ = Ξ−1 S T CC˜
−1
A˜ Φ
AΦ
Vector spaces
Ordered bases
corresponding transformation matrix A˜
Φ as follows: First, we find the matrix representation of the linear mapping ΨBB˜ : V → V that maps coordinates with respect to the new basis B˜ onto the (unique) coordinates with
respect to the “old” basis B (in V ). Then, we use the transformation matrix AΦ of ΦCB : V → W to map these coordinates onto the coordinates
with respect to C in W. Finally, we use a linear mapping ΞCC˜ : W → W
to map the coordinates with respect to C onto coordinates with respect to
C˜. Therefore, we can express the linear mapping ΦC˜B˜ as a composition of
linear mappings that involve the “old” basis:
ΦC˜B˜ = ΞCC˜ ◦ ΦCB ◦ ΨBB˜ = Ξ−1
CC˜ ◦ ΦCB ◦ ΨBB˜ . (2.114)
Concretely, we use ΨBB˜ = idV and ΞCC˜ = idW , i.e., the identity mappings
that map vectors onto themselves, but with respect to a different basis.
Definition 2.21 (Equivalence). Two matrices A, A˜ ∈ Rm×n
equivalent are equivalent
if there exist regular matrices S ∈ Rn×n and T ∈ Rm×m, such that
A˜ = T
−1AS.
Definition 2.22 (Similarity). Two matrices A, A˜ ∈ Rn×n
similar are similar if
there exists a regular matrix S ∈ Rn×n with A˜ = S
−1AS
Remark. Similar matrices are always equivalent. However, equivalent matrices are not necessarily similar. ♢
Remark. Consider vector spaces V, W, X. From the remark that follows
Theorem 2.17, we already know that for linear mappings Φ : V → W
and Ψ : W → X the mapping Ψ ◦ Φ : V → X is also linear. With
transformation matrices AΦ and AΨ of the corresponding mappings, the
overall transformation matrix is AΨ◦Φ = AΨAΦ. ♢
In light of this remark, we can look at basis changes from the perspective of composing linear mappings:
AΦ is the transformation matrix of a linear mapping ΦCB : V → W
with respect to the bases B, C.
A˜
Φ is the transformation matrix of the linear mapping ΦC˜B˜ : V → W
with respect to the bases B, ˜ C˜.
S is the transformation matrix of a linear mapping ΨBB˜ : V → V
(automorphism) that represents B˜ in terms of B. Normally, Ψ = idV is
the identity mapping in V .
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 57
T is the transformation matrix of a linear mapping ΞCC˜ : W → W
(automorphism) that represents C˜ in terms of C. Normally, Ξ = idW is
the identity mapping in W.
If we (informally) write down the transformations just in terms of bases,
then AΦ : B → C, A˜
Φ : B˜ → C˜, S : B˜ → B, T : C˜ → C and
T
−1
: C → C˜, and
B˜ → C˜ = B˜ → B→ C → C˜ (2.115)
A˜
Φ = T
−1AΦS . (2.116)
Note that the execution order in (2.116) is from right to left because vectors are multiplied at the right-hand side so that x 7→ Sx 7→ AΦ(Sx) 7→
T
−1

AΦ(Sx)

= A˜
Φx.
Example 2.24 (Basis Change)
Consider a linear mapping Φ : R3 → R4 whose transformation matrix is
AΦ =




1 2 0
−1 1 3
3 7 1
−1 2 4




(2.117)
with respect to the standard bases
B = (


1
0
0

 ,


0
1
0

 ,


0
0
1

), C = (




1
0
0
0




,




0
1
0
0




,




0
0
1
0




,




0
0
0
1




). (2.118)
We seek the transformation matrix A˜
Φ of Φ with respect to the new bases
B˜ = (


1
1
0

 ,


0
1
1

 ,


1
0
1

) ∈ R
3
, C˜ = (




1
1
0
0




,




1
0
1
0




,




0
1
1
0




,




1
0
0
1




). (2.119)
Then,
S =


1 0 1
1 1 0
0 1 1

 , T =




1 1 0 1
1 0 1 0
0 1 1 0
0 0 0 1




, (2.120)
where the ith column of S is the coordinate representation of ˜bi
in
terms of the basis vectors of B. Since B is the standard basis, the coordinate representation is straightforward to find. For a general basis B,
we would need to solve a linear equation system to find the λi such that
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)
58 Linear Algebra
P3
i=1 λibi = ˜bj , j = 1, . . . , 3. Similarly, the jth column of T is the coordinate representation of c˜j in terms of the basis vectors of C.
Therefore, we obtain
A˜
Φ = T
−1AΦS =
1
2




1 1 −1 −1
1 −1 1 −1
−1 1 1 1
0 0 0 2








3 2 1
0 4 2
10 8 4
1 6 3




(2.121a)
=




−4 −4 −2
6 0 0
4 8 4
1 6 3




. (2.121b)
In Chapter 4, we will be able to exploit the concept of a basis change
to find a basis with respect to which the transformation matrix of an endomorphism has a particularly simple (diagonal) form. In Chapter 10, we
will look at a data compression problem and find a convenient basis onto
which we can project the data while minimizing the compression loss.
2.7.3 Image and Kernel
The image and kernel of a linear mapping are vector subspaces with certain important properties. In the following, we will characterize them
more carefully.
Definition 2.23 (Image and Kernel).
kernel For Φ : V → W, we define the kernel/null space
null space
ker(Φ) := Φ−1
(0W ) = {v ∈ V : Φ(v) = 0W } (2.122)
image and the image/range
range
Im(Φ) := Φ(V ) = {w ∈ W|∃v ∈ V : Φ(v) = w} . (2.123)
domain We also call V and W also the domain and codomain of Φ, respectively.
codomain
Intuitively, the kernel is the set of vectors v ∈ V that Φ maps onto the
neutral element 0W ∈ W. The image is the set of vectors w ∈ W that
can be “reached” by Φ from any vector in V . An illustration is given in
Figure 2.2.
Remark. Consider a linear mapping Φ : V → W, where V, W are vector
spaces.
It always holds that Φ(0V ) = 0W and, therefore, 0V ∈ ker(Φ). In
particular, the null space is never empty.
Im(Φ) ⊆ W is a subspace of W, and ker(Φ) ⊆ V is a subspace of V .
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.7 Linear Mappings 59
Figure 2.2 Kernel
and image of a
linear mapping
Φ : V → W.
Im(Φ)
0W
ker(Φ)
0V
Φ : V → W V W
Φ is injective (one-to-one) if and only if ker(Φ) = {0}.
♢
Remark (Null Space and Column Space). Let us consider A ∈ Rm×n and
a linear mapping Φ : Rn → Rm, x 7→ Ax.
For A = [a1, . . . , an], where ai are the columns of A, we obtain
Im(Φ) = {Ax : x ∈ R
n
} =
(Xn
i=1
xiai
: x1, . . . , xn ∈ R
)
(2.124a)
= span[a1, . . . , an] ⊆ R
m , (2.124b)
i.e., the image is the span of the columns of A, also called the column column space
space. Therefore, the column space (image) is a subspace of Rm, where
m is the “height” of the matrix.
rk(A) = dim(Im(Φ)).
The kernel/null space ker(Φ) is the general solution to the homogeneous system of linear equations Ax = 0 and captures all possible
linear combinations of the elements in Rn
that produce 0 ∈ Rm.
The kernel is a subspace of Rn
, where n is the “width” of the matrix.
The kernel focuses on the relationship among the columns, and we can
use it to determine whether/how we can express a column as a linear
combination of other columns.
♢
Example 2.25 (Image and Kernel of a Linear Mapping)
The mapping
Φ : R
4 → R
2
,




x1
x2
x3
x4




7→

1 2 −1 0
1 0 0 1




x1
x2
x3
x4



 =

x1 + 2x2 − x3
x1 + x4

(2.125a)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
60 Linear Algebra
= x1

1
1

+ x2

2
0

+ x3

−1
0

+ x4

0
1

(2.125b)
is linear. To determine Im(Φ), we can take the span of the columns of the
transformation matrix and obtain
Im(Φ) = span[
1
1

,

2
0

,

−1
0

,

0
1

] . (2.126)
To compute the kernel (null space) of Φ, we need to solve Ax = 0, i.e.,
we need to solve a homogeneous equation system. To do this, we use
Gaussian elimination to transform A into reduced row-echelon form:

1 2 −1 0
1 0 0 1
⇝ · · · ⇝

1 0 0 1
0 1 −
1
2 −
1
2

. (2.127)
This matrix is in reduced row-echelon form, and we can use the Minus1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively,
we can express the non-pivot columns (columns 3 and 4) as linear combinations of the pivot columns (columns 1 and 2). The third column a3 is
equivalent to −
1
2
times the second column a2. Therefore, 0 = a3+
1
2
a2. In
the same way, we see that a4 = a1−
1
2
a2 and, therefore, 0 = a1−
1
2
a2−a4.
Overall, this gives us the kernel (null space) as
ker(Φ) = span[




0
1
2
1
0




,




−1
1
2
0
1




] . (2.128)
rank-nullity
theorem Theorem 2.24 (Rank-Nullity Theorem). For vector spaces V, W and a linear mapping Φ : V → W it holds that
dim(ker(Φ)) + dim(Im(Φ)) = dim(V ). (2.129)
fundamental The rank-nullity theorem is also referred to as the fundamental theorem
theorem of linear
mappings
of linear mappings (Axler, 2015, theorem 3.22). The following are direct
consequences of Theorem 2.24:
If dim(Im(Φ)) < dim(V ), then ker(Φ) is non-trivial, i.e., the kernel
contains more than 0V and dim(ker(Φ)) ⩾ 1.
If AΦ is the transformation matrix of Φ with respect to an ordered basis
and dim(Im(Φ)) < dim(V ), then the system of linear equations AΦx =
0 has infinitely many solutions.
If dim(V ) = dim(W), then the following three-way equivalence holds:
– Φ is injective
– Φ is surjective
– Φ is bijective
since Im(Φ) ⊆ W.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.8 Affine Spaces 61
2.8 Affine Spaces
In the following, we will have a closer look at spaces that are offset from
the origin, i.e., spaces that are no longer vector subspaces. Moreover, we
will briefly discuss properties of mappings between these affine spaces,
which resemble linear mappings.
Remark. In the machine learning literature, the distinction between linear
and affine is sometimes not clear so that we can find references to affine
spaces/mappings as linear spaces/mappings. ♢
2.8.1 Affine Subspaces
Definition 2.25 (Affine Subspace). Let V be a vector space, x0 ∈ V and
U ⊆ V a subspace. Then the subset
L = x0 + U := {x0 + u : u ∈ U} (2.130a)
= {v ∈ V |∃u ∈ U : v = x0 + u} ⊆ V (2.130b)
is called affine subspace or linear manifold of V . U is called direction or affine subspace
linear manifold
direction
direction space, and x0 is called support point. In Chapter 12, we refer to
direction space
support point
such a subspace as a hyperplane.
hyperplane
Note that the definition of an affine subspace excludes 0 if x0 ∈/ U.
Therefore, an affine subspace is not a (linear) subspace (vector subspace)
of V for x0 ∈/ U.
Examples of affine subspaces are points, lines, and planes in R3
, which
do not (necessarily) go through the origin.
Remark. Consider two affine subspaces L = x0 + U and L˜ = x˜0 + U˜ of a
vector space V . Then, L ⊆ L˜ if and only if U ⊆ U˜ and x0 − x˜0 ∈ U˜.
Affine subspaces are often described by parameters: Consider a k-dimensional affine space L = x0 + U of V . If (b1, . . . , bk) is an ordered basis of
U, then every element x ∈ L can be uniquely described as
x = x0 + λ1b1 + . . . + λkbk , (2.131)
where λ1, . . . , λk ∈ R. This representation is called parametric equation parametric equation
of L with directional vectors b1, . . . , bk and parameters λ1, . . . , λk. ♢ parameters
Example 2.26 (Affine Subspaces)
One-dimensional affine subspaces are called lines and can be written line
as y = x0 + λb1, where λ ∈ R and U = span[b1] ⊆ Rn
is a onedimensional subspace of Rn
. This means that a line is defined by a support point x0 and a vector b1 that defines the direction. See Figure 2.2
for an illustration.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
62 Linear Algebra
Two-dimensional affine subspaces of Rn
plane are called planes. The parametric equation for planes is y = x0 + λ1b1 + λ2b2, where λ1, λ2 ∈ R
and U = span[b1, b2] ⊆ Rn
. This means that a plane is defined by a
support point x0 and two linearly independent vectors b1, b2 that span
the direction space.
In Rn
hyperplane , the (n − 1)-dimensional affine subspaces are called hyperplanes,
and the corresponding parametric equation is y = x0 +
Pn−1
i=1 λibi
,
where b1, . . . , bn−1 form a basis of an (n − 1)-dimensional subspace
U of Rn
. This means that a hyperplane is defined by a support point
x0 and (n − 1) linearly independent vectors b1, . . . , bn−1 that span the
direction space. In R2
, a line is also a hyperplane. In R3
, a plane is also
a hyperplane.
Figure 2.2 Lines
are affine subspaces.
Vectors y on a line
x0 + λb1 lie in an
affine subspace L
with support point
x0 and direction b1.
0
x0
b1
y
L = x0 + λb1
Remark (Inhomogeneous systems of linear equations and affine subspaces).
For A ∈ Rm×n and x ∈ Rm, the solution of the system of linear equations Aλ = x is either the empty set or an affine subspace of Rn of
dimension n − rk(A). In particular, the solution of the linear equation
λ1b1 + . . . + λnbn = x, where (λ1, . . . , λn) ̸= (0, . . . , 0), is a hyperplane
in Rn
.
In Rn
, every k-dimensional affine subspace is the solution of an inhomogeneous system of linear equations Ax = b, where A ∈ Rm×n
, b ∈
Rm and rk(A) = n − k. Recall that for homogeneous equation systems
Ax = 0 the solution was a vector subspace, which we can also think of
as a special affine space with support point x0 = 0. ♢
2.8.2 Affine Mappings
Similar to linear mappings between vector spaces, which we discussed
in Section 2.7, we can define affine mappings between two affine spaces.
Linear and affine mappings are closely related. Therefore, many properties
that we already know from linear mappings, e.g., that the composition of
linear mappings is a linear mapping, also hold for affine mappings.
Definition 2.26 (Affine Mapping). For two vector spaces V, W, a linear
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
2.9 Further Reading 63
mapping Φ : V → W, and a ∈ W, the mapping
ϕ : V → W (2.132)
x 7→ a + Φ(x) (2.133)
is an affine mapping from V to W. The vector a is called the translation affine mapping
vector of ϕ. translation vector
Every affine mapping ϕ : V → W is also the composition of a linear
mapping Φ : V → W and a translation τ : W → W in W, such that
ϕ = τ ◦ Φ. The mappings Φ and τ are uniquely determined.
The composition ϕ
′ ◦ ϕ of affine mappings ϕ : V → W, ϕ
′
: W → X is
affine.
Affine mappings keep the geometric structure invariant. They also preserve the dimension and parallelism.
2.9 Further Reading
There are many resources for learning linear algebra, including the textbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and
Mehrmann (2015). There are also several online resources that we mentioned in the introduction to this chapter. We only covered Gaussian elimination here, but there are many other approaches for solving systems of
linear equations, and we refer to numerical linear algebra textbooks by
Stoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and
Johnson (2013) for an in-depth discussion.
In this book, we distinguish between the topics of linear algebra (e.g.,
vectors, matrices, linear independence, basis) and topics related to the
geometry of a vector space. In Chapter 3, we will introduce the inner
product, which induces a norm. These concepts allow us to define angles,
lengths and distances, which we will use for orthogonal projections. Projections turn out to be key in many machine learning algorithms, such as
linear regression and principal component analysis, both of which we will
cover in Chapters 9 and 10, respectively.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
64 Linear Algebra
Exercises
2.1 We consider (R\{−1}, ⋆), where
a ⋆ b := ab + a + b, a, b ∈ R\{−1} (2.134)
a. Show that (R\{−1}, ⋆) is an Abelian group.
b. Solve
3 ⋆ x ⋆ x = 15
in the Abelian group (R\{−1}, ⋆), where ⋆ is defined in (2.134).
2.2 Let n be in N\{0}. Let k, x be in Z. We define the congruence class k¯ of the
integer k as the set
k = {x ∈ Z | x − k = 0 (modn)}
= {x ∈ Z | ∃a ∈ Z: (x − k = n · a)} .
We now define Z/nZ (sometimes written Zn) as the set of all congruence
classes modulo n. Euclidean division implies that this set is a finite set containing n elements:
Zn = {0, 1, . . . , n − 1}
For all a, b ∈ Zn, we define
a ⊕ b := a + b
a. Show that (Zn, ⊕) is a group. Is it Abelian?
b. We now define another operation ⊗ for all a and b in Zn as
a ⊗ b = a × b , (2.135)
where a × b represents the usual multiplication in Z.
Let n = 5. Draw the times table of the elements of Z5\{0} under ⊗, i.e.,
calculate the products a ⊗ b for all a and b in Z5\{0}.
Hence, show that Z5\{0} is closed under ⊗ and possesses a neutral
element for ⊗. Display the inverse of all elements in Z5\{0} under ⊗.
Conclude that (Z5\{0}, ⊗) is an Abelian group.
c. Show that (Z8\{0}, ⊗) is not a group.
d. We recall that the B´ezout theorem states that two integers a and b are
relatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers
u and v such that au + bv = 1. Show that (Zn\{0}, ⊗) is a group if and
only if n ∈ N\{0} is prime.
2.3 Consider the set G of 3 × 3 matrices defined as follows:
G =





1 x z
0 1 y
0 0 1

 ∈ R
3×3






x, y, z ∈ R



We define · as the standard matrix multiplication.
Is (G, ·) a group? If yes, is it Abelian? Justify your answer.
2.4 Compute the following matrix products, if possible:
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Exercises 65
a.


1 2
4 5
7 8




1 1 0
0 1 1
1 0 1


b.


1 2 3
4 5 6
7 8 9




1 1 0
0 1 1
1 0 1


c.


1 1 0
0 1 1
1 0 1




1 2 3
4 5 6
7 8 9


d.

1 2 1 2
4 1 −1 −4





0 3
1 −1
2 1
5 2




e.




0 3
1 −1
2 1
5 2





1 2 1 2
4 1 −1 −4

2.5 Find the set S of all solutions in x of the following inhomogeneous linear
systems Ax = b, where A and b are defined as follows:
a.
A =




1 1 −1 −1
2 5 −7 −5
2 −1 1 3
5 2 −4 2




, b =




1
−2
4
6




b.
A =




1 −1 0 0 1
1 1 0 −3 0
2 −1 0 1 −1
−1 2 0 −2 −1




, b =




3
6
5
−1




2.6 Using Gaussian elimination, find all solutions of the inhomogeneous equation system Ax = b with
A =


0 1 0 0 1 0
0 0 0 1 1 0
0 1 0 0 0 1

 , b =


2
−1
1

 .
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
66 Linear Algebra
2.7 Find all solutions in x =


x1
x2
x3

 ∈ R3 of the equation system Ax = 12x,
where
A =


6 4 3
6 0 9
0 8 0


and P3
i=1 xi = 1.
2.8 Determine the inverses of the following matrices if possible:
a.
A =


2 3 4
3 4 5
4 5 6


b.
A =




1 0 1 0
0 1 1 0
1 1 0 1
1 1 1 0




2.9 Which of the following sets are subspaces of R3
?
a. A = {(λ, λ + µ
3
, λ − µ
3
) | λ, µ ∈ R}
b. B = {(λ
2
, −λ
2
, 0) | λ ∈ R}
c. Let γ be in R.
C = {(ξ1, ξ2, ξ3) ∈ R3
| ξ1 − 2ξ2 + 3ξ3 = γ}
d. D = {(ξ1, ξ2, ξ3) ∈ R3
| ξ2 ∈ Z}
2.10 Are the following sets of vectors linearly independent?
a.
x1 =


2
−1
3

 , x2 =


1
1
−2

 , x3 =


3
−3
8


b.
x1 =






1
2
1
0
0






, x2 =






1
1
0
1
1






, x3 =






1
0
0
1
1






2.11 Write
y =


1
−2
5


as linear combination of
x1 =


1
1
1

 , x2 =


1
2
3

 , x3 =


2
−1
1


Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Exercises 67
2.12 Consider two subspaces of R4
:
U1 = span[




1
1
−3
1




,




2
−1
0
−1




,




−1
1
−1
1




] , U2 = span[




−1
−2
2
1




,




2
−2
0
0




,




−3
6
−2
−1




] .
Determine a basis of U1 ∩ U2.
2.13 Consider two subspaces U1 and U2, where U1 is the solution space of the
homogeneous equation system A1x = 0 and U2 is the solution space of the
homogeneous equation system A2x = 0 with
A1 =




1 0 1
1 −2 −1
2 1 3
1 0 1




, A2 =




3 −3 0
1 2 3
7 −5 2
3 −1 2




.
a. Determine the dimension of U1, U2.
b. Determine bases of U1 and U2.
c. Determine a basis of U1 ∩ U2.
2.14 Consider two subspaces U1 and U2, where U1 is spanned by the columns of
A1 and U2 is spanned by the columns of A2 with
A1 =




1 0 1
1 −2 −1
2 1 3
1 0 1




, A2 =




3 −3 0
1 2 3
7 −5 2
3 −1 2




.
a. Determine the dimension of U1, U2
b. Determine bases of U1 and U2
c. Determine a basis of U1 ∩ U2
2.15 Let F = {(x, y, z) ∈ R3
| x+y−z = 0} and G = {(a−b, a+b, a−3b) | a, b ∈ R}.
a. Show that F and G are subspaces of R3
.
b. Calculate F ∩ G without resorting to any basis vector.
c. Find one basis for F and one for G, calculate F∩G using the basis vectors
previously found and check your result with the previous question.
2.16 Are the following mappings linear?
a. Let a, b ∈ R.
Φ : L
1
([a, b]) → R
f 7→ Φ(f) = Z b
a
f(x)dx ,
where L
1
([a, b]) denotes the set of integrable functions on [a, b].
b.
Φ : C
1 → C
0
f 7→ Φ(f) = f
′
,
where for k ⩾ 1, C
k denotes the set of k times continuously differentiable functions, and C
0 denotes the set of continuous functions.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
68 Linear Algebra
c.
Φ : R → R
x 7→ Φ(x) = cos(x)
d.
Φ : R
3 → R
2
x 7→

1 2 3
1 4 3
x
e. Let θ be in [0, 2π[ and
Φ : R
2 → R
2
x 7→

cos(θ) sin(θ)
− sin(θ) cos(θ)

x
2.17 Consider the linear mapping
Φ : R
3 → R
4
Φ




x1
x2
x3



 =




3x1 + 2x2 + x3
x1 + x2 + x3
x1 − 3x2
2x1 + 3x2 + x3




Find the transformation matrix AΦ.
Determine rk(AΦ).
Compute the kernel and image of Φ. What are dim(ker(Φ)) and dim(Im(Φ))?
2.18 Let E be a vector space. Let f and g be two automorphisms on E such that
f ◦ g = idE (i.e., f ◦ g is the identity mapping idE). Show that ker(f) =
ker(g ◦ f), Im(g) = Im(g ◦ f) and that ker(f) ∩ Im(g) = {0E}.
2.19 Consider an endomorphism Φ : R3 → R3 whose transformation matrix
(with respect to the standard basis in R3
) is
AΦ =


1 1 0
1 −1 0
1 1 1

 .
a. Determine ker(Φ) and Im(Φ).
b. Determine the transformation matrix A˜ Φ with respect to the basis
B = (


1
1
1

,


1
2
1

,


1
0
0

),
i.e., perform a basis change toward the new basis B.
2.20 Let us consider b1, b2, b
′
1
, b
′
2
, 4 vectors of R2
expressed in the standard basis
of R2
as
b1 =

2
1

, b2 =

−1
−1

, b
′
1 =

2
−2

, b
′
2 =

1
1

and let us define two ordered bases B = (b1, b2) and B
′ = (b
′
1
, b
′
2
) of R2
.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
Exercises 69
a. Show that B and B
′
are two bases of R2
and draw those basis vectors.
b. Compute the matrix P 1 that performs a basis change from B
′
to B.
c. We consider c1, c2, c3, three vectors of R3 defined in the standard basis
of R3
as
c1 =


1
2
−1

, c2 =


0
−1
2

, c3 =


1
0
−1


and we define C = (c1, c2, c3).
(i) Show that C is a basis of R3
, e.g., by using determinants (see
Section 4.1).
(ii) Let us call C
′ = (c
′
1
, c
′
2
, c
′
3
) the standard basis of R3
. Determine
the matrix P 2 that performs the basis change from C to C
′
.
d. We consider a homomorphism Φ : R2 −→ R3
, such that
Φ(b1 + b2) = c2 + c3
Φ(b1 − b2) = 2c1 − c2 + 3c3
where B = (b1, b2) and C = (c1, c2, c3) are ordered bases of R2
and R3
,
respectively.
Determine the transformation matrix AΦ of Φ with respect to the ordered bases B and C.
e. Determine A′
, the transformation matrix of Φ with respect to the bases
B′
and C
′
.
f. Let us consider the vector x ∈ R2 whose coordinates in B
′
are [2, 3]⊤.
In other words, x = 2b
′
1 + 3b
′
2
.
(i) Calculate the coordinates of x in B.
(ii) Based on that, compute the coordinates of Φ(x) expressed in C.
(iii) Then, write Φ(x) in terms of c
′
1
, c
′
2
, c
′
3
.
(iv) Use the representation of x in B
′
and the matrix A′
to find this
result directly.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
3
Analytic Geometry
In Chapter 2, we studied vectors, vector spaces, and linear mappings at
a general but abstract level. In this chapter, we will add some geometric interpretation and intuition to all of these concepts. In particular, we
will look at geometric vectors and compute their lengths and distances
or angles between two vectors. To be able to do this, we equip the vector space with an inner product that induces the geometry of the vector
space. Inner products and their corresponding norms and metrics capture
the intuitive notions of similarity and distances, which we use to develop
the support vector machine in Chapter 12. We will then use the concepts
of lengths and angles between vectors to discuss orthogonal projections,
which will play a central role when we discuss principal component analysis in Chapter 10 and regression via maximum likelihood estimation in
Chapter 9. Figure 3.1 gives an overview of how concepts in this chapter
are related and how they are connected to other chapters of the book.
Figure 3.1 A mind
map of the concepts
introduced in this
chapter, along with
when they are used
in other parts of the
book.
Inner product
Norm
Lengths Orthogonal
projection Angles Rotations
Chapter 4
Matrix
decomposition
Chapter 10
Dimensionality
reduction
Chapter 9
Regression
Chapter 12
Classification
induces
70
This material is published by Cambridge University Press as Mathematics for Machine Learning by
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view
and download for personal use only. Not for re-distribution, re-sale, or use in derivative works.
©by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2021. https://mml-book.com.
3.1 Norms 71
Figure 3.1 For
different norms, the
red lines indicate
the set of vectors
with norm 1. Left:
Manhattan norm;
Right: Euclidean
distance.
1
1 1
1
kxk1 = 1 kxk2 = 1
3.1 Norms
When we think of geometric vectors, i.e., directed line segments that start
at the origin, then intuitively the length of a vector is the distance of the
“end” of this directed line segment from the origin. In the following, we
will discuss the notion of the length of vectors using the concept of a norm.
Definition 3.1 (Norm). A norm on a vector space V is a function norm
∥ · ∥ : V → R , (3.1)
x 7→ ∥x∥ , (3.2)
which assigns each vector x its length ∥x∥ ∈ R, such that for all λ ∈ R length
and x, y ∈ V the following hold:
absolutely
Absolutely homogeneous: ∥λx∥ = homogeneous |λ|∥x∥
Triangle inequality: ∥x + y∥ ⩽ ∥x∥ + ∥y∥ triangle inequality
Positive definite: positive definite ∥x∥ ⩾ 0 and ∥x∥ = 0 ⇐⇒ x = 0
Figure 3.2 Triangle
inequality.
a b
c ≤ a + b
In geometric terms, the triangle inequality states that for any triangle,
the sum of the lengths of any two sides must be greater than or equal
to the length of the remaining side; see Figure 3.2 for an illustration.
Definition 3.1 is in terms of a general vector space V (Section 2.4), but
in this book we will only consider a finite-dimensional vector space Rn
.
Recall that for a vector x ∈ Rn we denote the elements of the vector using
a subscript, that is, xi
is the i
th element of the vector x.
Example 3.1 (Manhattan Norm)
The Manhattan norm on Rn
is defined for x ∈ Rn as Manhattan norm
∥x∥1 := Xn
i=1
|xi
| , (3.3)
where | · | is the absolute value. The left panel of Figure 3.1 shows all
vectors x ∈ R2 with ∥x∥1 = 1. The Manhattan norm is also called ℓ1 ℓ1 norm
norm.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
72 Analytic Geometry
Example 3.2 (Euclidean Norm)
The Euclidean norm of x ∈ Rn Euclidean norm is defined as
∥x∥2 :=
vuutXn
i=1
x
2
i =
√
x⊤x (3.4)
Euclidean distance and computes the Euclidean distance of x from the origin. The right panel
of Figure 3.1 shows all vectors x ∈ R2 with ∥x∥2 = 1. The Euclidean
ℓ2 norm norm is also called ℓ2 norm.
Remark. Throughout this book, we will use the Euclidean norm (3.4) by
default if not stated otherwise. ♢
3.2 Inner Products
Inner products allow for the introduction of intuitive geometrical concepts, such as the length of a vector and the angle or distance between
two vectors. A major purpose of inner products is to determine whether
vectors are orthogonal to each other.
3.2.1 Dot Product
We may already be familiar with a particular type of inner product, the
scalar product/dot product in Rn
scalar product , which is given by
dot product
x
⊤y =
Xn
i=1
xiyi
. (3.5)
We will refer to this particular inner product as the dot product in this
book. However, inner products are more general concepts with specific
properties, which we will now introduce.
3.2.2 General Inner Products
Recall the linear mapping from Section 2.7, where we can rearrange the
bilinear mapping mapping with respect to addition and multiplication with a scalar. A bilinear mapping Ω is a mapping with two arguments, and it is linear in
each argument, i.e., when we look at a vector space V then it holds that
for all x, y, z ∈ V, λ, ψ ∈ R that
Ω(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z) (3.6)
Ω(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z). (3.7)
Here, (3.6) asserts that Ω is linear in the first argument, and (3.7) asserts
that Ω is linear in the second argument (see also (2.87)).
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.2 Inner Products 73
Definition 3.2. Let V be a vector space and Ω : V × V → R be a bilinear
mapping that takes two vectors and maps them onto a real number. Then
Ω is called symmetric if Ω(x, y) = Ω(y, x) for all x, y ∈ V , i.e., the symmetric
order of the arguments does not matter.
Ω is called positive definite if positive definite
∀x ∈ V \{0} : Ω(x, x) > 0 , Ω(0, 0) = 0 . (3.8)
Definition 3.3. Let V be a vector space and Ω : V × V → R be a bilinear
mapping that takes two vectors and maps them onto a real number. Then
A positive definite, symmetric bilinear mapping Ω : V ×V → R is called
an inner product on V . We typically write ⟨x, y⟩ instead of Ω(x, y). inner product
The pair (V,⟨·, ·⟩) is called an inner product space or (real) vector space inner product space
vector space with
inner product
with inner product. If we use the dot product defined in (3.5), we call
(V,⟨·, ·⟩) a Euclidean vector space.
Euclidean vector
We will refer to these spaces as inner product spaces in this book. space
Example 3.3 (Inner Product That Is Not the Dot Product)
Consider V = R2
. If we define
⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2x2y2 (3.9)
then ⟨·, ·⟩ is an inner product but different from the dot product. The proof
will be an exercise.
3.2.3 Symmetric, Positive Definite Matrices
Symmetric, positive definite matrices play an important role in machine
learning, and they are defined via the inner product. In Section 4.3, we
will return to symmetric, positive definite matrices in the context of matrix
decompositions. The idea of symmetric positive semidefinite matrices is
key in the definition of kernels (Section 12.4).
Consider an n-dimensional vector space V with an inner product ⟨·, ·⟩ :
V × V → R (see Definition 3.3) and an ordered basis B = (b1, . . . , bn) of
V . Recall from Section 2.6.1 that any vectors x, y ∈ V can be written as
linear combinations of the basis vectors so that x =
Pn
i=1 ψibi ∈ V and
y =
Pn
j=1 λjbj ∈ V for suitable ψi
, λj ∈ R. Due to the bilinearity of the
inner product, it holds for all x, y ∈ V that
⟨x, y⟩ =
*Xn
i=1
ψibi
,
Xn
j=1
λjbj
+
=
Xn
i=1
Xn
j=1
ψi ⟨bi
, bj ⟩ λj = xˆ
⊤Ayˆ , (3.10)
where Aij := ⟨bi
, bj ⟩ and xˆ, yˆ are the coordinates of x and y with respect
to the basis B. This implies that the inner product ⟨·, ·⟩ is uniquely determined through A. The symmetry of the inner product also means that A
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
74 Analytic Geometry
is symmetric. Furthermore, the positive definiteness of the inner product
implies that
∀x ∈ V \{0} : x
⊤Ax > 0 . (3.11)
Definition 3.4 (Symmetric, Positive Definite Matrix). A symmetric matrix
A ∈ Rn×n
symmetric, positive that satisfies (3.11) is called symmetric, positive definite, or
definite just positive definite. If only ⩾ holds in (3.11), then A is called symmetric,
positive definite
symmetric, positive
semidefinite
positive semidefinite.
Example 3.4 (Symmetric, Positive Definite Matrices)
Consider the matrices
A1 =

9 6
6 5
, A2 =

9 6
6 3
. (3.12)
A1 is positive definite because it is symmetric and
x
⊤A1x =

x1 x2


9 6
6 5 x1
x2

(3.13a)
= 9x
2
1 + 12x1x2 + 5x
2
2 = (3x1 + 2x2)
2 + x
2
2 > 0 (3.13b)
for all x ∈ V \{0}. In contrast, A2 is symmetric but not positive definite
because x
⊤A2x = 9x
2
1 + 12x1x2 + 3x
2
2 = (3x1 + 2x2)
2 − x
2
2
can be less
than 0, e.g., for x = [2, −3]⊤.
If A ∈ Rn×n
is symmetric, positive definite, then
⟨x, y⟩ = xˆ
⊤Ayˆ (3.14)
defines an inner product with respect to an ordered basis B, where xˆ and
yˆ are the coordinate representations of x, y ∈ V with respect to B.
Theorem 3.5. For a real-valued, finite-dimensional vector space V and an
ordered basis B of V , it holds that ⟨·, ·⟩ : V × V → R is an inner product if
and only if there exists a symmetric, positive definite matrix A ∈ Rn×n with
⟨x, y⟩ = xˆ
⊤Ayˆ . (3.15)
The following properties hold if A ∈ Rn×n
is symmetric and positive
definite:
The null space (kernel) of A consists only of 0 because x
⊤Ax > 0 for
all x ̸= 0. This implies that Ax ̸= 0 if x ̸= 0.
The diagonal elements aii of A are positive because aii = e
⊤
i Aei > 0,
where ei
is the ith vector of the standard basis in Rn
.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.3 Lengths and Distances 75
3.3 Lengths and Distances
In Section 3.1, we already discussed norms that we can use to compute
the length of a vector. Inner products and norms are closely related in the
sense that any inner product induces a norm Inner products
induce norms.
∥x∥ := q
⟨x, x⟩ (3.16)
in a natural way, such that we can compute lengths of vectors using the inner product. However, not every norm is induced by an inner product. The
Manhattan norm (3.3) is an example of a norm without a corresponding
inner product. In the following, we will focus on norms that are induced
by inner products and introduce geometric concepts, such as lengths, distances, and angles.
Remark (Cauchy-Schwarz Inequality). For an inner product vector space
(V,⟨·, ·⟩) the induced norm ∥ · ∥ satisfies the Cauchy-Schwarz inequality Cauchy-Schwarz
inequality
| ⟨x, y⟩ | ⩽ ∥x∥∥y∥ . (3.17)
♢
Example 3.5 (Lengths of Vectors Using Inner Products)
In geometry, we are often interested in lengths of vectors. We can now use
an inner product to compute them using (3.16). Let us take x = [1, 1]⊤ ∈
R2
. If we use the dot product as the inner product, with (3.16) we obtain
∥x∥ =
√
x⊤x =
√
1
2 + 12 =
√
2 (3.18)
as the length of x. Let us now choose a different inner product:
⟨x, y⟩ := x
⊤

1 −
1
2
−
1
2
1

y = x1y1 −
1
2
(x1y2 + x2y1) + x2y2 . (3.19)
If we compute the norm of a vector, then this inner product returns smaller
values than the dot product if x1 and x2 have the same sign (and x1x2 >
0); otherwise, it returns greater values than the dot product. With this
inner product, we obtain
⟨x, x⟩ = x
2
1 − x1x2 + x
2
2 = 1 − 1 + 1 = 1 =⇒ ∥x∥ =
√
1 = 1 , (3.20)
such that x is “shorter” with this inner product than with the dot product.
Definition 3.6 (Distance and Metric). Consider an inner product space
(V,⟨·, ·⟩). Then
d(x, y) := ∥x − y∥ =
q
⟨x − y, x − y⟩ (3.21)
is called the distance between x and y for x, y ∈ V . If we use the dot distance
product as the inner product, then the distance is called Euclidean distance. Euclidean distance
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
76 Analytic Geometry
The mapping
d : V × V → R (3.22)
(x, y) 7→ d(x, y) (3.23)
metric is called a metric.
Remark. Similar to the length of a vector, the distance between vectors
does not require an inner product: a norm is sufficient. If we have a norm
induced by an inner product, the distance may vary depending on the
choice of the inner product. ♢
A metric d satisfies the following:
positive definite 1. d is positive definite, i.e., d(x, y) ⩾ 0 for all x, y ∈ V and d(x, y) =
0 ⇐⇒ x = y .
symmetric 2. d is symmetric, i.e., d(x, y) = d(y, x) for all x, y ∈ V .
triangle inequality 3. Triangle inequality: d(x, z) ⩽ d(x, y) + d(y, z) for all x, y, z ∈ V .
Remark. At first glance, the lists of properties of inner products and metrics look very similar. However, by comparing Definition 3.3 with Definition 3.6 we observe that ⟨x, y⟩ and d(x, y) behave in opposite directions.
Very similar x and y will result in a large value for the inner product and
a small value for the metric. ♢
3.4 Angles and Orthogonality
Figure 3.2 When
restricted to [0, π]
then f(ω) = cos(ω)
returns a unique
number in the
interval [−1, 1].
0 π/2 π
ω
−1
0
1
cos(
ω
)
In addition to enabling the definition of lengths of vectors, as well as the
distance between two vectors, inner products also capture the geometry
of a vector space by defining the angle ω between two vectors. We use
the Cauchy-Schwarz inequality (3.17) to define angles ω in inner product spaces between two vectors x, y, and this notion coincides with our
intuition in R2 and R3
. Assume that x ̸= 0, y ̸= 0. Then
−1 ⩽
⟨x, y⟩
∥x∥ ∥y∥
⩽ 1 . (3.24)
Therefore, there exists a unique ω ∈ [0, π], illustrated in Figure 3.2, with
cos ω =
⟨x, y⟩
∥x∥ ∥y∥
. (3.25)
angle The number ω is the angle between the vectors x and y. Intuitively, the
angle between two vectors tells us how similar their orientations are. For
example, using the dot product, the angle between x and y = 4x, i.e., y
is a scaled version of x, is 0: Their orientation is the same.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.4 Angles and Orthogonality 77
Example 3.6 (Angle between Vectors)
Let us compute the angle between x = [1, 1]⊤ ∈ R2 and y = [1, 2]⊤ ∈ R2
; Figure 3.3 The
angle ω between
two vectors x, y is
computed using the
inner product.
y
x
0 1
1
ω
see Figure 3.3, where we use the dot product as the inner product. Then
we get
cos ω =
⟨x, y⟩
p
⟨x, x⟩ ⟨y, y⟩
=
x
⊤y
p
x⊤xy⊤y
=
3
√
10
, (3.26)
and the angle between the two vectors is arccos( √
3
10 ) ≈ 0.32 rad, which
corresponds to about 18◦
.
A key feature of the inner product is that it also allows us to characterize
vectors that are orthogonal.
Definition 3.7 (Orthogonality). Two vectors x and y are orthogonal if and orthogonal
only if ⟨x, y⟩ = 0, and we write x ⊥ y. If additionally ∥x∥ = 1 = ∥y∥,
i.e., the vectors are unit vectors, then x and y are orthonormal. orthonormal
An implication of this definition is that the 0-vector is orthogonal to
every vector in the vector space.
Remark. Orthogonality is the generalization of the concept of perpendicularity to bilinear forms that do not have to be the dot product. In our
context, geometrically, we can think of orthogonal vectors as having a
right angle with respect to a specific inner product. ♢
Example 3.7 (Orthogonal Vectors)
Figure 3.1 The
angle ω between
two vectors x, y can
change depending
on the inner
product.
y x
−1 0 1
1
ω
Consider two vectors x = [1, 1]⊤, y = [−1, 1]⊤ ∈ R2
; see Figure 3.1.
We are interested in determining the angle ω between them using two
different inner products. Using the dot product as the inner product yields
an angle ω between x and y of 90◦
, such that x ⊥ y. However, if we
choose the inner product
⟨x, y⟩ = x
⊤

2 0
0 1
y , (3.27)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
78 Analytic Geometry
we get that the angle ω between x and y is given by
cos ω =
⟨x, y⟩
∥x∥∥y∥
= −
1
3
=⇒ ω ≈ 1.91 rad ≈ 109.5
◦
, (3.28)
and x and y are not orthogonal. Therefore, vectors that are orthogonal
with respect to one inner product do not have to be orthogonal with respect to a different inner product.
Definition 3.8 (Orthogonal Matrix). A square matrix A ∈ Rn×n
is an
orthogonal matrix orthogonal matrix if and only if its columns are orthonormal so that
AA⊤ = I = A
⊤A , (3.29)
which implies that
A
−1 = A
⊤
, (3.30)
It is convention to i.e., the inverse is obtained by simply transposing the matrix.
call these matrices
“orthogonal” but a
more precise
description would
be “orthonormal”.
Transformations by orthogonal matrices are special because the length
of a vector x is not changed when transforming it using an orthogonal
matrix A. For the dot product, we obtain
Transformations
with orthogonal
matrices preserve
distances and
angles.
∥Ax∥
2 = (Ax)
⊤(Ax) = x
⊤A
⊤Ax = x
⊤Ix = x
⊤x = ∥x∥
2
. (3.31)
Moreover, the angle between any two vectors x, y, as measured by their
inner product, is also unchanged when transforming both of them using
an orthogonal matrix A. Assuming the dot product as the inner product,
the angle of the images Ax and Ay is given as
cos ω =
(Ax)
⊤(Ay)
∥Ax∥ ∥Ay∥
=
x
⊤A
⊤Ay
q
x⊤A
⊤Axy⊤A
⊤Ay
=
x
⊤y
∥x∥ ∥y∥
, (3.32)
which gives exactly the angle between x and y. This means that orthogonal matrices A with A
⊤ = A
−1
preserve both angles and distances. It
turns out that orthogonal matrices define transformations that are rotations (with the possibility of flips). In Section 3.9, we will discuss more
details about rotations.
3.5 Orthonormal Basis
In Section 2.6.1, we characterized properties of basis vectors and found
that in an n-dimensional vector space, we need n basis vectors, i.e., n
vectors that are linearly independent. In Sections 3.3 and 3.4, we used
inner products to compute the length of vectors and the angle between
vectors. In the following, we will discuss the special case where the basis
vectors are orthogonal to each other and where the length of each basis
vector is 1. We will call this basis then an orthonormal basis.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.6 Orthogonal Complement 79
Let us introduce this more formally.
Definition 3.9 (Orthonormal Basis). Consider an n-dimensional vector
space V and a basis {b1, . . . , bn} of V . If
⟨bi
, bj ⟩ = 0 for i ̸= j (3.33)
⟨bi
, bi⟩ = 1 (3.34)
for all i, j = 1, . . . , n then the basis is called an orthonormal basis (ONB). orthonormal basis
ONB If only (3.33) is satisfied, then the basis is called an orthogonal basis. Note
orthogonal basis that (3.34) implies that every basis vector has length/norm 1.
Recall from Section 2.6.1 that we can use Gaussian elimination to find a
basis for a vector space spanned by a set of vectors. Assume we are given
a set {
˜b1, . . . ,
˜bn} of non-orthogonal and unnormalized basis vectors. We
concatenate them into a matrix B˜ = [˜b1, . . . ,
˜bn] and apply Gaussian elimination to the augmented matrix (Section 2.3.2) [B˜ B˜
⊤
|B˜ ] to obtain an
orthonormal basis. This constructive way to iteratively build an orthonormal basis {b1, . . . , bn} is called the Gram-Schmidt process (Strang, 2003).
Example 3.8 (Orthonormal Basis)
The canonical/standard basis for a Euclidean vector space Rn
is an orthonormal basis, where the inner product is the dot product of vectors.
In R2
, the vectors
b1 =
1
√
2

1
1

, b2 =
1
√
2

1
−1

(3.35)
form an orthonormal basis since b
⊤
1 b2 = 0 and ∥b1∥ = 1 = ∥b2∥.
We will exploit the concept of an orthonormal basis in Chapter 12 and
Chapter 10 when we discuss support vector machines and principal component analysis.
3.6 Orthogonal Complement
Having defined orthogonality, we will now look at vector spaces that are
orthogonal to each other. This will play an important role in Chapter 10,
when we discuss linear dimensionality reduction from a geometric perspective.
Consider a D-dimensional vector space V and an M-dimensional subspace U ⊆ V . Then its orthogonal complement U
⊥ is a (D−M)-dimensional orthogonal
complement subspace of V and contains all vectors in V that are orthogonal to every
vector in U. Furthermore, U ∩ U
⊥ = {0} so that any vector x ∈ V can be
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
80 Analytic Geometry
Figure 3.1 A plane
U in a
three-dimensional
vector space can be
described by its
normal vector,
which spans its
orthogonal
complement U⊥.
e3
e1
e2
w
U
uniquely decomposed into
x =
X
M
m=1
λmbm +
D
X−M
j=1
ψjb
⊥
j
, λm, ψj ∈ R , (3.36)
where (b1, . . . , bM) is a basis of U and (b
⊥
1
, . . . , b
⊥
D−M) is a basis of U
⊥.
Therefore, the orthogonal complement can also be used to describe a
plane U (two-dimensional subspace) in a three-dimensional vector space.
More specifically, the vector w with ∥w∥ = 1, which is orthogonal to the
plane U, is the basis vector of U
⊥. Figure 3.1 illustrates this setting. All
vectors that are orthogonal to w must (by construction) lie in the plane
normal vector U. The vector w is called the normal vector of U.
Generally, orthogonal complements can be used to describe hyperplanes
in n-dimensional vector and affine spaces.
3.7 Inner Product of Functions
Thus far, we looked at properties of inner products to compute lengths,
angles and distances. We focused on inner products of finite-dimensional
vectors. In the following, we will look at an example of inner products of
a different type of vectors: inner products of functions.
The inner products we discussed so far were defined for vectors with a
finite number of entries. We can think of a vector x ∈ Rn as a function
with n function values. The concept of an inner product can be generalized
to vectors with an infinite number of entries (countably infinite) and also
continuous-valued functions (uncountably infinite). Then the sum over
individual components of vectors (see Equation (3.5) for example) turns
into an integral.
An inner product of two functions u : R → R and v : R → R can be
defined as the definite integral
⟨u, v⟩ := Z b
a
u(x)v(x)dx (3.37)
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.8 Orthogonal Projections 81
for lower and upper limits a, b < ∞, respectively. As with our usual inner
product, we can define norms and orthogonality by looking at the inner
product. If (3.37) evaluates to 0, the functions u and v are orthogonal. To
make the preceding inner product mathematically precise, we need to take
care of measures and the definition of integrals, leading to the definition of
a Hilbert space. Furthermore, unlike inner products on finite-dimensional
vectors, inner products on functions may diverge (have infinite value). All
this requires diving into some more intricate details of real and functional
analysis, which we do not cover in this book.
Example 3.9 (Inner Product of Functions)
If we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure 3.2 f(x) =
sin(x) cos(x).
−2.5 0.0 2.5
x
−0.5
0.0
0.5
sin(
x) cos(
x
)
of (3.37), is shown in Figure 3.2. We see that this function is odd, i.e.,
f(−x) = −f(x). Therefore, the integral with limits a = −π, b = π of this
product evaluates to 0. Therefore, sin and cos are orthogonal functions.
Remark. It also holds that the collection of functions
{1, cos(x), cos(2x), cos(3x), . . . } (3.38)
is orthogonal if we integrate from −π to π, i.e., any pair of functions are
orthogonal to each other. The collection of functions in (3.38) spans a
large subspace of the functions that are even and periodic on [−π, π), and
projecting functions onto this subspace is the fundamental idea behind
Fourier series. ♢
In Section 6.4.6, we will have a look at a second type of unconventional
inner products: the inner product of random variables.
3.8 Orthogonal Projections
Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. In machine learning, we often deal
with data that is high-dimensional. High-dimensional data is often hard
to analyze or visualize. However, high-dimensional data quite often possesses the property that only a few dimensions contain most information,
and most other dimensions are not essential to describe key properties
of the data. When we compress or visualize high-dimensional data, we
will lose information. To minimize this compression loss, we ideally find
the most informative dimensions in the data. As discussed in Chapter 1, “Feature” is a
common expression
for data
representation.
data can be represented as vectors, and in this chapter, we will discuss
some of the fundamental tools for data compression. More specifically, we
can project the original high-dimensional data onto a lower-dimensional
feature space and work in this lower-dimensional space to learn more
about the dataset and extract relevant patterns. For example, machine
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
82 Analytic Geometry
Figure 3.1
Orthogonal
projection (orange
dots) of a
two-dimensional
dataset (blue dots)
onto a
one-dimensional
subspace (straight
line).
−4 −2 0 2 4
x1
−2
−1
0
1
2
x
2
learning algorithms, such as principal component analysis (PCA) by Pearson (1901) and Hotelling (1933) and deep neural networks (e.g., deep
auto-encoders (Deng et al., 2010)), heavily exploit the idea of dimensionality reduction. In the following, we will focus on orthogonal projections,
which we will use in Chapter 10 for linear dimensionality reduction and
in Chapter 12 for classification. Even linear regression, which we discuss
in Chapter 9, can be interpreted using orthogonal projections. For a given
lower-dimensional subspace, orthogonal projections of high-dimensional
data retain as much information as possible and minimize the difference/
error between the original data and the corresponding projection. An illustration of such an orthogonal projection is given in Figure 3.1. Before
we detail how to obtain these projections, let us define what a projection
actually is.
Definition 3.10 (Projection). Let V be a vector space and U ⊆ V a
projection subspace of V . A linear mapping π : V → U is called a projection if
π
2 = π ◦ π = π.
Since linear mappings can be expressed by transformation matrices (see
Section 2.7), the preceding definition applies equally to a special kind
projection matrix of transformation matrices, the projection matrices P π, which exhibit the
property that P
2
π = P π.
In the following, we will derive orthogonal projections of vectors in the
inner product space (Rn
,⟨·, ·⟩) onto subspaces. We will start with oneline dimensional subspaces, which are also called lines. If not mentioned otherwise, we assume the dot product ⟨x, y⟩ = x
⊤y as the inner product.
3.8.1 Projection onto One-Dimensional Subspaces (Lines)
Assume we are given a line (one-dimensional subspace) through the origin with basis vector b ∈ Rn
. The line is a one-dimensional subspace
U ⊆ Rn
spanned by b. When we project x ∈ Rn onto U, we seek the
vector πU (x) ∈ U that is closest to x. Using geometric arguments, let us
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.8 Orthogonal Projections 83
Figure 3.2
Examples of
projections onto
one-dimensional
subspaces.
b
x
πU (x)
ω
(a) Projection of x ∈ R2 onto a subspace U
with basis vector b.
ω cos ω
sin ω
b
x
(b) Projection of a two-dimensional vector
x with ∥x∥ = 1 onto a one-dimensional
subspace spanned by b.
characterize some properties of the projection πU (x) (Figure 3.2(a) serves
as an illustration):
The projection πU (x) is closest to x, where “closest” implies that the
distance ∥x−πU (x)∥ is minimal. It follows that the segment πU (x)−x
from πU (x) to x is orthogonal to U, and therefore the basis vector b of
U. The orthogonality condition yields ⟨πU (x) − x, b⟩ = 0 since angles
between vectors are defined via the inner product. λ is then the
coordinate of πU (x)
with respect to b.
The projection πU (x) of x onto U must be an element of U and, therefore, a multiple of the basis vector b that spans U. Hence, πU (x) = λb,
for some λ ∈ R.
In the following three steps, we determine the coordinate λ, the projection
πU (x) ∈ U, and the projection matrix P π that maps any x ∈ Rn onto U:
1. Finding the coordinate λ. The orthogonality condition yields
⟨x − πU (x), b⟩ = 0 πU (x)=λb ⇐⇒ ⟨x − λb, b⟩ = 0 . (3.39)
We can now exploit the bilinearity of the inner product and arrive at With a general inner
product, we get
λ = ⟨x, b⟩ if
∥b∥ = 1.
⟨x, b⟩ − λ ⟨b, b⟩ = 0 ⇐⇒ λ =
⟨x, b⟩
⟨b, b⟩
=
⟨b, x⟩
∥b∥
2
. (3.40)
In the last step, we exploited the fact that inner products are symmetric. If we choose ⟨·, ·⟩ to be the dot product, we obtain
λ =
b
⊤
x
b
⊤
b
=
b
⊤
x
∥b∥
2
. (3.41)
If ∥b∥ = 1, then the coordinate λ of the projection is given by b
⊤
x.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
84 Analytic Geometry
2. Finding the projection point πU (x) ∈ U. Since πU (x) = λb, we immediately obtain with (3.40) that
πU (x) = λb =
⟨x, b⟩
∥b∥
2
b =
b
⊤
x
∥b∥
2
b , (3.42)
where the last equality holds for the dot product only. We can also
compute the length of πU (x) by means of Definition 3.1 as
∥πU (x)∥ = ∥λb∥ = |λ| ∥b∥ . (3.43)
Hence, our projection is of length |λ| times the length of b. This also
adds the intuition that λ is the coordinate of πU (x) with respect to the
basis vector b that spans our one-dimensional subspace U.
If we use the dot product as an inner product, we get
∥πU (x)∥
(3.42) =
|b
⊤
x|
∥b∥
2
∥b∥
(3.25) = | cos ω| ∥x∥ ∥b∥
∥b∥
∥b∥
2
= | cos ω| ∥x∥ .
(3.44)
Here, ω is the angle between x and b. This equation should be familiar
from trigonometry: If ∥x∥ = 1, then x lies on the unit circle. It follows
The horizontal axis that the projection onto the horizontal axis spanned by b is exactly
is a one-dimensional
subspace.
cos ω, and the length of the corresponding vector πU (x) = |cos ω|. An
illustration is given in Figure 3.2(b).
3. Finding the projection matrix P π. We know that a projection is a linear mapping (see Definition 3.10). Therefore, there exists a projection
matrix P π, such that πU (x) = P πx. With the dot product as inner
product and
πU (x) = λb = bλ = b
b
⊤
x
∥b∥
2
=
bb⊤
∥b∥
2
x , (3.45)
we immediately see that
P π =
bb⊤
∥b∥
2
. (3.46)
Note that bb⊤
Projection matrices (and, consequently, P π) is a symmetric matrix (of rank
are always
symmetric.
1), and ∥b∥
2 = ⟨b, b⟩ is a scalar.
The projection matrix P π projects any vector x ∈ Rn onto the line through
the origin with direction b (equivalently, the subspace U spanned by b).
Remark. The projection πU (x) ∈ Rn
is still an n-dimensional vector and
not a scalar. However, we no longer require n coordinates to represent the
projection, but only a single one if we want to express it with respect to
the basis vector b that spans the subspace U: λ. ♢
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.8 Orthogonal Projections 85
Figure 3.1
Projection onto a
two-dimensional
subspace U with
basis b1, b2. The
projection πU (x) of
x ∈ R3 onto U can
be expressed as a
linear combination
of b1, b2 and the
displacement vector
x − πU (x) is
orthogonal to both
b1 and b2.
0
x
b1
b2
U
πU (x)
x − πU (x)
Example 3.10 (Projection onto a Line)
Find the projection matrix P π onto the line through the origin spanned
by b =

1 2 2⊤
. b is a direction and a basis of the one-dimensional
subspace (line through origin).
With (3.46), we obtain
P π =
bb⊤
b
⊤
b
=
1
9


1
2
2



1 2 2
=
1
9


1 2 2
2 4 4
2 4 4

 . (3.47)
Let us now choose a particular x and see whether it lies in the subspace
spanned by b. For x =

1 1 1⊤
, the projection is
πU (x) = P πx =
1
9


1 2 2
2 4 4
2 4 4




1
1
1

 =
1
9


5
10
10

 ∈ span[


1
2
2

] . (3.48)
Note that the application of P π to πU (x) does not change anything, i.e.,
P ππU (x) = πU (x). This is expected because according to Definition 3.10,
we know that a projection matrix P π satisfies P
2
πx = P πx for all x.
Remark. With the results from Chapter 4, we can show that πU (x) is an
eigenvector of P π, and the corresponding eigenvalue is 1. ♢
3.8.2 Projection onto General Subspaces
If U is given by a set
of spanning vectors,
which are not a
basis, make sure
you determine a
basis b1, . . . , bm
before proceeding.
In the following, we look at orthogonal projections of vectors x ∈ Rn
onto lower-dimensional subspaces U ⊆ Rn with dim(U) = m ⩾ 1. An
illustration is given in Figure 3.1.
Assume that (b1, . . . , bm) is an ordered basis of U. Any projection πU (x)
onto U is necessarily an element of U. Therefore, they can be represented
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
86 Analytic Geometry
as linear combinations of the basis vectors b1, . . . , bm of U, such that
πU (x) = Pm
i=1 λibi The basis vectors .
form the columns of
B ∈ Rn×m, where
B = [b1, . . . , bm].
As in the 1D case, we follow a three-step procedure to find the projection πU (x) and the projection matrix P π:
1. Find the coordinates λ1, . . . , λm of the projection (with respect to the
basis of U), such that the linear combination
πU (x) = Xm
i=1
λibi = Bλ , (3.49)
B = [b1, . . . , bm] ∈ R
n×m, λ = [λ1, . . . , λm]
⊤ ∈ R
m , (3.50)
is closest to x ∈ Rn
. As in the 1D case, “closest” means “minimum
distance”, which implies that the vector connecting πU (x) ∈ U and
x ∈ Rn must be orthogonal to all basis vectors of U. Therefore, we
obtain m simultaneous conditions (assuming the dot product as the
inner product)
⟨b1, x − πU (x)⟩ = b
⊤
1
(x − πU (x)) = 0 (3.51)
.
.
.
⟨bm, x − πU (x)⟩ = b
⊤
m(x − πU (x)) = 0 (3.52)
which, with πU (x) = Bλ, can be written as
b
⊤
1
(x − Bλ) = 0 (3.53)
.
.
.
b
⊤
m(x − Bλ) = 0 (3.54)
such that we obtain a homogeneous linear equation system



b
⊤
1
.
.
.
b
⊤
m




x − Bλ

 = 0 ⇐⇒ B
⊤
(x − Bλ) = 0 (3.55)
⇐⇒ B
⊤Bλ = B
⊤
x . (3.56)
normal equation The last expression is called normal equation. Since b1, . . . , bm are a
basis of U and, therefore, linearly independent, B
⊤B ∈ Rm×m is regular and can be inverted. This allows us to solve for the coefficients/
coordinates
λ = (B
⊤B)
−1B
⊤
x . (3.57)
The matrix (B
⊤B)
−1B
⊤
pseudo-inverse is also called the pseudo-inverse of B, which
can be computed for non-square matrices B. It only requires that B
⊤B
is positive definite, which is the case if B is full rank. In practical applications (e.g., linear regression), we often add a “jitter term” ϵI to
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.8 Orthogonal Projections 87
B
⊤B to guarantee increased numerical stability and positive definiteness. This “ridge” can be rigorously derived using Bayesian inference.
See Chapter 9 for details.
2. Find the projection πU (x) ∈ U. We already established that πU (x) =
Bλ. Therefore, with (3.57)
πU (x) = B(B
⊤B)
−1B
⊤
x . (3.58)
3. Find the projection matrix P π. From (3.58), we can immediately see
that the projection matrix that solves P πx = πU (x) must be
P π = B(B
⊤B)
−1B
⊤
. (3.59)
Remark. The solution for projecting onto general subspaces includes the
1D case as a special case: If dim(U) = 1, then B
⊤B ∈ R is a scalar and
we can rewrite the projection matrix in (3.59) P π = B(B
⊤B)
−1B
⊤
as
P π =
BB⊤
B⊤B
, which is exactly the projection matrix in (3.46). ♢
Example 3.11 (Projection onto a Two-dimensional Subspace)
For a subspace U = span[


1
1
1

 ,


0
1
2

] ⊆ R3 and x =


6
0
0

 ∈ R3 find the
coordinates λ of x in terms of the subspace U, the projection point πU (x)
and the projection matrix P π.
First, we see that the generating set of U is a basis (linear independence) and write the basis vectors of U into a matrix B =


1 0
1 1
1 2

.
Second, we compute the matrix B
⊤B and the vector B
⊤
x as
B
⊤B =

1 1 1
0 1 2


1 0
1 1
1 2

 =

3 3
3 5
, B
⊤
x =

1 1 1
0 1 2


6
0
0

 =

6
0

.
(3.60)
Third, we solve the normal equation B
⊤Bλ = B
⊤
x to find λ:

3 3
3 5 λ1
λ2

=

6
0

⇐⇒ λ =

5
−3

. (3.61)
Fourth, the projection πU (x) of x onto U, i.e., into the column space of
B, can be directly computed via
πU (x) = Bλ =


5
2
−1

 . (3.62)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
88 Analytic Geometry
projection error The corresponding projection error is the norm of the difference vector
The projection error between the original vector and its projection onto U, i.e.,
is also called the
reconstruction error. ∥x − πU (x)∥ =




1 −2 1⊤


 =
√
6 . (3.63)
Fifth, the projection matrix (for any x ∈ R3
) is given by
P π = B(B
⊤B)
−1B
⊤ =
1
6


5 2 −1
2 2 2
−1 2 5

 . (3.64)
To verify the results, we can (a) check whether the displacement vector
πU (x) − x is orthogonal to all basis vectors of U, and (b) verify that
P π = P
2
π
(see Definition 3.10).
Remark. The projections πU (x) are still vectors in Rn although they lie in
an m-dimensional subspace U ⊆ Rn
. However, to represent a projected
vector we only need the m coordinates λ1, . . . , λm with respect to the
basis vectors b1, . . . , bm of U. ♢
Remark. In vector spaces with general inner products, we have to pay
attention when computing angles and distances, which are defined by
means of the inner product. ♢ We can find
approximate
solutions to
unsolvable linear
equation systems
using projections.
Projections allow us to look at situations where we have a linear system
Ax = b without a solution. Recall that this means that b does not lie in
the span of A, i.e., the vector b does not lie in the subspace spanned by
the columns of A. Given that the linear equation cannot be solved exactly,
we can find an approximate solution. The idea is to find the vector in the
subspace spanned by the columns of A that is closest to b, i.e., we compute
the orthogonal projection of b onto the subspace spanned by the columns
of A. This problem arises often in practice, and the solution is called the
least-squares least-squares solution (assuming the dot product as the inner product) of
solution an overdetermined system. This is discussed further in Section 9.4. Using
reconstruction errors (3.63) is one possible approach to derive principal
component analysis (Section 10.3).
Remark. We just looked at projections of vectors x onto a subspace U with
basis vectors {b1, . . . , bk}. If this basis is an ONB, i.e., (3.33) and (3.34)
are satisfied, the projection equation (3.58) simplifies greatly to
πU (x) = BB⊤
x (3.65)
since B
⊤B = I with coordinates
λ = B
⊤
x . (3.66)
This means that we no longer have to compute the inverse from (3.58),
which saves computation time. ♢
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.8 Orthogonal Projections 89
3.8.3 Gram-Schmidt Orthogonalization
Projections are at the core of the Gram-Schmidt method that allows us to
constructively transform any basis (b1, . . . , bn) of an n-dimensional vector
space V into an orthogonal/orthonormal basis (u1, . . . ,un) of V . This
basis always exists (Liesen and Mehrmann, 2015) and span[b1, . . . , bn] =
span[u1, . . . ,un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt
orthogonalization constructs an orthogonal basis (u1, . . . ,un) from any basis (b1, . . . , bn) of
V as follows:
u1 := b1 (3.67)
uk := bk − πspan[u1,...,uk−1](bk), k = 2, . . . , n . (3.68)
In (3.68), the kth basis vector bk is projected onto the subspace spanned
by the first k − 1 constructed orthogonal vectors u1, . . . ,uk−1; see Section 3.8.2. This projection is then subtracted from bk and yields a vector
uk that is orthogonal to the (k − 1)-dimensional subspace spanned by
u1, . . . ,uk−1. Repeating this procedure for all n basis vectors b1, . . . , bn
yields an orthogonal basis (u1, . . . , un) of V . If we normalize the uk, we
obtain an ONB where ∥uk∥ = 1 for k = 1, . . . , n.
Example 3.12 (Gram-Schmidt Orthogonalization)
Figure 3.2
Gram-Schmidt
orthogonalization.
(a) non-orthogonal
basis (b1, b2) of R2
;
(b) first constructed
basis vector u1 and
orthogonal
projection of b2
onto span[u1];
(c) orthogonal basis
(u1, u2) of R2
.
b1
b2
0
(a) Original non-orthogonal
basis vectors b1, b2.
u1
b2
0 πspan[u1](b2)
(b) First new basis vector
u1 = b1 and projection of b2
onto the subspace spanned by
u1.
u1
b2
0 πspan[u1](b2)
u2
(c) Orthogonal basis vectors u1
and u2 = b2 − πspan[u1]
(b2).
Consider a basis (b1, b2) of R2
, where
b1 =

2
0

, b2 =

1
1

; (3.69)
see also Figure 3.2(a). Using the Gram-Schmidt method, we construct an
orthogonal basis (u1,u2) of R2 as follows (assuming the dot product as
the inner product):
u1 := b1 =

2
0

, (3.70)
u2 := b2 − πspan[u1](b2)
(3.45) = b2 −
u1u
⊤
1
∥u1∥
2
b2 =

1
1

−

1 0
0 0 1
1

=

0
1

.
(3.71)
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
90 Analytic Geometry
Figure 3.3
Projection onto an
affine space.
(a) original setting;
(b) setting shifted
by −x0 so that
x − x0 can be
projected onto the
direction space U;
(c) projection is
translated back to
x0 + πU (x − x0),
which gives the final
orthogonal
projection πL(x).
L
x0
x
b2
0 b1
(a) Setting.
0 b1
x − x0
U = L − x0
πU(x − x0)
b2
(b) Reduce problem to projection πU onto vector subspace.
L
x0
x
b2
0 b1
πL(x)
(c) Add support point back in
to get affine projection πL.
These steps are illustrated in Figures 3.2(b) and (c). We immediately see
that u1 and u2 are orthogonal, i.e., u
⊤
1 u2 = 0.
3.8.4 Projection onto Affine Subspaces
Thus far, we discussed how to project a vector onto a lower-dimensional
subspace U. In the following, we provide a solution to projecting a vector
onto an affine subspace.
Consider the setting in Figure 3.3(a). We are given an affine space L =
x0 + U, where b1, b2 are basis vectors of U. To determine the orthogonal
projection πL(x) of x onto L, we transform the problem into a problem
that we know how to solve: the projection onto a vector subspace. In
order to get there, we subtract the support point x0 from x and from L,
so that L − x0 = U is exactly the vector subspace U. We can now use the
orthogonal projections onto a subspace we discussed in Section 3.8.2 and
obtain the projection πU (x − x0), which is illustrated in Figure 3.3(b).
This projection can now be translated back into L by adding x0, such that
we obtain the orthogonal projection onto an affine space L as
πL(x) = x0 + πU (x − x0), (3.72)
where πU (·) is the orthogonal projection onto the subspace U, i.e., the
direction space of L; see Figure 3.3(c).
From Figure 3.3, it is also evident that the distance of x from the affine
space L is identical to the distance of x − x0 from U, i.e.,
d(x, L) = ∥x − πL(x)∥ = ∥x − (x0 + πU (x − x0))∥ (3.73a)
= d(x − x0, πU (x − x0)) = d(x − x0, U). (3.73b)
We will use projections onto an affine subspace to derive the concept of
a separating hyperplane in Section 12.1.
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.9 Rotations 91
Figure 3.2 A
rotation rotates
objects in a plane
about the origin. If
the rotation angle is
positive, we rotate
counterclockwise.
Original
Rotated by 112.5◦
Figure 3.1 The
robotic arm needs to
rotate its joints in
order to pick up
objects or to place
them correctly.
Figure taken
from (Deisenroth
et al., 2015).
3.9 Rotations
Length and angle preservation, as discussed in Section 3.4, are the two
characteristics of linear mappings with orthogonal transformation matrices. In the following, we will have a closer look at specific orthogonal
transformation matrices, which describe rotations.
A rotation is a linear mapping (more specifically, an automorphism of rotation
a Euclidean vector space) that rotates a plane by an angle θ about the
origin, i.e., the origin is a fixed point. For a positive angle θ > 0, by common convention, we rotate in a counterclockwise direction. An example is
shown in Figure 3.2, where the transformation matrix is
R =

−0.38 −0.92
0.92 −0.38
. (3.74)
Important application areas of rotations include computer graphics and
robotics. For example, in robotics, it is often important to know how to
rotate the joints of a robotic arm in order to pick up or place an object,
see Figure 3.1.
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
92 Analytic Geometry
Figure 3.2 Rotation
of the standard basis
in R2 by an angle θ.
e1
e2
θ
θ
Φ(e2) = [− sin θ, cos θ]
⊤
Φ(e1) = [cos θ,sin θ]
⊤
cos θ
sin θ
− sin θ
cos θ
3.9.1 Rotations in R2
Consider the standard basis 
e1 =

1
0

, e2 =

0
1
 of R2
, which defines
the standard coordinate system in R2
. We aim to rotate this coordinate
system by an angle θ as illustrated in Figure 3.2. Note that the rotated
vectors are still linearly independent and, therefore, are a basis of R2
.
This means that the rotation performs a basis change.
Rotations Φ are linear mappings so that we can express them by a
rotation matrix rotation matrix R(θ). Trigonometry (see Figure 3.2) allows us to determine the coordinates of the rotated axes (the image of Φ) with respect to
the standard basis in R2
. We obtain
Φ(e1) = 
cos θ
sin θ

, Φ(e2) = 
− sin θ
cos θ

. (3.75)
Therefore, the rotation matrix that performs the basis change into the
rotated coordinates R(θ) is given as
R(θ) =
Φ(e1) Φ(e2)

=

cos θ − sin θ
sin θ cos θ

. (3.76)
3.9.2 Rotations in R3
In contrast to the R2
case, in R3 we can rotate any two-dimensional plane
about a one-dimensional axis. The easiest way to specify the general rotation matrix is to specify how the images of the standard basis e1, e2, e3 are
supposed to be rotated, and making sure these images Re1, Re2, Re3 are
orthonormal to each other. We can then obtain a general rotation matrix
R by combining the images of the standard basis.
To have a meaningful rotation angle, we have to define what “counterclockwise” means when we operate in more than two dimensions. We
use the convention that a “counterclockwise” (planar) rotation about an
axis refers to a rotation about an axis when we look at the axis “head on,
from the end toward the origin”. In R3
, there are therefore three (planar)
rotations about the three standard basis vectors (see Figure 3.2):
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.9 Rotations 93
Figure 3.2 Rotation
of a vector (gray) in
R3 by an angle θ
about the e3-axis.
The rotated vector is
shown in blue.
e1
e2
e3
θ
Rotation about the e1-axis
R1(θ) =
Φ(e1) Φ(e2) Φ(e3)

=


1 0 0
0 cos θ − sin θ
0 sin θ cos θ

 . (3.77)
Here, the e1 coordinate is fixed, and the counterclockwise rotation is
performed in the e2e3 plane.
Rotation about the e2-axis
R2(θ) =


cos θ 0 sin θ
0 1 0
− sin θ 0 cos θ

 . (3.78)
If we rotate the e1e3 plane about the e2 axis, we need to look at the e2
axis from its “tip” toward the origin.
Rotation about the e3-axis
R3(θ) =


cos θ − sin θ 0
sin θ cos θ 0
0 0 1

 . (3.79)
Figure 3.2 illustrates this.
3.9.3 Rotations in n Dimensions
The generalization of rotations from 2D and 3D to n-dimensional Euclidean vector spaces can be intuitively described as fixing n − 2 dimensions and restrict the rotation to a two-dimensional plane in the n-dimensional space. As in the three-dimensional case, we can rotate any plane
(two-dimensional subspace of Rn
).
Definition 3.11 (Givens Rotation). Let V be an n-dimensional Euclidean
vector space and Φ : V → V an automorphism with transformation ma-
©2021 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020).
94 Analytic Geometry
trix
Rij (θ) :=






Ii−1 0 · · · · · · 0
0 cos θ 0 − sin θ 0
0 0 Ij−i−1 0 0
0 sin θ 0 cos θ 0
0 · · · · · · 0 In−j






∈ R
n×n
, (3.80)
Givens rotation for 1 ⩽ i < j ⩽ n and θ ∈ R. Then Rij (θ) is called a Givens rotation.
Essentially, Rij (θ) is the identity matrix In with
rii = cos θ , rij = − sin θ , rji = sin θ , rjj = cos θ . (3.81)
In two dimensions (i.e., n = 2), we obtain (3.76) as a special case.
3.9.4 Properties of Rotations
Rotations exhibit a number of useful properties, which can be derived by
considering them as orthogonal matrices (Definition 3.8):
Rotations preserve distances, i.e., ∥x−y∥ = ∥Rθ(x)−Rθ(y)∥. In other
words, rotations leave the distance between any two points unchanged
after the transformation.
Rotations preserve angles, i.e., the angle between Rθx and Rθy equals
the angle between x and y.
Rotations in three (or more) dimensions are generally not commutative. Therefore, the order in which rotations are applied is important,
even if they rotate about the same point. Only in two dimensions vector
rotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for all
ϕ, θ ∈ [0, 2π). They form an Abelian group (with multiplication) only if
they rotate about the same point (e.g., the origin).
3.10 Further Reading
In this chapter, we gave a brief overview of some of the important concepts
of analytic geometry, which we will use in later chapters of the book.
For a broader and more in-depth overview of some of the concepts we
presented, we refer to the following excellent books: Axler (2015) and
Boyd and Vandenberghe (2018).
Inner products allow us to determine specific bases of vector (sub)spaces,
where each vector is orthogonal to all others (orthogonal bases) using the
Gram-Schmidt method. These bases are important in optimization and
numerical algorithms for solving linear equation systems. For instance,
Krylov subspace methods, such as conjugate gradients or the generalized
minimal residual method (GMRES), minimize residual errors that are orthogonal to each other (Stoer and Burlirsch, 2002).
In machine learning, inner products are important in the context of
Draft (2023-02-15) of “Mathematics for Machine Learning”. Feedback: https://mml-book.com.
3.10 Further Reading 95
kernel methods (Scholkopf and Smola ¨ , 2002). Kernel methods exploit the
fact that many linear algorithms can be expressed purely by inner product computations. Then, the “kernel trick” allows us to compute these
inner products implicitly in a (potentially infinite-dimensional) feature
space, without even knowing this feature space explicitly. This allowed the
“non-linearization” of many algorithms used in machine learning, such as
kernel-PCA (Scholkopf et al. ¨ , 1997) for dimensionality reduction. Gaussian processes (Rasmussen and Williams, 2006) also fall into the category
of kernel methods and are the current state of the art in probabilistic regression (fitting curves to data points). The idea of kernels is explored
further in Chapter 12.
Projections are often used in computer graphics, e.g., to generate shadows. In optimization, orthogonal projections are often used to (iteratively)
minimize residual errors. This also has applications in machine learning,
e.g., in linear regression where we want to find a (linear) function that
minimizes the residual errors, i.e., the lengths of the orthogonal projections of the data onto the linear function (Bishop, 2006). We will investigate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also
uses projections to reduce the dimensionality of high-dimensional data.
We will discuss this in more detail in Chapter 10.