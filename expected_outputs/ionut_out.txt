Contents                                                              5.7 Higher-Order Derivatives 164                                      Part II Central Machine Learning Problems 249               
                                                                                                                                                                                                        
Foreword 1                                                            5.8 Linearization and Multivariate Taylor Series 165                  8 When Models Meet Data 251                                 
                                                                                                                                                                                                        
Part I Mathematical Foundations 9                                     5.9 Further Reading 170                                               8.1 Data, Models, and Learning 251                          
                                                                                                                                                                                                        
1 Introdu                                                             Exercises 170                                                         8.2 Empirical Risk Minimization 258                         
                                                                                                                                                                                                        
ii Contents                                                           6 Probability and Distributions 172                                   8.3 Parameter Estimation 265                                
                                                                                                                                                                                                        
4.2 Eigenvalues and Eigenvectors 105                                  6.1 Construction of a Probability Space 172                           8.4 Probabilistic Modeling and Inference 272                
                                                                                                                                                                                                        
4.3 Cholesky Decomposition 114                                        6.2 Discrete and Continuous Probabilities 178                         8.5 Directed Graphical Models 278                           
                                                                                                                                                                                                        
4.4 Eigendecomposition and Diagonalization 115                        6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183                    Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
4.5 Singular Value Decomposition 119                                  6.4 Summary Statistics and Independence 186                                                                                       
                                                                                                                                            Contents iii                                                
4.6 Matrix Approximation 129                                          6.5 Gaussian Distribution 197                                                                                                     
                                                                                                                                            8.6 Model Selection 283                                     
4.7 Matrix Phylogeny 134                                              6.6 Conjugacy and the Exponential Family 205                                                                                      
                                                                                                                                            9 Linear Regression 289                                     
4.8 Further Reading 135                                               6.7 Change of Variables/Inverse Transform 214                                                                                     
                                                                                                                                            9.1 Problem Formulation 291                                 
Exercises 137                                                         6.8 Further Reading 221                                                                                                           
                                                                                                                                            9.2 Parameter Estimation 292                                
5 Vector Calculus 139                                                 Exercises 221                                                                                                                     
                                                                                                                                            9.3 Bayesian Linear Regression 303                          
5.1 Differentiation of Univariate Functions 141                       7 Continuous Optimization 225                                                                                                     
                                                                                                                                            9.4 Maximum Likelihood as Orthogonal Projection 313         
5.2 Partial Differentiation and Gradients 146                         7.1 Optimization Using Gradient Descent 227                                                                                       
                                                                                                                                            9.5 Further Reading 315                                     
5.3 Gradients of Vector-Valued Functions 149                          7.2 Constrained Optimization and Lagrange Multipliers 233                                                                         
                                                                                                                                            10  Dimensionality  Reduction   with   Principal   Component
5.4 Gradients of Matrices 155                                         7.3 Convex Optimization 236                                           Analysis 317                                                
                                                                                                                                                                                                        
5.5 Useful Identities for Computing Gradients 158                     7.4 Further Reading 246                                               10.1 Problem Setting 318                                    
                                                                                                                                                                                                        
5.6 Backpropagation and Automatic Differentiation 159                 Exercises 247                                                         10.2 Maximum Variance Perspective 320                       
                                                                                                                                                                                                        

%%%

10.3 Projection Perspective 325                                       ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          tend to spend                                               
                                                                      by Cambridge University Press (2020).                                                                                             
10.4 Eigenvector Computation and Low-Rank Approximations 333                                                                                early  parts  of  the  course   covering   some   of   these
                                                                      Foreword                                                              pre-requisites. For historical reasons, courses  in  machine
10.5 PCA in High Dimensions 335                                                                                                             learning tend to be taught in the computer                  
                                                                      Machine learning is the latest in a long line of attempts to                                                                      
10.6 Key Steps of PCA in Practice 336                                 distill human                                                         science department, where students are often trained in  the
                                                                                                                                            first two areas                                             
10.7 Latent Variable Perspective 339                                  knowledge and reasoning into a form  that  is  suitable  for                                                                      
                                                                      constructing machines and engineering automated systems.  As          of knowledge, but not so much in mathematics and statistics.
10.8 Further Reading 343                                              machine learning becomes                                                                                                          
                                                                                                                                            Current  machine  learning  textbooks  primarily  focus   on
11 Density Estimation with Gaussian Mixture Models 348                more ubiquitous and its software packages become  easier  to          machine learning algorithms  and  methodologies  and  assume
                                                                      use,  it  is  natural  and  desirable  that  the   low-level          that the reader is competent in mathematics and  statistics.
11.1 Gaussian Mixture Model 349                                       technical details are abstracted away                                 Therefore, these books only spend                           
                                                                                                                                                                                                        
11.2 Parameter Learning via Maximum Likelihood 350                    and hidden from the practitioner. However, this brings  with          one or two chapters on background mathematics, either at the
                                                                      it the danger                                                         beginning                                                   
11.3 EM Algorithm 360                                                                                                                                                                                   
                                                                      that a practitioner becomes unaware of the design  decisions          of the book or as appendices. We have found many people  who
11.4 Latent-Variable Perspective 363                                  and, hence,                                                           want to                                                     
                                                                                                                                                                                                        
11.5 Further Reading 368                                              the limits of machine learning algorithms.                            delve into the foundations of basic machine learning methods
                                                                                                                                            who struggle with the  mathematical  knowledge  required  to
12 Classification with Support Vector Machines 370                    The enthusiastic practitioner who  is  interested  to  learn          read a machine learning                                     
                                                                      more about the                                                                                                                    
12.1 Separating Hyperplanes 372                                                                                                             textbook. Having taught undergraduate and  graduate  courses
                                                                      magic  behind   successful   machine   learning   algorithms          at universities, we find that the gap  between  high  school
12.2 Primal Support Vector Machine 374                                currently faces a                                                     mathematics and the mathematics level  required  to  read  a
                                                                                                                                            standard machine learning textbook is too                   
12.3 Dual Support Vector Machine 383                                  daunting set of pre-requisite knowledge:                                                                                          
                                                                                                                                            big for many people.                                        
12.4 Kernels 388                                                      Programming languages and data analysis tools                                                                                     
                                                                                                                                            This book  brings  the  mathematical  foundations  of  basic
12.5 Numerical Solution 390                                           Large-scale computation and the associated frameworks                 machine learning concepts  to  the  fore  and  collects  the
                                                                                                                                            information in a single place so                            
12.6 Further Reading 392                                              Mathematics and statistics and how machine  learning  builds                                                                      
                                                                      on it                                                                 that this skills gap is narrowed or even closed.            
References 395                                                                                                                                                                                          
                                                                      At universities, introductory courses  on  machine  learning          1                                                           

%%%

This material is published by Cambridge University Press  as          modern machine learning. We mothe popular mind                        and Girolami,  2016)  or  programmatic  aspects  of  machine
Mathematics for Machine Learning by                                                                                                         learning (Muller ¨                                          
                                                                      with phobia and                                                                                                                   
Marc Peter Deisenroth, A. Aldo Faisal, and  Cheng  Soon  Ong                                                                                and Guido, 2016; Raschka and Mirjalili,  2017;  Chollet  and
(2020). This version is free to view                                  anxiety. You’d think                                                  Allaire, 2018),                                             
                                                                                                                                                                                                        
and download for personal use only. Not for re-distribution,          we’re discussing                                                      we provide only  four  representative  examples  of  machine
re-sale, or use in derivative works.                                                                                                        learning algorithms. Instead, we focus on  the  mathematical
                                                                      spiders.” (Strogatz,                                                  concepts behind the models                                  
©by M. P. Deisenroth, A. A. Faisal, and  C.  S.  Ong,  2021.                                                                                                                                            
https://mml-book.com.                                                 2014, page 281)                                                       themselves. We hope that readers will  be  able  to  gain  a
                                                                                                                                            deeper understanding  of  the  basic  questions  in  machine
2 Foreword                                                            tivate  the  need  for  mathematical  concepts  by  directly          learning and connect practical questions  arising  from  the
                                                                      pointing out their                                                    use of machine learning with fundamental choices            
Why Another Book on Machine Learning?                                                                                                                                                                   
                                                                      usefulness in the context of  fundamental  machine  learning          in the mathematical model.                                  
Machine learning builds upon the language of mathematics  to          problems. In                                                                                                                      
express                                                                                                                                     We do not aim to write a classical  machine  learning  book.
                                                                      the interest of keeping the book  short,  many  details  and          Instead, our                                                
concepts  that  seem  intuitively  obvious  but   that   are          more advanced                                                                                                                     
surprisingly difficult                                                                                                                      intention is to provide the mathematical background, applied
                                                                      concepts  have  been  left  out.  Equipped  with  the  basic          to four central machine learning problems, to make it easier
to formalize. Once formalized properly, we can gain insights          concepts presented                                                    to read other machine                                       
into the task                                                                                                                                                                                           
                                                                      here, and how they fit into the larger  context  of  machine          learning textbooks.                                         
we want to  solve.  One  common  complaint  of  students  of          learning, the                                                                                                                     
mathematics                                                                                                                                 Who Is the Target Audience?                                 
                                                                      reader can find numerous resources for further study,  which                                                                      
around the globe is that the topics  covered  seem  to  have          we provide at                                                         As applications of machine  learning  become  widespread  in
little relevance                                                                                                                            society, we                                                 
                                                                      the end of the  respective  chapters.  For  readers  with  a                                                                      
to practical problems. We believe that machine  learning  is          mathematical background, this  book  provides  a  brief  but          believe that everybody should have some understanding of its
an obvious and                                                        precisely stated glimpse of machine                                   underlying                                                  
                                                                                                                                                                                                        
direct motivation for people to learn mathematics.                    learning. In contrast to other books that focus  on  methods          principles. This book is written in an academic mathematical
                                                                      and models                                                            style, which                                                
This book  is  intended  to  be  a  guidebook  to  the  vast                                                                                                                                            
mathematical lit-                                                     of machine learning (MacKay, 2003; Bishop,  2006;  Alpaydin,          enables us to be precise about the concepts  behind  machine
                                                                      2010;  Barber,  2012;  Murphy,  2012;   Shalev-Shwartz   and          learning. We                                                
“Math is linked in erature that  forms  the  foundations  of          Ben-David,                   2014;                    Rogers                                                                      

%%%

encourage readers unfamiliar with this seemingly terse style          the provision of open-source software, online tutorials  and          benefits and limits of their favorite method, and to  extend
to persevere                                                          cloud-based tools  allows  users  to  not  worry  about  the          and                                                         
                                                                      specifics of pipelines. Users can focus on                                                                                        
and to keep the goals of each topic  in  mind.  We  sprinkle                                                                                generalize existing machine  learning  algorithms.  We  hope
comments and                                                          extracting insights from  data  using  off-the-shelf  tools.          that this book                                              
                                                                      This enables nontech-savvy domain experts  to  benefit  from                                                                      
remarks throughout the text, in the hope  that  it  provides          machine learning. This is similar to listening to music; the          provides  the  impetus  for  more  rigorous  and  principled
useful guidance                                                       user is able to choose and discern between                            development of                                              
                                                                                                                                                                                                        
with respect to the big picture.                                      different types of machine learning, and benefits  from  it.          machine learning methods.                                   
                                                                      More  experienced  users  are  like  music  critics,  asking                                                                      
The book assumes the reader to have  mathematical  knowledge          important questions about the                                         Fledgling Composer As machine learning  is  applied  to  new
commonly                                                                                                                                    domains,                                                    
                                                                      application of machine learning in society such  as  ethics,                                                                      
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          fairness, and privacy of the individual. We hope  that  this          developers of machine learning need to develop  new  methods
Feedback: https://mml-book.com.                                       book provides a foundation for                                        and extend                                                  
                                                                                                                                                                                                        
Foreword 3                                                            thinking about the  certification  and  risk  management  of          existing algorithms. They are often researchers who need  to
                                                                      machine learning                                                      understand                                                  
covered in high school mathematics and physics. For example,                                                                                                                                            
the reader                                                            systems, and allows them to use their  domain  expertise  to          the mathematical  basis  of  machine  learning  and  uncover
                                                                      build better                                                          relationships between different tasks. This  is  similar  to
should have  seen  derivatives  and  integrals  before,  and                                                                                composers of music who, within                              
geometric vectors                                                     machine learning systems.                                                                                                         
                                                                                                                                            the rules and structure of musical theory,  create  new  and
in  two  or  three  dimensions.  Starting  from  there,   we          Experienced Artist Skilled practitioners of machine learning          amazing pieces.                                             
generalize these concepts. Therefore, the target audience of          can plug                                                                                                                          
the book includes undergraduate                                                                                                             We hope this book provides a high-level  overview  of  other
                                                                      and play different tools  and  libraries  into  an  analysis          technical books                                             
university   students,   evening   learners   and   learners          pipeline. The stereotypical practitioner  would  be  a  data                                                                      
participating in online                                               scientist or engineer who understands                                 for people who want to become composers of machine learning.
                                                                                                                                            There is                                                    
machine learning courses.                                             machine learning interfaces and their use cases, and is able                                                                      
                                                                      to perform                                                            a great need in society for new researchers who are able  to
In analogy to music, there are three  types  of  interaction                                                                                propose and                                                 
that people                                                           wonderful feats of prediction from data. This is similar  to                                                                      
                                                                      a virtuoso playing music, where highly skilled practitioners          explore novel approaches for attacking the  many  challenges
have with machine learning:                                           can bring existing instruments to life and  bring  enjoyment          of learning                                                 
                                                                      to their audience. Using the mathematics presented here as a                                                                      
Astute Listener The democratization of machine  learning  by          primer,  practitioners  would  be  able  to  understand  the          from                                                   data.

%%%

©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          Adam Gaier                                                            Cheng Li                                                    
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      Adele Jackson                                                         Chris Sherlock                                              
4 Foreword                                                                                                                                                                                              
                                                                      Aditya Menon                                                          Christopher Gray                                            
Acknowledgments                                                                                                                                                                                         
                                                                      Alasdair Tran                                                         Daniel McNamara                                             
We are grateful to many people who looked at early drafts of                                                                                                                                            
the book                                                              Aleksandar Krnjaic                                                    Daniel Wood                                                 
                                                                                                                                                                                                        
and suffered through painful  expositions  of  concepts.  We          Alexander Makrigiorgos                                                Darren Siegel                                               
tried to implement their ideas that we  did  not  vehemently                                                                                                                                            
disagree with. We would                                               Alfredo Canziani                                                      David Johnston                                              
                                                                                                                                                                                                        
like to especially acknowledge Christfried  Webers  for  his          Ali Shafti                                                            Dawei Chen                                                  
careful reading                                                                                                                                                                                         
                                                                      Amr Khalifa                                                           Ellen Broad                                                 
of many parts of the book, and his detailed  suggestions  on                                                                                                                                            
structure and                                                         Andrew Tanggara                                                       Fengkuangtian Zhu                                           
                                                                                                                                                                                                        
presentation. Many friends and  colleagues  have  also  been          Angus Gruen                                                           Fiona Condon                                                
kind enough                                                                                                                                                                                             
                                                                      Antal A. Buss                                                         Georgios Theodorou                                          
to provide their time and energy on  different  versions  of                                                                                                                                            
each chapter.                                                         Antoine Toisoul Le Cann                                               He Xin                                                      
                                                                                                                                                                                                        
We have been lucky to benefit from  the  generosity  of  the          Areg Sarvazyan                                                        Irene Raissa Kameni                                         
online  community,  who  have  suggested  improvements   via                                                                                                                                            
https://github.com, which                                             Artem Artemev                                                         Jakub Nabaglo                                               
                                                                                                                                                                                                        
greatly improved the book.                                            Artyom Stepanov                                                       James Hensman                                               
                                                                                                                                                                                                        
The   following   people   have   found    bugs,    proposed          Bill Kromydas                                                         Jamie Liu                                                   
clarifications and suggested relevant literature, either via                                                                                                                                            
https://github.com or personal                                        Bob Williamson                                                        Jean Kaddour                                                
                                                                                                                                                                                                        
communication. Their names are sorted alphabetically.                 Boon Ping Lim                                                         Jean-Paul Ebejer                                            
                                                                                                                                                                                                        
Abdul-Ganiy Usman                                                     Chao Qu                                                               Jerry Qiang                                                 
                                                                                                                                                                                                        

%%%

Jitesh Sindhare                                                       Mengyan Zhang                                                         Samuel Ogunmola                                             
                                                                                                                                                                                                        
John Lloyd                                                            Michael Bennett                                                       Sandeep Mavadia                                             
                                                                                                                                                                                                        
Jonas Ngnawe                                                          Michael Pedersen                                                      Sarvesh Nikumbh                                             
                                                                                                                                                                                                        
Jon Martin                                                            Minjeong Shin                                                         Sebastian Raschka                                           
                                                                                                                                                                                                        
Justin Hsi                                                            Mohammad Malekzadeh                                                   Senanayak Sesh Kumar Karri                                  
                                                                                                                                                                                                        
Kai Arulkumaran                                                       Naveen Kumar                                                          Seung-Heon Baek                                             
                                                                                                                                                                                                        
Kamil Dreczkowski                                                     Nico Montali                                                          Shahbaz Chaudhary                                           
                                                                                                                                                                                                        
Lily Wang                                                             Oscar Armas                                                           Shakir Mohamed                                              
                                                                                                                                                                                                        
Lionel Tondji Ngoupeyou                                               Patrick Henriksen                                                     Shawn Berry                                                 
                                                                                                                                                                                                        
Lydia Knufing ¨                                                       Patrick Wieschollek                                                   Sheikh Abdul Raheem Ali                                     
                                                                                                                                                                                                        
Mahmoud Aslan                                                         Pattarawat Chormai                                                    Sheng Xue                                                   
                                                                                                                                                                                                        
Mark Hartenstein                                                      Paul Kelly                                                            Sridhar Thiagarajan                                         
                                                                                                                                                                                                        
Mark van der Wilk                                                     Petros Christodoulou                                                  Syed Nouman Hasany                                          
                                                                                                                                                                                                        
Markus Hegland                                                        Piotr Januszewski                                                     Szymon Brych                                                
                                                                                                                                                                                                        
Martin Hewing                                                         Pranav Subramani                                                      Thomas Buhler ¨                                             
                                                                                                                                                                                                        
Matthew Alger                                                         Quyu Kong                                                             Timur Sharapov                                              
                                                                                                                                                                                                        
Matthew Lee                                                           Ragib Zaman                                                           Tom Melamed                                                 
                                                                                                                                                                                                        
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          Rui Zhang                                                             Vincent Adam                                                
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                      Ryan-Rhys Griffiths                                                   Vincent Dutordoir                                           
Foreword 5                                                                                                                                                                                              
                                                                      Salomon Kabongo                                                       Vu Minh                                                     
Maximus                                               McCann                                                                                                                                            

%%%

Wasim Aftab                                                           insad                                                                 Symbol Typical meaning                                      
                                                                                                                                                                                                        
Wen Zhi                                                               HorizonP                                                              a, b, c, α, β, γ Scalars are lowercase                      
                                                                                                                                                                                                        
Wojciech Stokowiec                                                    cs-maillist                                                           x, y, z Vectors are bold lowercase                          
                                                                                                                                                                                                        
Xiaonan Chong                                                         kudo23                                                                A, B, C Matrices are bold uppercase                         
                                                                                                                                                                                                        
Xiaowei Zhang                                                         empet                                                                 x                                                           
                                                                                                                                                                                                        
Yazhou Hao                                                            victorBigand                                                          ⊤, A                                                        
                                                                                                                                                                                                        
Yicheng Luo                                                           17SKYE                                                                ⊤                                                           
                                                                                                                                                                                                        
Young Lee                                                             jessjing1995                                                          Transpose of a vector or matrix                             
                                                                                                                                                                                                        
Yu Lu                                                                 We are also very grateful to Parameswaran Raman and the many          A                                                           
                                                                      anonymous  reviewers,  organized  by  Cambridge   University                                                                      
Yun Cheng                                                             Press, who read one                                                   −1                                                          
                                                                                                                                                                                                        
Yuxiao Huang                                                          or more chapters of earlier versions of the manuscript,  and          Inverse of a matrix                                         
                                                                      provided constructive criticism  that  led  to  considerable                                                                      
Zac Cranko                                                            improvements. A special mention goes to Dinesh  Singh  Negi,          ⟨x, y⟩ Inner product of x and y                             
                                                                      our LATEX support, for detailed and prompt                                                                                        
Zijian Cao                                                                                                                                  x                                                           
                                                                      advice about LATEX-related issues. Last but  not  least,  we                                                                      
Zoe Nolan                                                             are very grateful                                                     ⊤y Dot product of x and y                                   
                                                                                                                                                                                                        
Contributors through  GitHub,  whose  real  names  were  not          to our editor Lauren Cowles, who has been patiently  guiding          B = (b1, b2, b3) (Ordered) tuple                            
listed on their                                                       us through                                                                                                                        
                                                                                                                                            B  =  [b1,  b2,  b3]  Matrix  of  column   vectors   stacked
GitHub profile, are:                                                  the gestation process of this book.                                   horizontally                                                
                                                                                                                                                                                                        
SamDataMad                                                            ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          B = {b1, b2, b3} Set of vectors (unordered)                 
                                                                      by Cambridge University Press (2020).                                                                                             
bumptiousmonkey                                                                                                                             Z, N Integers and natural numbers, respectively             
                                                                      6 Foreword                                                                                                                        
idoamihai                                                                                                                                   R, C Real and complex numbers, respectively                 
                                                                      Table of Symbols                                                                                                                  
deepakiim                                                                                                                                   Rn   n-dimensional   vector   space    of    real    numbers

%%%

∀x Universal quantifier: for all x                                    Im(Φ) Image of linear mapping Φ                                       θ Parameter vector                                          
                                                                                                                                                                                                        
∃x Existential quantifier: there exists x                             ker(Φ) Kernel (null space) of a linear mapping Φ                      ∂f                                                          
                                                                                                                                                                                                        
a := b a is defined as b                                              span[b1] Span (generating set) of b1                                  ∂x Partial derivative of f with respect to x                
                                                                                                                                                                                                        
a =: b b is defined as a                                              tr(A) Trace of A                                                      df                                                          
                                                                                                                                                                                                        
a ∝ b a is proportional to b, i.e., a = constant · b                  det(A) Determinant of A                                               dx                                                          
                                                                                                                                                                                                        
g ◦ f Function composition: “g after f”                               | · | Absolute value or determinant (depending on context)            Total derivative of f with respect to x                     
                                                                                                                                                                                                        
⇐⇒ If and only if                                                     ∥·∥ Norm; Euclidean, unless specified                                 ∇ Gradient                                                  
                                                                                                                                                                                                        
=⇒ Implies                                                            λ Eigenvalue or Lagrange multiplier                                   f∗ = minx f(x) The smallest function value of f             
                                                                                                                                                                                                        
A, C Sets                                                             Eλ Eigenspace corresponding to eigenvalue λ                           x∗ ∈ arg minx f(x) The value x∗ that minimizes f (note:  arg
                                                                                                                                            min returns a set of values)                                
a ∈ A a is an element of set A                                        Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       L Lagrangian                                                
∅ Empty set                                                                                                                                                                                             
                                                                      Foreword 7                                                            L Negative log-likelihood                                   
A\B A without B: the set of elements in A but not in B                                                                                                                                                  
                                                                      Symbol Typical meaning                                                n                                                           
D Number of dimensions; indexed by d = 1, . . . , D                                                                                                                                                     
                                                                      x ⊥ y Vectors x and y are orthogonal                                  k                                                           
N Number of data points; indexed by n = 1, . . . , N                                                                                                                                                    
                                                                      V Vector space                                                                                                                   
Im Identity matrix of size m × m                                                                                                                                                                        
                                                                      V                                                                     Binomial coefficient, n choose k                            
0m,n Matrix of zeros of size m × n                                                                                                                                                                      
                                                                      ⊥ Orthogonal complement of vector space V                             VX[x] Variance of x with respect to the random variable X   
1m,n Matrix of ones of size m × n                                                                                                                                                                       
                                                                      PN                                                                    EX[x] Expectation of x with respect to the random variable X
ei Standard/canonical vector (where i is the component  that                                                                                                                                            
is 1)                                                                 Qn=1 xn Sum of the xn: x1 + . . . + xN                                CovX,Y [x, y] Covariance between x and y.                   
                                                                                                                                                                                                        
dim Dimensionality of vector space                                    N                                                                     X ⊥⊥ Y |Z X is conditionally independent of Y given Z       
                                                                                                                                                                                                        
rk(A)          Rank          of           matrix           A          n=1  xn  Product  of  the  xn:   x1   ·   .   .   .   ·   xN          X ∼ p Random  variable  X  is  distributed  according  to  p

%%%

N                                                                     SVM Support vector machine                                            data, a model, and learning.                                
                                                                                                                                                                                                        
µ, Σ                                                                  ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          Since machine learning is inherently data driven, data is at
                                                                      by Cambridge University Press (2020                                   the core data                                               
                                                                                                                                                                                                       
                                                                      Part I                                                                of machine learning. The goal  of  machine  learning  is  to
Gaussian distribution with mean µ and covariance Σ                                                                                          design  generalpurpose  methodologies  to  extract  valuable
                                                                      Mathematical Foundations                                              patterns from data, ideally                                 
Ber(µ) Bernoulli distribution with parameter µ                                                                                                                                                          
                                                                      9                                                                     without much domain-specific expertise. For example, given a
Bin(N, µ) Binomial distribution with parameters N, µ                                                                                        large corpus                                                
                                                                      This material is published by Cambridge University Press  as                                                                      
Beta(α, β) Beta distribution with parameters α, β                     Mathematics for Machine Learning by                                   of  documents  (e.g.,  books  in  many  libraries),  machine
                                                                                                                                            learning methods                                            
Table of Abbreviations and Acronyms                                   Marc Peter Deisenroth, A. Aldo Faisal, and  Cheng  Soon  Ong                                                                      
                                                                      (2020). This version is free to view                                  can be used to automatically find relevant topics  that  are
Acronym Meaning                                                                                                                             shared across                                               
                                                                      and download for personal use only. Not for re-distribution,                                                                      
e.g. Exempli gratia (Latin: for example)                              re-sale, or use in derivative works.                                  documents (Hoffman et al., 2010). To achieve this  goal,  we
                                                                                                                                            design models that are typically related to the process that
GMM Gaussian mixture model                                            ©by M. P. Deisenroth, A. A. Faisal, and  C.  S.  Ong,  2021.          generates data, similar to model                            
                                                                      https://mml-book.com.                                                                                                             
i.e. Id est (Latin: this means)                                                                                                             the dataset we are  given.  For  example,  in  a  regression
                                                                      1                                                                     setting, the model                                          
i.i.d. Independent, identically distributed                                                                                                                                                             
                                                                      Introduction and Motivation                                           would describe a function that maps  inputs  to  real-valued
MAP Maximum a posteriori                                                                                                                    outputs. To                                                 
                                                                      Machine  learning  is  about   designing   algorithms   that                                                                      
MLE Maximum likelihood estimation/estimator                           automatically extract                                                 paraphrase Mitchell (1997): A model is said  to  learn  from
                                                                                                                                            data if its performance on a given task improves  after  the
ONB Orthonormal basis                                                 valuable information from data.  The  emphasis  here  is  on          data is taken into account.                                 
                                                                      “automatic”, i.e.,                                                                                                                
PCA Principal component analysis                                                                                                            The goal is to find good models that generalize well to  yet
                                                                      machine  learning   is   concerned   about   general-purpose          unseen data,                                                
PPCA Probabilistic principal component analysis                       methodologies that                                                                                                                
                                                                                                                                            which we may care about  in  the  future.  Learning  can  be
REF Row-echelon form                                                  can be applied to many datasets, while  producing  something          understood as a learning                                    
                                                                      that is meaningful. There are three concepts that are at the                                                                      
SPD Symmetric, positive definite                                      core of machine learning:                                             way to automatically find patterns and structure in data  by
                                                                                                                                            optimizing                                               the

%%%

parameters of the model.                                              A challenge we face regularly in machine  learning  is  that          outline these                                               
                                                                      concepts and                                                                                                                      
While machine learning has seen many  success  stories,  and                                                                                components here, and we will revisit them again in Chapter 8
software is                                                           words are  slippery,  and  a  particular  component  of  the          once we                                                     
                                                                      machine learning                                                                                                                  
readily available to design  and  train  rich  and  flexible                                                                                have discussed the necessary mathematical concepts.         
machine learning                                                      system can be abstracted to different mathematical concepts.                                                                      
                                                                      For example,                                                          While not all data is  numerical,  it  is  often  useful  to
systems, we believe that  the  mathematical  foundations  of                                                                                consider data in                                            
machine  learning  are  important  in  order  to  understand          the word “algorithm” is  used  in  at  least  two  different                                                                      
fundamental principles upon                                           senses in the context of  machine  learning.  In  the  first          a number format. In this  book,  we  assume  that  data  has
                                                                      sense, we use the phrase “machine                                     already been                                                
which more complicated machine learning systems  are  built.                                                                                                                                            
Understanding these principles can facilitate  creating  new          learning algorithm” to mean a system that makes  predictions          appropriately  converted  into  a  numerical  representation
machine learning solutions,                                           based on inpredictor put data. We refer to these  algorithms          suitable  for  readdata  as  vectors  ing  into  a  computer
                                                                      as predictors. In the second sense,                                   program. Therefore, we think of data as vectors. As         
understanding  and  debugging   existing   approaches,   and                                                                                                                                            
learning about the                                                    we use the exact same phrase “machine learning algorithm” to          another illustration of how subtle words are, there are  (at
                                                                      mean a                                                                least) three                                                
inherent assumptions and limitations of the methodologies we                                                                                                                                            
are working with.                                                     system that adapts some internal parameters of the predictor          different ways to think about vectors: a vector as an  array
                                                                      so that it                                                            of numbers (a                                               
11                                                                                                                                                                                                      
                                                                      performs well on future unseen input data. Here we refer  to          computer  science  view),  a  vector  as  an  arrow  with  a
This material is published by Cambridge University Press  as          this adaptatraining tion as training a system.                        direction and magnitude (a physics view), and a vector as an
Mathematics for Machine Learning by                                                                                                         object that obeys addition and                              
                                                                      This book will not resolve the issue of  ambiguity,  but  we                                                                      
Marc Peter Deisenroth, A. Aldo Faisal, and  Cheng  Soon  Ong          want to highlight upfront that, depending  on  the  context,          scaling (a mathematical view).                              
(2020). This version is free to view                                  the same expressions can                                                                                                          
                                                                                                                                            model A model is typically used to describe  a  process  for
and download for personal use only. Not for re-distribution,          mean different things.  However,  we  attempt  to  make  the          generating data, similar to the dataset at hand.  Therefore,
re-sale, or use in derivative works.                                  context sufficiently clear to reduce the level of ambiguity.          good models can also be thought                             
                                                                                                                                                                                                        
©by M. P. Deisenroth, A. A. Faisal, and  C.  S.  Ong,  2021.          The first part of  this  book  introduces  the  mathematical          of  as   simplified   versions   of   the   real   (unknown)
https://mml-book.com.                                                 concepts and                                                          data-generating process,                                    
                                                                                                                                                                                                        
12 Introduction and Motivation                                        foundations needed to talk about the three  main  components          capturing aspects that are relevant for  modeling  the  data
                                                                      of a machine                                                          and extracting                                              
1.1 Finding Words for Intuitions                                                                                                                                                                        
                                                                      learning system: data, models, and learning. We will briefly          hidden patterns from it. A good model can then  be  used  to

%%%

predict what                                                          unseen data, and, in practical applications, we  often  need          particularly interesting by  themselves,  and  the  lack  of
                                                                      to expose our                                                         motivation means                                            
would happen in the real world without performing real-world                                                                                                                                            
experiments.                                                          machine learning  system  to  situations  that  it  has  not          that most foundational definitions are quickly forgotten.   
                                                                      encountered before.                                                                                                               
learning We now come to the crux of the matter, the learning                                                                                Top-down: Drilling down from practical needs to  more  basic
component of                                                          Let us summarize the main concepts of machine learning  that          requirements. This goal-driven approach  has  the  advantage
                                                                      we cover                                                              that the readers                                            
machine learning. Assume  we  are  given  a  dataset  and  a                                                                                                                                            
suitable model.                                                       in this book:                                                         know at all times why they need  to  work  on  a  particular
                                                                                                                                            concept, and                                                
Training the model  means  to  use  the  data  available  to          We represent data as vectors.                                                                                                     
optimize some parameters of the  model  with  respect  to  a                                                                                there is a clear path of required knowledge. The downside of
utility function that evaluates how                                   We  choose  an   appropriate   model,   either   using   the          this strategy is that the knowledge is built on  potentially
                                                                      probabilistic or optimization view.                                   shaky foundations, and                                      
well the model predicts the  training  data.  Most  training                                                                                                                                            
methods can be                                                        We learn from available data by using numerical optimization          the readers have to remember a set of words that they do not
                                                                      methods                                                               have any                                                    
thought of as an approach analogous to climbing  a  hill  to                                                                                                                                            
reach its peak.                                                       with the aim that the model performs well on data  not  used          way of understanding.                                       
                                                                      for training.                                                                                                                     
In this analogy, the peak  of  the  hill  corresponds  to  a                                                                                We decided to write this book in a modular way  to  separate
maximum of some                                                       1.2 Two Ways to Read This Book                                        foundational                                                
                                                                                                                                                                                                        
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          We  can  consider  two  strategies  for  understanding   the          (mathematical) concepts from applications so that this  book
Feedback: https://mml-book.com.                                       mathematics for                                                       can be read                                                 
                                                                                                                                                                                                        
1.2 Two Ways to Read This Book 13                                     machine learning:                                                     in both ways. The book is split into two parts, where Part I
                                                                                                                                            lays the mathematical foundations and Part  II  applies  the
desired performance measure. However, in  practice,  we  are          Bottom-up: Building up the  concepts  from  foundational  to          concepts from Part I to a set                               
interested in                                                         more advanced. This is often the preferred approach in  more                                                                      
                                                                      technical fields,                                                     of fundamental machine learning problems,  which  form  four
the model to perform well on unseen data. Performing well on                                                                                pillars of                                                  
data that                                                             such as mathematics. This strategy has  the  advantage  that                                                                      
                                                                      the reader                                                            machine learning as illustrated in Figure  1.2:  regression,
we have already seen (training data) may only mean  that  we                                                                                dimensionality                                              
found a                                                               at all times is able to rely  on  their  previously  learned                                                                      
                                                                      concepts. Unfortunately, for  a  practitioner  many  of  the          reduction, density estimation, and classification.  Chapters
good way  to  memorize  the  data.  However,  this  may  not          foundational concepts are not                                         in Part I mostly                                            
generalize                      well                      to                                                                                                                                            

%%%

build upon the previous ones, but it is possible to  skip  a          between the two parts  of  the  book  to  link  mathematical          are similar should be predicted to have similar  outputs  by
chapter and work                                                      concepts with                                                         our machine                                                 
                                                                                                                                                                                                        
backward if necessary. Chapters in Part II are only  loosely          machine learning algorithms.                                          learning algorithm (our predictor). To formalize the idea of
coupled and                                                                                                                                 similarity between vectors, we need to introduce  operations
                                                                      Of course there are more than two ways to  read  this  book.          that take two vectors as                                    
can be read in any order. There are  many  pointers  forward          Most readers                                                                                                                      
and backward                                                                                                                                input  and  return  a  numerical  value  representing  their
                                                                      learn  using  a  combination  of  top-down   and   bottom-up          similarity. The conanalytic geometry struction of similarity
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          approaches, sometimes building up basic mathematical  skills          and distances is central to analytic geometry and is        
by Cambridge University Press (2020).                                 before attempting more complex concepts, but  also  choosing                                                                      
                                                                      topics based on applications of machine                               discussed in Chapter 3.                                     
14 Introduction and Motivation                                                                                                                                                                          
                                                                      learning.                                                             In Chapter 4, we introduce some fundamental  concepts  about
Figure 1.2 The                                                                                                                              matrimatrix ces and matrix decomposition. Some operations on
                                                                      Part I Is about Mathematics                                           matrices are extremely                                      
foundations and                                                                                                                                                                                         
                                                                      The four pillars of machine learning we cover in  this  book          decomposition useful in machine learning, and they allow for
four pillars of                                                       (see Figure 1.2)                                                      an intuitive interpretation                                 
                                                                                                                                                                                                        
machine learning.                                                     require a solid mathematical foundation, which is  laid  out          of the data and more efficient learning.                    
                                                                      in Part I.                                                                                                                        
Classification                                                                                                                              We often consider data to be noisy observations of some true
                                                                      We represent numerical data as vectors and represent a table          underlying signal. We hope that by applying machine learning
Density                                                               of such                                                               we can identify the                                         
                                                                                                                                                                                                        
Regression                                                            data as a matrix. The  study  of  vectors  and  matrices  is          signal from the noise. This requires us to have  a  language
                                                                      called linear algebra,                                                for quantifying what “noise” means. We often would also like
Dimensionality                                                                                                                              to have predictors that                                     
                                                                      linear  algebra  which  we  introduce  in  Chapter  2.   The                                                                      
Estimation                                                            collection of vectors as a matrix is                                  Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
Reduction                                                             also described there.                                                                                                             
                                                                                                                                            1.2 Two Ways to Read This Book 15                           
Machine Learning                                                      Given two vectors  representing  two  objects  in  the  real                                                                      
                                                                      world, we want                                                        allow us to express  some  sort  of  uncertainty,  e.g.,  to
Vector Calculus Probability & Distributions Optimization                                                                                    quantify the confidence we  have  about  the  value  of  the
                                                                      to make statements about their similarity. The idea is  that          prediction at a particular test data                        
Linear Algebra Analytic Geometry Matrix Decomposition                 vectors that                                                                                                                      
                                                                                                                                            point.  Quantification  of  uncertainty  is  the  realm   of

%%%

probability theory and probability theory                             that guard against overly optimistic evaluations of  machine          objective of density estimation is  to  find  a  probability
                                                                      learning systems.  Recall  that  the  goal  is  to  build  a          distribution that describes a given dataset. We  will  focus
is covered in Chapter 6.                                              predictor that performs well on                                       on Gaussian mixture models for this                         
                                                                                                                                                                                                        
To  train  machine  learning  models,  we   typically   find          unseen data.                                                          purpose, and we will discuss an iterative scheme to find the
parameters that                                                                                                                             parameters of                                               
                                                                      In  Chapter  9,  we  will  have  a  close  look  at   linear                                                                      
maximize  some  performance   measure.   Many   optimization          regression, where our linear regression                               this model. As in dimensionality  reduction,  there  are  no
techniques require the concept of a gradient, which tells us                                                                                labels associated                                           
the direction in which to                                             objective is to find functions that map inputs  x  ∈  RD  to                                                                      
                                                                      corresponding observed function values y ∈ R, which  we  can          with the data points x ∈ RD.  However,  we  do  not  seek  a
search for a solution. Chapter 5 is  about  vector  calculus          interpret as the labels of their                                      low-dimensional                                             
and details the vector calculus                                                                                                                                                                         
                                                                      respective inputs. We will discuss classical  model  fitting          representation of the data. Instead, we are interested in  a
concept of gradients, which we subsequently use  in  Chapter          (parameter estimation) via maximum likelihood and maximum  a          density model                                               
7, where we                                                           posteriori estimation,                                                                                                            
                                                                                                                                            that describes the data.                                    
talk about optimization to find maxima/minima of  functions.          as well as Bayesian linear regression,  where  we  integrate                                                                      
optimization                                                          the parameters                                                        Chapter 12 concludes the book with an in-depth discussion of
                                                                                                                                            the fourth                                                  
Part II Is about Machine Learning                                     out instead of optimizing them.                                                                                                   
                                                                                                                                            ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
The second part of  the  book  introduces  four  pillars  of          Chapter 10 focuses on dimensionality reduction,  the  second          by Cambridge University Press (2020).                       
machine learning                                                      pillar in Fig- dimensionality                                                                                                     
                                                                                                                                            16 Introduction and Motivation                              
as shown in Figure 1.2. We illustrate how  the  mathematical          ure 1.2 reduction , using principal component analysis.  The                                                                      
concepts introduced in the first part of the  book  are  the          key objective of  dimensionality  reduction  is  to  find  a          classification  pillar:  classification.  We  will   discuss
foundation for each pillar.                                           compact, lower-dimensional representation                             classification in the context of support                    
                                                                                                                                                                                                        
Broadly speaking, chapters are  ordered  by  difficulty  (in          of high-dimensional data x ∈ RD, which is  often  easier  to          vector machines. Similar to regression (Chapter 9), we  have
ascending order).                                                     analyze than                                                          inputs x and                                                
                                                                                                                                                                                                        
In Chapter 8, we restate the  three  components  of  machine          the  original  data.   Unlike   regression,   dimensionality          corresponding labels y. However,  unlike  regression,  where
learning                                                              reduction is only concerned about modeling the data –  there          the labels were                                             
                                                                      are no labels associated with a                                                                                                   
(data, models, and parameter estimation) in  a  mathematical                                                                                real-valued, the  labels  in  classification  are  integers,
fashion. In                                                           data point x.                                                         which requires special                                      
                                                                                                                                                                                                        
addition,  we   provide   some   guidelines   for   building          In Chapter 11, we will move to  our  third  pillar:  density          care.                                                       
experimental                                         set-ups          estimation.         The          density          estimation                                                                      

%%%

1.3 Exercises and Feedback                                            these objects. This                                                   x,                                                          
                                                                                                                                                                                                        
We provide some exercises in  Part  I,  which  can  be  done          is known as an algebra.  Linear  algebra  is  the  study  of          →                                                           
mostly by pen and                                                     vectors and certain algebra                                                                                                       
                                                                                                                                            y can be added, such that →                                 
paper.  For  Part  II,  we  provide  programming   tutorials          rules to manipulate vectors. The vectors  many  of  us  know                                                                      
(jupyter notebooks)                                                   from school are                                                       x+                                                          
                                                                                                                                                                                                        
to  explore  some  properties  of   the   machine   learning          called “geometric vectors”, which are usually denoted  by  a          →                                                           
algorithms we discuss                                                 small arrow                                                                                                                       
                                                                                                                                            y =                                                         
in this book.                                                         above the letter, e.g., −→x and  −→y  .  In  this  book,  we                                                                      
                                                                      discuss more general                                                  →                                                           
We  appreciate  that  Cambridge  University  Press  strongly                                                                                                                                            
supports our                                                          concepts of vectors and use a bold letter to represent them,          z                                                           
                                                                      e.g., x and y.                                                                                                                    
aim to democratize education and  learning  by  making  this                                                                                is another geometric vector. Furthermore, multiplication  by
book freely                                                           In general, vectors are special objects that  can  be  added          a scalar                                                    
                                                                      together and                                                                                                                      
available for download at                                                                                                                   λ                                                           
                                                                      multiplied by scalars to produce another object of the  same                                                                      
https://mml-book.com                                                  kind. From                                                            →                                                           
                                                                                                                                                                                                        
where tutorials, errata, and  additional  materials  can  be          an  abstract  mathematical  viewpoint,   any   object   that          x, λ ∈ R, is also a geometric vector. In  fact,  it  is  the
found. Mistakes                                                       satisfies these two                                                   original vector                                             
                                                                                                                                                                                                        
can be reported and feedback provided  using  the  preceding          properties  can  be  considered  a  vector.  Here  are  some          scaled by λ. Therefore, geometric vectors are  instances  of
URL.                                                                  examples of such                                                      the vector                                                  
                                                                                                                                                                                                        
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          vector objects:                                                       concepts  introduced  previously.  Interpreting  vectors  as
Feedback: https://mml-book.com.                                                                                                             geometric vectors enables us to  use  our  intuitions  about
                                                                      1. Geometric vectors.  This  example  of  a  vector  may  be          direction and magnitude to                                  
2                                                                     familiar from high                                                                                                                
                                                                                                                                            reason about mathematical operations.                       
Linear Algebra                                                        school mathematics and  physics.  Geometric  vectors  –  see                                                                      
                                                                      Figure 2.1(a)                                                         2. Polynomials are also  vectors;  see  Figure  2.1(b):  Two
When formalizing intuitive concepts, a common approach is to                                                                                polynomials can                                             
construct a                                                           – are directed segments, which can be drawn (at least in two                                                                      
                                                                      dimensions). Two geometric vectors →                                  Figure 2.1                                                  
set of objects (symbols) and a set of  rules  to  manipulate                                                                                                                                            

%%%

Different types of                                                    0                                                                     concepts. However,  they  are  both  vectors  in  the  sense
                                                                                                                                            previously described.                                       
vectors. Vectors can                                                  2                                                                                                                                 
                                                                                                                                            3. Audio signals are vectors. Audio signals are  represented
be surprising                                                         4                                                                     as a series of                                              
                                                                                                                                                                                                        
objects, including                                                    y                                                                     numbers. We can add audio signals together, and their sum is
                                                                                                                                            a new                                                       
(a) geometric                                                         (b) Polynomials.                                                                                                                  
                                                                                                                                            audio signal. If we scale an audio signal, we also obtain an
vectors                                                               17                                                                    audio signal.                                               
                                                                                                                                                                                                        
and (b) polynomials.                                                  This material is published by Cambridge University Press  as          Therefore, audio signals are a type of vector, too.         
                                                                      Mathematics for Machine Learning by                                                                                               
→                                                                                                                                           4. Elements of Rn                                           
                                                                      Marc Peter Deisenroth, A. Aldo Faisal, and  Cheng  Soon  Ong                                                                      
x →                                                                   (2020). This version is free to view                                  (tuples of n real numbers) are vectors. Rn                  
                                                                                                                                                                                                        
y                                                                     and download for personal use only. Not for re-distribution,          is more                                                     
                                                                      re-sale, or use in derivative works.                                                                                              
→                                                                                                                                           abstract than polynomials, and it is the concept we focus on
                                                                      ©by M. P. Deisenroth, A. A. Faisal, and  C.  S.  Ong,  2021.          in this                                                     
x +                                                                   https://mml-book.com.                                                                                                             
                                                                                                                                            book. For instance,                                         
→                                                                     18 Linear Algebra                                                                                                                 
                                                                                                                                            a =                                                         
y                                                                     be added together, which results in another polynomial;  and                                                                      
                                                                      they can                                                                                                                         
(a) Geometric vectors.                                                                                                                                                                                  
                                                                      be multiplied by a scalar  λ  ∈  R,  and  the  result  is  a                                                                     
−2 0 2                                                                polynomial as                                                                                                                     
                                                                                                                                            1                                                           
x                                                                     well. Therefore, polynomials are (rather unusual)  instances                                                                      
                                                                      of vectors.                                                           2                                                           
−6                                                                                                                                                                                                      
                                                                      Note that polynomials  are  very  different  from  geometric          3                                                           
−4                                                                    vectors. While                                                                                                                    
                                                                                                                                                                                                       
−2                                                                    geometric vectors are concrete “drawings”,  polynomials  are                                                                      
                                                                      abstract                                                                                          ∈                             R

%%%

3                                                                     vector concepts.                                                      spaces, in which case there is a 1:1 correspondence         
                                                                                                                                                                                                        
(2.1)                                                                 We can add them together and multiply them  by  scalars.  We          between any kind of vector and Rn                           
                                                                      will largely Pavel Grinfeld’s                                                                                                     
is an example of a triplet of numbers. Adding two vectors a,                                                                                . When it is convenient, we will use                        
b ∈ Rn                                                                series on linear                                                                                                                  
                                                                                                                                            intuitions about geometric vectors and consider  array-based
component-wise results in another vector: a + b = c ∈ Rn              algebra:                                                              algorithms.                                                 
                                                                                                                                                                                                        
. Moreover,                                                           http://tinyurl.                                                       One major idea in mathematics is the idea of “closure”. This
                                                                                                                                            is the question: What is the set  of  all  things  that  can
multiplying a ∈ Rn by λ ∈ R results in a scaled vector λa  ∈          com/nahclwm                                                           result from my proposed operations? In the case of  vectors:
Rn                                                                                                                                          What is the set of vectors that can result by               
                                                                      Gilbert Strang’s                                                                                                                  
.                                                                                                                                           starting with a small set of vectors,  and  adding  them  to
                                                                      course on linear                                                      each other and                                              
Considering vectors as elements of Rn                                                                                                                                                                   
                                                                      algebra:                                                              scaling them? This results in a vector space (Section  2.4).
Be careful to check has an additional benefit that                                                                                          The concept of                                              
                                                                      http://tinyurl.                                                                                                                   
whether array                                                                                                                               a vector space and its properties underlie much  of  machine
                                                                      com/29p5q8j                                                           learning. The                                               
operations actually                                                                                                                                                                                     
                                                                      3Blue1Brown series                                                    concepts introduced in this chapter are summarized in Figure
perform vector                                                                                                                              2.2.                                                        
                                                                      on linear algebra:                                                                                                                
operations when                                                                                                                             This chapter is mostly based on the lecture notes and  books
                                                                      https://tinyurl.                                                      by Drumm                                                    
implementing on a                                                                                                                                                                                       
                                                                      com/h5g4kps                                                           and Weil (2001), Strang (2003), Hogben  (2013),  Liesen  and
computer.                                                                                                                                   Mehrmann                                                    
                                                                      focus on vectors in Rn                                                                                                            
it loosely corresponds  to  arrays  of  real  numbers  on  a                                                                                (2015), as well as Pavel Grinfeld’s Linear  Algebra  series.
computer. Many                                                        since most algorithms in linear algebra are formulated in Rn          Other excellent                                             
                                                                                                                                                                                                        
programming languages support array operations, which  allow          . We will see in Chapter 8 that we often consider data to             Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
for convenient implementation  of  algorithms  that  involve                                                                                Feedback: https://mml-book.com.                             
vector operations.                                                    be represented as vectors in Rn                                                                                                   
                                                                                                                                            2.1 Systems of Linear Equations 19                          
Linear algebra focuses on  the  similarities  between  these          . In this book, we will focus  on  finitedimensional  vector                                                                      

%%%

Figure 2.2 A mind                                                     Linear                                                                resources are Gilbert Strang’s Linear Algebra course at  MIT
                                                                                                                                            and the Linear                                              
map of the concepts                                                   independence                                                                                                                      
                                                                                                                                            Algebra Series by 3Blue1Brown.                              
introduced in this                                                    Basis                                                                                                                             
                                                                                                                                            Linear algebra plays an important role in  machine  learning
chapter, along with                                                   Chapter 10                                                            and general mathematics. The  concepts  introduced  in  this
                                                                                                                                            chapter are further expanded to include the idea of geometry
where they are used                                                   Dimensionality                                                        in Chapter 3. In Chapter 5, we                              
                                                                                                                                                                                                        
in other parts of the                                                 reduction                                                             will discuss vector calculus, where a  principled  knowledge
                                                                                                                                            of matrix operations is essential. In Chapter  10,  we  will
book.                                                                 Chapter 12                                                            use projections  (to  be  introduced  in  Section  3.8)  for
                                                                                                                                            dimensionality reduction with principal  component  analysis
Vector                                                                Classification                                                        (PCA). In Chapter 9,  we  will  discuss  linear  regression,
                                                                                                                                            where                                                       
Vector space                                                          Chapter 3                                                                                                                         
                                                                                                                                            linear  algebra   plays   a   central   role   for   solving
Matrix Chapter 5                                                      Analytic geometry                                                     least-squares problems.                                     
                                                                                                                                                                                                        
Vector calculus                                                       composes                                                              2.1 Systems of Linear Equations                             
                                                                                                                                                                                                        
Group                                                                 closure                                                               Systems of linear equations play a central  part  of  linear
                                                                                                                                            algebra. Many                                               
System of                                                             Abelian                                                                                                                           
                                                                                                                                            problems can be formulated as systems of  linear  equations,
linear equations                                                      with +                                                                and linear                                                  
                                                                                                                                                                                                        
Matrix                                                                represents                                                            algebra gives us the tools for solving them.                
                                                                                                                                                                                                        
inverse                                                               represents                                                            Example 2.1                                                 
                                                                                                                                                                                                        
Gaussian                                                              solved by                                                             A company produces products  N1,  .  .  .  ,  Nn  for  which
                                                                                                                                            resources                                                   
elimination                                                           solves                                                                                                                            
                                                                                                                                            R1, . . . , Rm are required. To produce a unit of product Nj
Linear/affine                                                         property of                                                           , aij units of                                              
                                                                                                                                                                                                        
mapping                                                               maximal set                                                           resource Ri are needed, where i = 1, . . . , m and j = 1,  .
                                                                                                                                            .                  .                  ,                   n.

%%%

The objective is to find an optimal production plan, i.e., a          where aij ∈ R and bi ∈ R.                                             From the first and third equation, it follows that x1  =  1.
plan of how                                                                                                                                 From (1)+(2),                                               
                                                                      system of linear Equation (2.3) is the  general  form  of  a                                                                      
many units xj of product Nj should be produced if a total of          system of linear equations, and                                       we get 2x1 + 3x3 = 5, i.e., x3 = 1. From (3),  we  then  get
bi units of                                                                                                                                 that x2 = 1.                                                
                                                                      equations x1, . . . , xn are the unknowns  of  this  system.                                                                      
resource Ri are available and  (ideally)  no  resources  are          Every n-tuple (x1, . . . , xn) ∈                                      Therefore, (1,  1,  1)  is  the  only  possible  and  unique
left over.                                                                                                                                  solution (verify that                                       
                                                                      Rn                                                                                                                                
If we produce x1, . . . ,  xn  units  of  the  corresponding                                                                                (1, 1, 1) is a solution by plugging in).                    
products, we need                                                     solution that satisfies (2.3) is a solution  of  the  linear                                                                      
                                                                      equation system.                                                      As a third example, we consider                             
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                                                                            
by Cambridge University Press (2020).                                 Example 2.2                                                           x1 + x2 + x3 = 3 (1)                                        
                                                                                                                                                                                                        
20 Linear Algebra                                                     The system of linear equations                                        x1 − x2 + 2x3 = 2 (2)                                       
                                                                                                                                                                                                        
a total of                                                            x1 + x2 + x3 = 3 (1)                                                  2x1 + 3x3 = 5 (3)                                           
                                                                                                                                                                                                        
ai1x1 + · · · + ainxn (2.2)                                           x1 − x2 + 2x3 = 2 (2)                                                 . (2.6)                                                     
                                                                                                                                                                                                        
many units of resource Ri                                             2x1 + 3x3 = 1 (3)                                                     Since  (1)+(2)=(3),  we  can   omit   the   third   equation
                                                                                                                                            (redundancy). From                                          
. An optimal production plan (x1, . . . , xn) ∈ Rn                    (2.4)                                                                                                                             
                                                                                                                                            (1) and (2), we get 2x1 = 5−3x3 and 2x2 = 1+x3. We define x3
,                                                                     has no solution:  Adding  the  first  two  equations  yields          = a ∈ R                                                     
                                                                      2x1+3x3 = 5, which                                                                                                                
therefore, has to satisfy the following system of equations:                                                                                as a free variable, such that any triplet                   
                                                                      contradicts the third equation (3).                                                                                               
a11x1 + · · · + a1nxn = b1                                                                                                                                                                             
                                                                      Let us have a look at the system of linear equations                                                                              
.                                                                                                                                           5                                                           
                                                                      x1 + x2 + x3 = 3 (1)                                                                                                              
.                                                                                                                                           2                                                           
                                                                      x1 − x2 + 2x3 = 2 (2)                                                                                                             
.                                                                                                                                           −                                                           
                                                                      x2 + x3 = 2 (3)                                                                                                                   
am1x1 + · · · + amnxn = bm                                                                                                                  3                                                           
                                                                      . (2.5)                                                                                                                           
,                                                      (2.3)                                                                                2                                                           

%%%

a,                                                                    a line.                                                               parallel). An illustration is given in Figure  2.1  for  the
                                                                                                                                            system                                                      
1                                                                     2x1 − 4x2 = 1 4x1 + 4x2 = 5 x1                                                                                                    
                                                                                                                                            4x1 + 4x2 = 5                                               
2                                                                     x2                                                                                                                                
                                                                                                                                            2x1 − 4x2 = 1                                               
+                                                                     is a solution of the system of linear  equations,  i.e.,  we                                                                      
                                                                      obtain a solution                                                     (2.8)                                                       
1                                                                                                                                                                                                       
                                                                      set that contains infinitely many solutions.                          where the solution space is the point (x1, x2) = (1,        
2                                                                                                                                                                                                       
                                                                      In general, for a real-valued system of linear equations  we          1                                                           
a, a                                                                 obtain either                                                                                                                     
                                                                                                                                            4                                                           
, a ∈ R (2.7)                                                         no,  exactly  one,  or  infinitely  many  solutions.  Linear                                                                      
                                                                      regression (Chapter 9)                                                ). Similarly, for three                                     
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                       solves a version of Example 2.1 when  we  cannot  solve  the          variables,  each  linear  equation  determines  a  plane  in
                                                                      system of linear                                                      three-dimensional                                           
2.1 Systems of Linear Equations 21                                                                                                                                                                      
                                                                      equations.                                                            space. When we intersect these  planes,  i.e.,  satisfy  all
Figure 2.1 The                                                                                                                              linear equations at                                         
                                                                      Remark  (Geometric  Interpretation  of  Systems  of   Linear                                                                      
solution space of a                                                   Equations). In a                                                      the same time, we can obtain a solution set that is a plane,
                                                                                                                                            a line, a point                                             
system of two linear                                                  system of linear equations with two variables x1,  x2,  each                                                                      
                                                                      linear equation                                                       or empty (when the planes have no common intersection). ♢   
equations with two                                                                                                                                                                                      
                                                                      defines a line on the x1x2-plane.  Since  a  solution  to  a          For a systematic  approach  to  solving  systems  of  linear
variables can be                                                      system of linear                                                      equations, we                                               
                                                                                                                                                                                                        
geometrically                                                         equations must satisfy  all  equations  simultaneously,  the          will introduce a useful compact  notation.  We  collect  the
                                                                      solution set is the                                                   coefficients aij                                            
interpreted as the                                                                                                                                                                                      
                                                                      intersection of these lines. This intersection set can be  a          into vectors and collect the vectors into matrices. In other
intersection of two                                                   line (if the linear                                                   words, we write                                             
                                                                                                                                                                                                        
lines. Every linear                                                   equations describe the same line), a point, or  empty  (when          the system from (2.3) in the following form:                
                                                                      the lines are                                                                                                                     
equation                                          represents                                                                                                                                           

%%%

                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
a11                                                                   x2 + · · · +                                                          bm                                                          
                                                                                                                                                                                                        
.                                                                                                                                                                                                     
                                                                                                                                                                                                        
.                                                                                                                                                                                                     
                                                                                                                                                                                                        
.                                                                                                                                           (2.9)                                                     
                                                                                                                                                                                                        
am1                                                                   a1n                                                                   ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                                                                                            by Cambridge University Press (2020).                       
                                                                     .                                                                                                                                 
                                                                                                                                            22 Linear Algebra                                           
                                                                     .                                                                                                                                 
                                                                                                                                            ⇐⇒                                                          
                                                                     .                                                                                                                                 
                                                                                                                                                                                                       
x1 +                                                                  amn                                                                                                                               
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                            a11 · · · a1n                                               
                                                                                                                                                                                                      
                                                                                                                                            .                                                           
a12                                                                   xn =                                                                                                                              
                                                                                                                                            .                                                           
.                                                                                                                                                                                                      
                                                                                                                                            .                                                           
.                                                                                                                                                                                                      
                                                                                                                                            .                                                           
.                                                                                                                                                                                                      
                                                                                                                                            .                                                           
am2                                                                   b1                                                                                                                                
                                                                                                                                            .                                                           
                                                                     .                                                                                                                                 
                                                                                                                                            am1            ·             ·             ·             amn

%%%

                                                                     .                                                                     A =                                                         
                                                                                                                                                                                                        
                                                                     bm                                                                                                                               
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                      . (2.10)                                                                                                                       
                                                                                                                                                                                                        
                                                                     In the following,  we  will  have  a  close  look  at  these                                                                     
                                                                      matrices and define computation rules.  We  will  return  to                                                                      
x1                                                                    solving linear equations in Section 2.3.                              a11 a12 · · · a1n                                           
                                                                                                                                                                                                        
.                                                                     2.2 Matrices                                                          a21 a22 · · · a2n                                           
                                                                                                                                                                                                        
.                                                                     Matrices play a central role in linear algebra. They can  be          .                                                           
                                                                      used to compactly represent systems of linear equations, but                                                                      
.                                                                     they also represent linear                                            .                                                           
                                                                                                                                                                                                        
xn                                                                    functions (linear mappings) as we will see later in  Section          .                                                           
                                                                      2.7. Before we                                                                                                                    
                                                                                                                                           .                                                           
                                                                      discuss some of  these  interesting  topics,  let  us  first                                                                      
                                                                     define what a matrix                                                  .                                                           
                                                                                                                                                                                                        
 =                                                                   is and what kind of operations we can do with  matrices.  We          .                                                           
                                                                      will see more                                                                                                                     
                                                                                                                                           .                                                           
                                                                      properties of matrices in Chapter 4.                                                                                              
                                                                                                                                           .                                                           
                                                                      matrix Definition 2.1 (Matrix). With m, n ∈ N a  real-valued                                                                      
                                                                     (m, n) matrix A is                                                    .                                                           
                                                                                                                                                                                                        
b1                                                                    an m·n-tuple of elements aij , i = 1, . . . , m, j = 1, .  .          am1 am2 · · · amn                                           
                                                                      . , n, which is ordered                                                                                                           
.                                                                                                                                                                                                      
                                                                      according to a rectangular scheme consisting of m rows and n                                                                      
.                                                                     columns:                                                                                                                         
                                                                                                                                                                                                        

%%%

                                                                     can be                                                                                                                           
                                                                                                                                                                                                        
                                                                     equivalently represented as  a  ∈  Rmn  by  stacking  all  n           ∈ R                                                       
                                                                      columns of the                                                                                                                    
                                                                                                                                           m×n                                                         
                                                                      matrix into a long vector; see Figure 2.2.                                                                                        
, aij ∈ R . (2.11)                                                                                                                          . (2.12)                                                    
                                                                      2.2.1 Matrix Addition and Multiplication                                                                                          
row By convention (1, n)-matrices are called  rows  and  (m,                                                                                For matrices A ∈ Rm×n                                       
1)-matrices are called                                                The sum of two matrices A ∈ Rm×n                                                                                                  
                                                                                                                                            , B ∈ Rn×k Note the size of the , the elements  cij  of  the
column columns.  These  special  matrices  are  also  called          , B ∈ Rm×n                                                            product                                                     
row/column vectors.                                                                                                                                                                                     
                                                                      is defined as the elementwise sum, i.e.,                              matrices. C = AB ∈ Rm×k are computed as                     
row vector                                                                                                                                                                                              
                                                                      A + B :=                                                              C =                                                         
column vector                                                                                                                                                                                           
                                                                                                                                           np.einsum(’il,                                              
Figure 2.2 By                                                                                                                                                                                           
                                                                                                                                           lj’, A, B) cij =                                            
stacking its                                                                                                                                                                                            
                                                                                                                                           Xn                                                          
columns, a matrix A                                                                                                                                                                                     
                                                                      a11 + b11 · · · a1n + b1n                                             l=1                                                         
can be represented                                                                                                                                                                                      
                                                                      .                                                                     ailblj , i = 1, . . . , m, j = 1, . . . , k. (2.13)         
as a long vector a.                                                                                                                                                                                     
                                                                      .                                                                     Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
re-shape                                                                                                                                    Feedback: https://mml-book.com.                             
                                                                      .                                                                                                                                 
A ∈ R                                                                                                                                       2.2 Matrices 23                                             
                                                                      .                                                                                                                                 
4×2 a ∈ R                                                                                                                                   This means, to compute element cij we multiply the  elements
                                                                      .                                                                     of the ith There are n columns                              
8                                                                                                                                                                                                       
                                                                      .                                                                     in A and n rows in                                          
Rm×n                                                                                                                                                                                                    
                                                                      am1 + bm1 · · · amn + bmn                                             B so that we can                                            
is the set of all real-valued (m, n)-matrices. A ∈ Rm×n                                                                                                                                                 
                                                                                                                                           compute                      ailblj                      for

%%%

l = 1, . . . , n.                                                     B                                                                     ∈ R2×3                                                      
                                                                                                                                                                                                        
Commonly, the dot                                                     k×m                                                                   , B =                                                       
                                                                                                                                                                                                        
product between                                                       = |{z}                                                                                                                           
                                                                                                                                                                                                        
two vectors a, b is                                                   C                                                                                                                                
                                                                                                                                                                                                        
denoted by a⊤b or                                                     n×m                                                                   0 2                                                         
                                                                                                                                                                                                        
⟨a, b⟩.                                                               (2.14)                                                                1 −1                                                        
                                                                                                                                                                                                        
row of A with the jth column of B and sum them up. Later  in          The  product  BA  is  not  defined  if  m  ̸=  n  since  the          0 1                                                         
Section 3.2,                                                          neighboring dimensions                                                                                                            
                                                                                                                                                                                                       
we will call this the dot product of the  corresponding  row          do not match. ♢                                                                                                                   
and column. In                                                                                                                               ∈ R3×2                                                    
                                                                      Remark.  Matrix  multiplication  is  not   defined   as   an                                                                      
cases, where we need to be explicit that we  are  performing          element-wise operation                                                , we obtain                                                 
multiplication,                                                                                                                                                                                         
                                                                      on matrix elements, i.e., cij ̸= aij bij (even if  the  size          AB =                                                        
we  use  the  notation  A  ·  B  to  denote   multiplication          of A, B was chosen appropriately). This kind of element-wise                                                                      
(explicitly showing                                                   multiplication often appears                                                                                                     
                                                                                                                                                                                                        
“·”).                                                                 in    programming     languages     when     we     multiply          1 2 3                                                       
                                                                      (multi-dimensional) arrays                                                                                                        
Remark.  Matrices  can   only   be   multiplied   if   their                                                                                3 2 1                                                      
“neighboring” dimensions                                              with each  other,  and  is  called  a  Hadamard  product.  ♢                                                                      
                                                                      Hadamard product                                                                                                                 
match. For instance, an n × k-matrix  A  can  be  multiplied                                                                                                                                            
with a k × mmatrix B, but only from the left side:                    Example 2.3                                                                                                                      
                                                                                                                                                                                                        
|{z}                                                                  For A =                                                               0 2                                                         
                                                                                                                                                                                                        
A                                                                                                                                          1 −1                                                        
                                                                                                                                                                                                        
n×k                                                                   1 2 3                                                                 0 1                                                         
                                                                                                                                                                                                        
|{z}                                                                  3 2 1                                                                                                                           
                                                                                                                                                                                                        

%%%

 =                                                                                                                                        , we define the identity matrix                             
                                                                                                                                                                                                        
                                                                     6 4 2                                                                 identity matrix                                             
                                                                                                                                                                                                        
2 3                                                                   −2 0 2                                                                In :=                                                       
                                                                                                                                                                                                        
2 5                                                                  3 2 1                                                                                                                            
                                                                                                                                                                                                        
∈ R                                                                                                                                                                                                   
                                                                                                                                                                                                        
2×2                                                                    ∈ R                                                                                                                            
                                                                                                                                                                                                        
, (2.15)                                                              3×3                                                                                                                              
                                                                                                                                                                                                        
BA =                                                                  . (2.16)                                                                                                                         
                                                                                                                                                                                                        
                                                                     Figure 2.3 Even if                                                                                                               
                                                                                                                                                                                                        
                                                                     both matrix                                                                                                                      
                                                                                                                                                                                                        
0 2                                                                   multiplications AB                                                                                                               
                                                                                                                                                                                                        
1 −1                                                                  and BA are                                                                                                                       
                                                                                                                                                                                                        
0 1                                                                   defined, the                                                                                                                     
                                                                                                                                                                                                        
                                                                     dimensions of the                                                     1 0 · · · 0 · · · 0                                         
                                                                                                                                                                                                        
                                                                     results can be                                                        0 1 · · · 0 · · · 0                                         
                                                                                                                                                                                                        
                                                                     different.                                                            .                                                           
                                                                                                                                                                                                        
1 2 3                                                                 From  this  example,  we  can  already   see   that   matrix          .                                                           
                                                                      multiplication is not                                                                                                             
3 2 1                                                                                                                                      .                                                           
                                                                      commutative, i.e., AB ̸= BA; see  also  Figure  2.3  for  an                                                                      
=                                                                     illustration.                                                         .                                                           
                                                                                                                                                                                                        
                                                                     Definition 2.2 (Identity Matrix). In Rn×n                             .                                                           
                                                                                                                                                                                                        

%%%

.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                     ∈ R                                                         
                                                                                                                                                                                                        
.                                                                     .                                                                     n×n                                                         
                                                                                                                                                                                                        
.                                                                     .                                                                     (2.17)                                                      
                                                                                                                                                                                                        
.                                                                     .                                                                     ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                                                                                            by Cambridge University Press (2020).                       
.                                                                     .                                                                                                                                 
                                                                                                                                            24 Linear Algebra                                           
.                                                                     .                                                                                                                                 
                                                                                                                                            as the n × n-matrix containing  1  on  the  diagonal  and  0
.                                                                     .                                                                     everywhere else.                                            
                                                                                                                                                                                                        
.                                                                     .                                                                     Now that we defined matrix multiplication,  matrix  addition
                                                                                                                                            and the                                                     
.                                                                     .                                                                                                                                 
                                                                                                                                            identity matrix, let us have a look at  some  properties  of
.                                                                     0 0 · · · 0 · · · 1                                                   matrices:                                                   
                                                                                                                                                                                                        
0 0 · · · 1 · · · 0                                                                                                                        associativity                                               
                                                                                                                                                                                                        
.                                                                                                                                          Associativity:                                              
                                                                                                                                                                                                        
.                                                                                                                                          ∀A ∈ R                                                      
                                                                                                                                                                                                        
.                                                                                                                                          m×n                                                         
                                                                                                                                                                                                        
.                                                                                                                                          , B ∈ R                                                     
                                                                                                                                                                                                        
.                                                                                                                                          n×p                                                         
                                                                                                                                                                                                        
.                                                                                                                                          , C ∈ R                                                     
                                                                                                                                                                                                        

%%%

p×q                                                                   and rows.                                                             a11 a12                                                     
                                                                                                                                                                                                        
: (AB)C = A(BC) (2.18)                                                B ∈ Rn×n have the property that AB = In = BA.  B  is  called          a21 a22                                                    
                                                                      the                                                                                                                               
distributivity                                                                                                                              ∈ R                                                         
                                                                      inverse of A and denoted by A                                                                                                     
Distributivity:                                                                                                                             2×2                                                         
                                                                      −1                                                                                                                                
∀A, B ∈ R                                                                                                                                   . (2.21)                                                    
                                                                      .                                                                                                                                 
m×n                                                                                                                                         If we multiply A with                                       
                                                                      inverse Unfortunately,  not  every  matrix  A  possesses  an                                                                      
, C, D ∈ R                                                            inverse A                                                             A                                                           
                                                                                                                                                                                                        
n×p                                                                   −1                                                                    ′                                                           
                                                                                                                                                                                                        
: (A + B)C = AC + BC (2.19a)                                          . If this                                                             :=                                                         
                                                                                                                                                                                                        
A(C + D) = AC + AD (2.19b)                                            regular    inverse    does    exist,     A     is     called          a22 −a12                                                    
                                                                      regular/invertible/nonsingular, otherwise                                                                                         
Multiplication with the identity matrix:                                                                                                    −a21 a11                                                   
                                                                      invertible                                                                                                                        
∀A ∈ R                                                                                                                                      (2.22)                                                      
                                                                      nonsingular                                                                                                                       
m×n                                                                                                                                         we obtain                                                   
                                                                      singular/noninvertible. When the matrix inverse  exists,  it                                                                      
: ImA = AIn = A (2.20)                                                is unique. In Secsingular                                             AA′ =                                                       
                                                                                                                                                                                                        
Note that Im ̸= In for m ̸= n.                                        noninvertible                                                                                                                    
                                                                                                                                                                                                        
2.2.2 Inverse and Transpose                                           tion 2.3, we will discuss  a  general  way  to  compute  the          a11a22 − a12a21 0                                           
                                                                      inverse of a matrix                                                                                                               
Definition 2.3 (Inverse). Consider a square matrix A ∈ Rn×n                                                                                 0 a11a22 − a12a21                                          
                                                                      by solving a system of linear equations.                                                                                          
A square matrix . Let matrix                                                                                                                = (a11a22 − a12a21)I .                                      
                                                                      Remark (Existence  of  the  Inverse  of  a  2  ×  2-matrix).                                                                      
possesses the same                                                    Consider a matrix                                                     (2.23)                                                      
                                                                                                                                                                                                        
number of columns                                                     A :=                                                                 Therefore,                                                  
                                                                                                                                                                                                        

%%%

A                                                                     4 4 5                                                                 “primary diagonal”,                                         
                                                                                                                                                                                                        
−1 =                                                                  6 7 7                                                                 “leading diagonal”,                                         
                                                                                                                                                                                                        
1                                                                                                                                          or “major diagonal”)                                        
                                                                                                                                                                                                        
a11a22 − a12a21                                                       , B =                                                               of a matrix A is the                                        
                                                                                                                                                                                                        
a22 −a12                                                                                                                                   collection of entries                                       
                                                                                                                                                                                                        
−a21 a11                                                                                                                                  Aij where i = j.                                            
                                                                                                                                                                                                        
(2.24)                                                                −7 −7 6                                                               In general, A                                               
                                                                                                                                                                                                        
if and only if a11a22 −a12a21 ̸= 0. In Section 4.1, we  will          2 1 −1                                                                ⊤                                                           
see that a11a22 −                                                                                                                                                                                       
                                                                      4 5 −4                                                                can be obtained by writing the columns of A as the rows     
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                                                                                            of A                                                        
                                                                                                                                                                                                        
2.2 Matrices 25                                                        (2.25)                                                              ⊤                                                           
                                                                                                                                                                                                        
a12a21 is the determinant of a 2×2-matrix.  Furthermore,  we          are inverse to each other since AB = I = BA.                          . The following are important  properties  of  inverses  and
can generally                                                                                                                               transposes:                                                 
                                                                      Definition 2.4 (Transpose). For A ∈ Rm×n                                                                                          
use the determinant to check whether a matrix is invertible.                                                                                The scalar case of                                          
♢                                                                     the matrix B ∈ Rn×m with                                                                                                          
                                                                                                                                            (2.28) is                                                   
Example 2.4 (Inverse Matrix)                                          bij = aji is called the transpose of A. We write B = A                                                                            
                                                                                                                                            1                                                           
The matrices                                                          ⊤                                                                                                                                 
                                                                                                                                            2+4 = 1                                                     
A =                                                                   . transpose                                                                                                                       
                                                                                                                                            6                                                           
                                                                     The main diagonal                                                                                                                 
                                                                                                                                            ̸= 1                                                        
                                                                     (sometimes called                                                                                                                 
                                                                                                                                            2 + 1                                                       
1 2 1                                                                 “principal diagonal”,                                                                                                             
                                                                                                                                            4                                                           

%%%

.                                                                     ⊤ + B                                                                 −1                                                          
                                                                                                                                                                                                        
AA−1 = I = A                                                          ⊤                                                                     )                                                           
                                                                                                                                                                                                        
−1A (2.26)                                                            (2.30)                                                                ⊤ = (A                                                      
                                                                                                                                                                                                        
(AB)                                                                  (AB)                                                                  ⊤                                                           
                                                                                                                                                                                                        
−1 = B                                                                ⊤ = B                                                                 )                                                           
                                                                                                                                                                                                        
−1A                                                                   ⊤A                                                                    −1 =: A                                                     
                                                                                                                                                                                                        
−1                                                                    ⊤                                                                     −⊤                                                          
                                                                                                                                                                                                        
(2.27)                                                                (2.31)                                                                .                                                           
                                                                                                                                                                                                        
(A + B)                                                               Definition 2.5 (Symmetric Matrix). A matrix A ∈ Rn×n                  Remark (Sum and Product of Symmetric Matrices). The  sum  of
                                                                                                                                            symmetric matrices A, B ∈ Rn×n                              
−1                                                                    is symmetric if symmetric matrix                                                                                                  
                                                                                                                                            is always symmetric. However, although their                
̸= A                                                                  A = A                                                                                                                             
                                                                                                                                            product is always defined, it is generally not symmetric:   
−1 + B                                                                ⊤                                                                                                                                 
                                                                                                                                                                                                       
−1                                                                    .                                                                                                                                 
                                                                                                                                            1 0                                                         
(2.28)                                                                Note that only (n, n)-matrices can be symmetric.  Generally,                                                                      
                                                                      we call                                                               0 0 1 1                                                   
(A                                                                                                                                                                                                      
                                                                      (n, n)-matrices also square matrices  because  they  possess          1 1                                                        
⊤                                                                     the same num- square matrix                                                                                                       
                                                                                                                                            =                                                           
)                                                                     ber of rows and columns. Moreover, if A is invertible,  then                                                                      
                                                                      so is A                                                                                                                          
⊤ = A (2.29)                                                                                                                                                                                            
                                                                      ⊤                                                                     1 1                                                         
(A + B)                                                                                                                                                                                                 
                                                                      , and                                                                 0 0                                                        
⊤ = A                                                                                                                                                                                                   
                                                                      (A                                                                    .                                                     (2.32)

%%%

♢                                                                     ⊤                                                                                                                                
                                                                                                                                                                                                        
2.2.3 Multiplication by a Scalar                                      λ = λC                                                                λ + ψ 2λ + 2ψ                                               
                                                                                                                                                                                                        
Let us look at  what  happens  to  matrices  when  they  are          ⊤                                                                     3λ + 3ψ 4λ + 4ψ                                             
multiplied by a                                                                                                                                                                                         
                                                                      since λ = λ                                                                                                                      
scalar λ ∈ R. Let A ∈ Rm×n and λ ∈ R. Then λA = K, Kij  =  λ                                                                                                                                            
aij .                                                                 ⊤ for all λ ∈ R. distributivity                                       (2.34a)                                                     
                                                                                                                                                                                                        
Practically, λ scales each element of A. For λ, ψ ∈  R,  the          Distributivity:                                                       =                                                           
following holds:                                                                                                                                                                                        
                                                                      (λ + ψ)C = λC + ψC, C ∈ Rm×n                                                                                                     
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                                                                            
by Cambridge University Press (2020).                                 λ(B + C) = λB + λC, B, C ∈ Rm×n                                       λ 2λ                                                        
                                                                                                                                                                                                        
26 Linear Algebra                                                     Example 2.5 (Distributivity)                                          3λ 4λ                                                       
                                                                                                                                                                                                        
associativity Associativity:                                          If we define                                                                                                                     
                                                                                                                                                                                                        
(λψ)C = λ(ψC), C ∈ Rm×n                                               C :=                                                                 +                                                           
                                                                                                                                                                                                        
λ(BC) = (λB)C = B(λC) = (BC)λ, B ∈ Rm×n                               1 2                                                                                                                              
                                                                                                                                                                                                        
, C ∈ Rn×k                                                            3 4                                                                  ψ 2ψ                                                        
                                                                                                                                                                                                        
.                                                                     , (2.33)                                                              3ψ 4ψ                                                       
                                                                                                                                                                                                        
Note that this allows us to move scalar values around.                then for any λ, ψ ∈ R we obtain                                                                                                  
                                                                                                                                                                                                        
(λC)                                                                  (λ + ψ)C =                                                            = λC + ψC . (2.34b)                                         
                                                                                                                                                                                                        
⊤ = C                                                                                                                                      2.2.4 Compact Representations of Systems of Linear Equations
                                                                                                                                                                                                        
⊤                                                                     (λ + ψ)1 (λ + ψ)2                                                     If we consider the system of linear equations               
                                                                                                                                                                                                        
λ                                                                     (λ + ψ)3 (λ + ψ)4                                                    2x1 + 3x2 + 5x3 = 1                                         
                                                                                                                                                                                                        
⊤ = C                                                                 =                                                                     4x1 − 2x2 − 7x3 = 8                                         
                                                                                                                                                                                                        

%%%

9x1 + 5x2 − 3x3 = 2                                                   1                                                                     .                                                           
                                                                                                                                                                                                        
(2.35)                                                                8                                                                     .                                                           
                                                                                                                                                                                                        
and use the rules for matrix multiplication,  we  can  write          2                                                                     am1x1 + · · · + amnxn = bm ,                                
this equation                                                                                                                                                                                           
                                                                                                                                           (2.37)                                                      
system in a more compact form as                                                                                                                                                                        
                                                                       . (2.36)                                                            where aij ∈ R and bi ∈ R are  known  constants  and  xj  are
                                                                                                                                           unknowns,                                                   
                                                                      Note that x1 scales the first column, x2 the second one, and                                                                      
                                                                     x3 the third                                                          i = 1, . . . , m, j = 1, . . . , n. Thus far,  we  saw  that
                                                                                                                                            matrices can be used as                                     
2 3 5                                                                 one.                                                                                                                              
                                                                                                                                            a compact way of formulating systems of linear equations  so
4 −2 −7                                                               Generally, a system of linear  equations  can  be  compactly          that we can                                                 
                                                                      represented in                                                                                                                    
9 5 −3                                                                                                                                      write Ax = b, see (2.10). Moreover, we defined basic  matrix
                                                                      their matrix form as Ax = b; see (2.3), and the  product  Ax          operations,                                                 
                                                                     is a (linear)                                                                                                                     
                                                                                                                                            such as addition and  multiplication  of  matrices.  In  the
                                                                     combination of the columns of  A.  We  will  discuss  linear          following, we will                                          
                                                                      combinations in                                                                                                                   
                                                                                                                                           focus on solving systems of linear equations and provide  an
                                                                      more detail in Section 2.5.                                           algorithm for                                               
                                                                                                                                                                                                       
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          finding the inverse of a matrix.                            
x1                                                                    Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                            2.3.1 Particular and General Solution                       
x2                                                                    2.3 Solving Systems of Linear Equations 27                                                                                        
                                                                                                                                            Before discussing how to generally solve systems  of  linear
x3                                                                    2.3 Solving Systems of Linear Equations                               equations, let                                              
                                                                                                                                                                                                        
                                                                     In (2.3), we introduced the  general  form  of  an  equation          us have a  look  at  an  example.  Consider  the  system  of
                                                                      system, i.e.,                                                         equations                                                   
 =                                                                                                                                                                                                     
                                                                      a11x1 + · · · + a1nxn = b1                                                                                                       
                                                                                                                                                                                                       
                                                                      .                                                                     1 0 8 −4                                                    
                                                                                                                                                                                                       

%%%

0 1 2 12                                                             equations is                                                          + 8                                                        
                                                                                                                                                                                                        
                                                                     in a particularly easy form, where  the  first  two  columns          0                                                           
                                                                      consist of a 1                                                                                                                    
                                                                                                                                           1                                                           
                                                                      P                                                                                                                                 
                                                                                                                                                                                                      
                                                                      and a 0. Remember that we want to find scalars x1, . .  .  ,                                                                      
                                                                     x4, such that                                                         . (2.39)                                                    
                                                                                                                                                                                                        
x1                                                                    4                                                                     Therefore, a solution is [42, 8, 0, 0]⊤.  This  solution  is
                                                                                                                                            called a particular particular solution                     
x2                                                                    i=1 xici = b, where we define ci to be the ith column of the                                                                      
                                                                      matrix and                                                            solution or special solution. However, this is not the  only
x3                                                                                                                                          solution of this special solution                           
                                                                      b the right-hand-side of (2.38). A solution to  the  problem                                                                      
x4                                                                    in (2.38) can                                                         system  of  linear  equations.  To  capture  all  the  other
                                                                                                                                            solutions, we need                                          
                                                                     be found immediately by taking 42 times the first column and                                                                      
                                                                      8 times the                                                           to be creative in generating 0 in a  non-trivial  way  using
                                                                                                                                           the columns of                                              
                                                                      second column so that                                                                                                             
                                                                                                                                           the matrix: Adding 0 to our special solution does not change
                                                                      b =                                                                   the special                                                 
 =                                                                                                                                                                                                     
                                                                                                                                           solution. To do so, we express the third  column  using  the
                                                                                                                                           first two columns                                           
                                                                      42                                                                                                                                
42                                                                                                                                          (which are of this very simple form)                        
                                                                      8                                                                                                                                 
8                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                           8                                                           
                                                                      = 42                                                                                                                             
. (2.38)                                                                                                                                    2                                                           
                                                                      1                                                                                                                                 
The system has two equations and four  unknowns.  Therefore,                                                                                                                                           
in general                                                            0                                                                                                                                 
                                                                                                                                            = 8                                                        
we would expect infinitely many solutions.  This  system  of                                                                                                                                           

%%%

1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                          λ2                                                          
                                                                                                                                                                                                        
+ 2                                                                  8                                                                                                                                
                                                                                                                                                                                                        
0                                                                     2                                                                                                                                
                                                                                                                                                                                                        
1                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
(2.40)                                                                                                                                     −4                                                          
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                               12                                                          
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                                                                                           0                                                           
28 Linear Algebra                                                                                                                                                                                       
                                                                                                                                           −1                                                          
so that 0 = 8c1 + 2c2 − 1c3 + 0c4 and (x1, x2, x3, x4) = (8,                                                                                                                                            
2, −1, 0). In                                                                                                                                                                                         
                                                                                                                                                                                                        
fact, any scaling of this solution by λ1 ∈ R produces the  0           = λ1(8c1 + 2c2 − c3) = 0 . (2.41)                                                                                            
vector, i.e.,                                                                                                                                                                                           
                                                                      Following the same line of reasoning, we express the  fourth                                                                     
                                                                     column of the                                                                                                                     
                                                                                                                                                                                                       
1 0 8 −4                                                              matrix in (2.38) using the first two  columns  and  generate                                                                      
                                                                      another set of                                                                                                                   
0 1 2 12                                                                                                                                                                                               
                                                                      non-trivial versions of 0 as                                           = λ2(−4c1 + 12c2 − c4) = 0 (2.42)                       
                                                                                                                                                                                                       
                                                                                                                                           for any λ2 ∈ R. Putting everything together, we  obtain  all
                                                                                                                                         solutions of the                                            
                                                                      1 0 8 −4                                                                                                                          
λ1                                                                                                                                          general solution equation system in (2.38), which is  called
                                                                      0 1 2 12                                                             the general solution, as the set                            
                                                                                                                                                                                                       

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                   
                                                                                                                                                                                                        
x ∈ R                                                                 8                                                                                                                                
                                                                                                                                                                                                        
4                                                                     2                                                                     , λ1, λ2 ∈ R                                                
                                                                                                                                                                                                        
: x =                                                                 −1                                                                                                                               
                                                                                                                                                                                                        
                                                                     0                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                          . (2.43)                                                    
                                                                                                                                                                                                        
                                                                                                                                          Remark. The general approach we followed  consisted  of  the
                                                                                                                                            following                                                   
42                                                                                                                                                                                                     
                                                                                                                                            three steps:                                                
8                                                                     + λ2                                                                                                                              
                                                                                                                                            1. Find a particular solution to Ax = b.                    
0                                                                                                                                                                                                      
                                                                                                                                            2. Find all solutions to Ax = 0.                            
0                                                                                                                                                                                                      
                                                                                                                                            3. Combine the solutions from steps 1. and 2. to the general
                                                                                                                                          solution.                                                   
                                                                                                                                                                                                        
                                                                                                                                          Neither the general nor the particular solution is unique. ♢
                                                                                                                                                                                                        
                                                                     −4                                                                    The system of linear equations in the preceding example  was
                                                                                                                                            easy to                                                     
                                                                     12                                                                                                                                
                                                                                                                                            solve because the matrix in  (2.38)  has  this  particularly
+ λ1                                                                  0                                                                     convenient form,                                            
                                                                                                                                                                                                        
                                                                     −1                                                                    which allowed us to find  the  particular  and  the  general
                                                                                                                                            solution by inspection. However,  general  equation  systems

%%%

are not of this simple form.                                          R\{0}                                                                 4 −8 3 −3 1 2                                               
                                                                                                                                                                                                        
Fortunately, there exists a constructive algorithmic way  of          Addition of two equations (rows)                                      1 −2 1 −1 1 0                                               
transforming                                                                                                                                                                                            
                                                                      Example 2.6                                                           1 −2 0 −3 4 a                                               
any system of linear equations into this particularly simple                                                                                                                                            
form: Gaussian                                                        For a ∈ R, we seek all solutions of the following system  of                                                                     
                                                                      equations:                                                                                                                        
elimination. Key  to  Gaussian  elimination  are  elementary                                                                                                                                           
transformations                                                       −2x1 + 4x2 − 2x3 − x4 + 4x5 = −3                                                                                                  
                                                                                                                                                                                                       
of systems of linear equations, which transform the equation          4x1 − 8x2 + 3x3 − 3x4 + x5 = 2                                                                                                    
system into                                                                                                                                                                                            
                                                                      x1 − 2x2 + x3 − x4 + x5 = 0                                                                                                       
a simple form. Then, we can apply the  three  steps  to  the                                                                                Swap with R3                                                
simple form that                                                      x1 − 2x2 − 3x4 + 4x5 = a                                                                                                          
                                                                                                                                            Swap with R1                                                
we just discussed in the context of the example in (2.38).            . (2.44)                                                                                                                          
                                                                                                                                            where we used the vertical line to  separate  the  left-hand
2.3.2 Elementary Transformations                                      We start by converting this system  of  equations  into  the          side from the                                               
                                                                      compact matrix                                                                                                                    
elementary Key to solving a system of linear  equations  are                                                                                right-hand  side  in  (2.44).  We  use  ⇝  to   indicate   a
elementary transformations                                            notation Ax = b.  We  no  longer  mention  the  variables  x          transformation of the                                       
                                                                      explicitly and                                                                                                                    
transformations that keep the solution  set  the  same,  but                                                                                augmented  matrix  using  elementary  transformations.   The
that transform the equation system                                    build the augmented matrix (in the form                               augmented                                                   
                                                                                                                                                                                                        
into a simpler form:                                                  A | b                                                                 matrix                                                      
                                                                                                                                                                                                        
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          ) augmented matrix                                                    A | b                                                       
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                                                                                           compactly                                                   
2.3 Solving Systems of Linear Equations 29                                                                                                                                                              
                                                                                                                                           represents the                                              
Exchange of two equations (rows in the  matrix  representing                                                                                                                                            
the system                                                                                                                                 system of linear                                            
                                                                                                                                                                                                        
of equations)                                                                                                                              equations Ax = b.                                           
                                                                                                                                                                                                        
Multiplication of an equation (row)  with  a  constant  λ  ∈          −2          4          −2          −1          4          −3          Swapping      Rows      1      and      3      leads      to

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1 −2 1 −1 1 0                                                                                                                    
                                                                                                                                                                                                        
                                                                     0 0 −1 1 −3 2                                                                                                                    
                                                                                                                                                                                                        
                                                                     0 0 0 −3 6 −3                                                         ·(−1)                                                       
                                                                                                                                                                                                        
1 −2 1 −1 1 0                                                         0 0 −1 −2 3 a                                                         ·(−                                                         
                                                                                                                                                                                                        
4 −8 3 −3 1 2                                                                                                                              1                                                           
                                                                                                                                                                                                        
−2 4 −2 −1 4 −3                                                                                                                            3                                                           
                                                                                                                                                                                                        
1 −2 0 −3 4 a                                                                                                                              )                                                           
                                                                                                                                                                                                        
                                                                                                                                          ⇝                                                           
                                                                                                                                                                                                        
                                                                     −R2 − R3                                                                                                                         
                                                                                                                                                                                                        
                                                                     ⇝                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
−4R1                                                                                                                                                                                                  
                                                                                                                                                                                                        
+2R1                                                                                                                                       1 −2 1 −1 1 0                                               
                                                                                                                                                                                                        
−R1                                                                                                                                        0 0 1 −1 3 −2                                               
                                                                                                                                                                                                        
When we  now  apply  the  indicated  transformations  (e.g.,          1 −2 1 −1 1 0                                                         0 0 0 1 −2 1                                                
subtract Row 1                                                                                                                                                                                          
                                                                      0 0 −1 1 −3 2                                                         0 0 0 0 0 a+ 1                                              
four times from Row 2), we obtain                                                                                                                                                                       
                                                                      0 0 0 −3 6 −3                                                                                                                    
                                                                                                                                                                                                       
                                                                      0 0 0 0 0 a+ 1                                                                                                                   
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       

%%%

                                                                     x1                                                                    −1                                                          
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          x2                                                                    1                                                           
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      x3                                                                    0                                                           
30 Linear Algebra                                                                                                                                                                                       
                                                                      x4                                                                                                                               
row-echelon form This (augmented) matrix is in a  convenient                                                                                                                                            
form, the row-echelon form                                            x5                                                                                                                               
                                                                                                                                                                                                        
(REF).  Reverting  this  compact  notation  back  into   the                                                                                                                                          
explicit notation with                                                                                                                                                                                  
                                                                                                                                                                                                      
the variables we seek, we obtain                                                                                                                                                                        
                                                                                                                                                                                                      
x1 − 2x2 + x3 − x4 + x5 = 0                                                                                                                                                                             
                                                                                                                                                                                                      
x3 − x4 + 3x5 = −2                                                                                                                                                                                      
                                                                                                                                           . (2.46)                                                    
x4 − 2x5 = 1                                                                                                                                                                                            
                                                                                                                                           general solution The general solution,  which  captures  the
0 = a + 1                                                                                                                                   set of all possible solutions, is                           
                                                                      =                                                                                                                                 
. (2.45)                                                                                                                                                                                               
                                                                                                                                                                                                       
particular solution Only for a  =  −1  this  system  can  be                                                                                                                                       
solved. A particular solution is                                                                                                                                                                       
                                                                                                                                                                                                   
                                                                                                                                                                                                      
                                                                                                                                            x ∈ R                                                       
                                                                                                                                                                                                      
                                                                                                                                            5                                                           
                                                                                                                                                                                                      
                                                                                                                                            : x =                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                     2                                                                                                                                 
                                                                                                                                                                                                       
                                                                     0                                                                                                                                 
                                                                                                                                                                                                       

%%%

                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                     2                                                                     −1                                                          
                                                                                                                                                                                                        
                                                                     1                                                                     2                                                           
                                                                                                                                                                                                        
2                                                                     0                                                                     1                                                           
                                                                                                                                                                                                        
0                                                                     0                                                                                                                                
                                                                                                                                                                                                        
−1                                                                    0                                                                                                                                
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          , λ1, λ2 ∈ R                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     + λ2                                                                                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                 
                                                                                                                                                                                                        
+ λ1                                                                                                                                       . (2.47)                                                    
                                                                                                                                                                                                        
                                                                                                                                          In the following, we  will  detail  a  constructive  way  to
                                                                                                                                            obtain a particular                                         
                                                                                                                                                                                                      
                                                                                                                                            and general solution of a system of linear equations.       
                                                                                                                                                                                                      
                                                                                                                                            Remark  (Pivots  and  Staircase  Structure).   The   leading
                                                                                                                                          coefficient of a row                                        
                                                                                                                                                                                                        
                                                                     2                                                                     pivot (first nonzero number from the  left)  is  called  the
                                                                                                                                            pivot              and               is               always

%%%

strictly to the right of the pivot  of  the  row  above  it.          form makes                                                            1                                                           
Therefore, any equation system in  row-echelon  form  always                                                                                                                                            
has a “staircase” structure. ♢                                        Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          0                                                           
                                                                      Feedback: https://mml-book.com.                                                                                                   
row-echelon form Definition 2.6 (Row-Echelon Form). A matrix                                                                                0                                                           
is in row-echelon form if                                             2.3 Solving Systems of Linear Equations 31                                                                                        
                                                                                                                                            0                                                           
All rows that contain only zeros are at the  bottom  of  the          our lives easier when we  need  to  determine  a  particular                                                                      
matrix; correspondingly, all rows that contain at least  one          solution. To do                                                                                                                  
nonzero element are on                                                                                                                                                                                  
                                                                      this, we express the right-hand side of the equation  system                                                                     
top of rows that contain only zeros.                                  using the pivot                                                                                                                   
                                                                                                                                                                                                       
Looking at nonzero rows only, the first nonzero number  from          columns, such that b =                                                                                                            
the left                                                                                                                                                                                               
                                                                      PP                                                                                                                                
pivot (also called the pivot or the leading coefficient)  is                                                                                + λ2                                                        
always strictly to the                                                i=1 λipi                                                                                                                          
                                                                                                                                                                                                       
leading coefficient right of the pivot of the row above it.           , where pi                                                                                                                        
                                                                                                                                                                                                       
In other texts, it is                                                 , i = 1, . . . , P, are the pivot                                                                                                 
                                                                                                                                                                                                       
sometimes required                                                    columns. The λi are determined easiest if we start with  the                                                                      
                                                                      rightmost pivot                                                                                                                  
that the pivot is 1.                                                                                                                                                                                    
                                                                      column and work our way to the left.                                  1                                                           
Remark   (Basic   and   Free   Variables).   The   variables                                                                                                                                            
corresponding to the                                                  In the previous example, we would try to find λ1, λ2, λ3  so          1                                                           
                                                                      that                                                                                                                              
pivots in the row-echelon form are  called  basic  variables                                                                                0                                                           
and the other                                                         λ1                                                                                                                                
                                                                                                                                            0                                                           
basic variable variables are free variables. For example, in                                                                                                                                           
(2.45), x1, x3, x4 are basic                                                                                                                                                                           
                                                                                                                                                                                                       
free variable variables, whereas x2, x5 are free  variables.                                                                                                                                           
♢                                                                                                                                                                                                      
                                                                                                                                                                                                       
Remark (Obtaining a Particular  Solution).  The  row-echelon                                                                                                                                           

%%%

                                                                     1                                                                     later in Section 2.3.3 because it allows us to determine the
                                                                                                                                            general solution of  a  system  of  linear  equations  in  a
+ λ3                                                                  0                                                                     straightforward way.                                        
                                                                                                                                                                                                        
                                                                                                                                          Gaussian                                                    
                                                                                                                                                                                                        
                                                                                                                                          Remark (Gaussian Elimination). Gaussian  elimination  is  an
                                                                                                                                            algorithm that elimination                                  
                                                                                                                                                                                                      
                                                                                                                                            performs elementary transformations to  bring  a  system  of
                                                                                                                                          linear equations                                            
                                                                                                                                                                                                        
−1                                                                    . (2.48)                                                              into reduced row-echelon form. ♢                            
                                                                                                                                                                                                        
−1                                                                    From here, we find relatively directly that λ3 = 1, λ2 = −1,          Example 2.7 (Reduced Row Echelon Form)                      
                                                                      λ1 = 2. When                                                                                                                      
1                                                                                                                                           Verify that the following matrix is in  reduced  row-echelon
                                                                      we put everything together, we must not forget the non-pivot          form (the pivots                                            
0                                                                     columns                                                                                                                           
                                                                                                                                            are in bold):                                               
                                                                     for  which  we  set  the  coefficients  implicitly   to   0.                                                                      
                                                                      Therefore, we get the                                                 A =                                                         
                                                                                                                                                                                                       
                                                                      particular solution x = [2, 0, −1, 1, 0]⊤. ♢                                                                                     
                                                                                                                                                                                                       
                                                                      Remark (Reduced Row Echelon Form). An equation system is  in                                                                     
 =                                                                   reduced reduced                                                                                                                   
                                                                                                                                            1 3 0 0 3                                                   
                                                                     row-echelon form (also:  row-reduced  echelon  form  or  row                                                                      
                                                                      canonical form) if row-echelon form                                   0 0 1 0 9                                                   
                                                                                                                                                                                                       
                                                                      It is in row-echelon form.                                            0 0 0 1 −4                                                  
                                                                                                                                                                                                       
                                                                      Every pivot is 1.                                                                                                                
                                                                                                                                                                                                       
                                                                      The pivot is the only nonzero entry in its column.                     . (2.49)                                                  
0                                                                                                                                                                                                       
                                                                      ♢                                                                     The key idea for finding the solutions of Ax = 0 is to  look
−2                                                                                                                                          at the nonpivot columns, which we will need to express as  a
                                                                      The reduced row-echelon form will  play  an  important  role          (linear)                   combination                    of

%%%

the pivot columns. The reduced row echelon form  makes  this          the fifth column to obtain 0.  In  the  end,  we  are  still                                                                     
relatively                                                            solving a homogeneous                                                                                                             
                                                                                                                                                                                                       
straightforward, and we express  the  non-pivot  columns  in          equation system.                                                                                                                  
terms of sums                                                                                                                                                                                          
                                                                      To summarize, all solutions of Ax = 0, x ∈ R5 are given by                                                                        
and multiples of the pivot columns that are on  their  left:                                                                                                                                           
The second column is 3 times the first column (we can ignore                                                                                                                                           
the pivot columns on the                                                                                                                                                                               
                                                                                                                                                                                                   
right of the second column). Therefore, to obtain 0, we need                                                                                                                                           
to subtract                                                                                                                                                                                        
                                                                                                                                            + λ2                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          x ∈ R                                                                                                                             
by Cambridge University Press (2020).                                                                                                                                                                  
                                                                      5                                                                                                                                 
32 Linear Algebra                                                                                                                                                                                      
                                                                      : x = λ1                                                                                                                          
the second column from three times the first column. Now, we                                                                                                                                           
look at the                                                                                                                                                                                            
                                                                                                                                                                                                       
fifth column, which is  our  second  non-pivot  column.  The                                                                                                                                           
fifth column can                                                                                                                                                                                       
                                                                                                                                                                                                       
be expressed as 3 times the first pivot column, 9 times  the                                                                                                                                           
second pivot                                                                                                                                                                                           
                                                                                                                                            3                                                           
column, and −4 times the third pivot column. We need to keep                                                                                                                                           
track of                                                                                                                                    0                                                           
                                                                                                                                                                                                       
the indices of the pivot columns and translate this  into  3                                                                                9                                                           
times the first column, 0 times the second column (which  is          3                                                                                                                                 
a non-pivot column), 9 times                                                                                                                −4                                                          
                                                                      −1                                                                                                                                
the third column (which is our second pivot column), and  −4                                                                                −1                                                          
times the                                                             0                                                                                                                                 
                                                                                                                                                                                                       
fourth column (which is the third  pivot  column).  Then  we          0                                                                                                                                 
need to subtract                                                                                                                                                                                       
                                                                      0                                                                                                                                 

%%%

                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
, λ1, λ2 ∈ R                                                                                                                               .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                      .                                                           
                                                                                                                                                                                                        
                                                                 0 · · · 0 1 ∗ · · · ∗ 0 ∗ · · · ∗ 0 ∗ · · · ∗                         .                                                           
                                                                                                                                                                                                        
. (2.50)                                                              .                                                                     .                                                           
                                                                                                                                                                                                        
2.3.3 The Minus-1 Trick                                               .                                                                     .                                                           
                                                                                                                                                                                                        
In the following, we introduce a practical trick for reading          .                                                                     .                                                           
out the solutions  x  of  a  homogeneous  system  of  linear                                                                                                                                            
equations Ax = 0, where                                               .                                                                     .                                                           
                                                                                                                                                                                                        
A ∈ Rk×n                                                              .                                                                     .                                                           
                                                                                                                                                                                                        
, x ∈ Rn                                                              . 0 0 · · · 0 1 ∗ · · · ∗                                             .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
To start, we assume that A is in  reduced  row-echelon  form          .                                                                     .                                                           
without any                                                                                                                                                                                             
                                                                      .                                                                     .                                                           
rows that just contain zeros, i.e.,                                                                                                                                                                     
                                                                      .                                                                     . 0                                                         
A =                                                                                                                                                                                                     
                                                                      .                                                                     .                                                           
                                                                                                                                                                                                       
                                                                      .                                                                     .                                                           
                                                                                                                                                                                                       

%%%

.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     0 · · · 0 0 0 · · · 0 0 0 · · · 0 1 ∗ · · · ∗               
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                     ,                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     (2.51)                                                      
                                                                                                                                                                                                        
.                                                                     .                                                                     where  ∗  can  be  an  arbitrary  real  number,   with   the
                                                                                                                                            constraints that the first                                  
.                                                                     . 0                                                                                                                               
                                                                                                                                            nonzero entry per row must be 1 and all other entries in the
.                                                                     .                                                                     corresponding                                               
                                                                                                                                                                                                        
.                                                                     .                                                                     column must be 0. The columns j1, . . . , jk with the pivots
                                                                                                                                            (marked in                                                  
.                                                                     .                                                                                                                                 
                                                                                                                                            bold) are the standard unit vectors e1, . .  .  ,  ek  ∈  Rk

%%%

. We extend this matrix                                               1 3 0 0 3                                                             0 0 0 0 −1                                                  
                                                                                                                                                                                                        
to an n × n-matrix A˜ by adding n − k rows of the form                0 0 1 0 9                                                                                                                        
                                                                                                                                                                                                        
0 · · · 0 −1 0 · · · 0                                                0 0 0 1 −4                                                                                                                       
                                                                                                                                                                                                        
(2.52)                                                                                                                                                                                                
                                                                                                                                                                                                        
so that the diagonal of the  augmented  matrix  A˜  contains           . (2.53)                                                                                                                       
either 1 or −1.                                                                                                                                                                                         
                                                                      We now augment this matrix to a 5 × 5 matrix by adding  rows                                                                     
Then, the columns of A˜ that contain the −1  as  pivots  are          of the                                                                                                                            
solutions of                                                                                                                                                                                           
                                                                      form (2.52) at the places where the pivots on  the  diagonal                                                                      
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          are missing                                                           . (2.54)                                                    
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                      and obtain                                                            From this form, we can immediately read out the solutions of
2.3 Solving Systems of Linear Equations 33                                                                                                  Ax = 0 by                                                   
                                                                      A˜ =                                                                                                                              
the homogeneous equation system Ax = 0. To be more  precise,                                                                                taking the columns of A˜ , which contain −1 on the diagonal:
these                                                                                                                                                                                                  
                                                                                                                                                                                                       
columns form a basis (Section 2.6.1) of the  solution  space                                                                                                                                           
of Ax = 0,                                                                                                                                                                                         
                                                                                                                                                                                                       
which we will later call  the  kernel  or  null  space  (see                                                                                                                                       
Section 2.7.3). kernel                                                                                                                                                                                 
                                                                                                                                            x ∈ R                                                       
null space                                                                                                                                                                                             
                                                                                                                                            5                                                           
Example 2.8 (Minus-1 Trick)                                                                                                                                                                            
                                                                                                                                            : x = λ1                                                    
Let us revisit the matrix in (2.49),  which  is  already  in          1 3 0 0 3                                                                                                                         
reduced REF:                                                                                                                                                                                           
                                                                      0 −1 0 0 0                                                                                                                        
A =                                                                                                                                                                                                    
                                                                      0 0 1 0 9                                                                                                                         
                                                                                                                                                                                                      
                                                                      0 0 0 1 −4                                                                                                                        
                                                                                                                                                                                                      

%%%

                                                                     3                                                                     of A ∈ Rn×n                                                 
                                                                                                                                                                                                        
                                                                     0                                                                     , we need to find a matrix X                                
                                                                                                                                                                                                        
3                                                                     9                                                                     that satisfies AX = In. Then, X = A                         
                                                                                                                                                                                                        
−1                                                                    −4                                                                    −1                                                          
                                                                                                                                                                                                        
0                                                                     −1                                                                    . We can write this down as                                 
                                                                                                                                                                                                        
0                                                                                                                                          a set of simultaneous linear equations AX  =  In,  where  we
                                                                                                                                            solve for                                                   
0                                                                                                                                                                                                      
                                                                                                                                            X = [x1| · · · |xn]. We use the  augmented  matrix  notation
                                                                                                                                          for a compact                                               
                                                                                                                                                                                                        
                                                                                                                                          representation of this set of systems  of  linear  equations
                                                                                                                                            and obtain                                                  
                                                                                                                                                                                                      
                                                                                                                                            A|In                                                        
                                                                                                                                                                                                      
                                                                                                                                            ⇝ · · · ⇝                                                   
                                                                     , λ1, λ2 ∈ R                                                                                                                      
                                                                                                                                            In|A                                                        
                                                                                                                                                                                                      
                                                                                                                                            −1                                                          
+ λ2                                                                                                                                                                                               
                                                                                                                                            . (2.56)                                                    
                                                                                                                                                                                                  
                                                                                                                                            This means that if we bring the  augmented  equation  system
                                                                     , (2.55)                                                              into reduced                                                
                                                                                                                                                                                                        
                                                                     which is  identical  to  the  solution  in  (2.50)  that  we          row-echelon form,  we  can  read  out  the  inverse  on  the
                                                                      obtained by “insight”.                                                right-hand side of                                          
                                                                                                                                                                                                       
                                                                      Calculating the Inverse                                               the equation system. Hence, determining  the  inverse  of  a
                                                                                                                                           matrix is equivalent to solving systems of linear equations.
                                                                      To compute the inverse A                                                                                                          
                                                                                                                                           ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                      −1                                                                    by      Cambridge       University       Press       (2020).

%%%

34 Linear Algebra                                                                                                                                                                                     
                                                                                                                                                                                                        
Example 2.9  (Calculating  an  Inverse  Matrix  by  Gaussian                                                                                                                                          
Elimination)                                                                                                                                                                                            
                                                                      1 0 2 0 1 0 0 0                                                                                                                  
To determine the inverse of                                                                                                                                                                             
                                                                      1 1 0 0 0 1 0 0                                                       ,                                                           
A =                                                                                                                                                                                                     
                                                                      1 2 0 1 0 0 1 0                                                       such that the desired inverse is  given  as  its  right-hand
                                                                                                                                           side:                                                       
                                                                      1 1 1 1 0 0 0 1                                                                                                                   
                                                                                                                                           A                                                           
                                                                                                                                                                                                       
                                                                                                                                           −1 =                                                        
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
1 0 2 0                                                                                                                                                                                                
                                                                                                                                                                                                       
1 1 0 0                                                                                                                                                                                                
                                                                      and use  Gaussian  elimination  to  bring  it  into  reduced                                                                      
1 2 0 1                                                               row-echelon form                                                                                                                 
                                                                                                                                                                                                        
1 1 1 1                                                                                                                                    −1 2 −2 2                                                   
                                                                                                                                                                                                        
                                                                                                                                          1 −1 2 −2                                                   
                                                                                                                                                                                                        
                                                                                                                                          1 −1 1 −1                                                   
                                                                                                                                                                                                        
                                                                                                                                          −1 0 −1 2                                                   
                                                                                                                                                                                                        
                                                                     1 0 0 0 −1 2 −2 2                                                                                                                
                                                                                                                                                                                                        
(2.57)                                                                0 1 0 0 1 −1 2 −2                                                                                                                
                                                                                                                                                                                                        
we write down the augmented matrix                                    0 0 1 0 1 −1 1 −1                                                                                                                
                                                                                                                                                                                                        
                                                                     0 0 0 1 −1 0 −1 2                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                          .                                                     (2.58)

%%%

We  can  verify  that  (2.58)  is  indeed  the  inverse   by          ⊤Ax = A                                                               In the following, we therefore briefly  discuss  alternative
performing the multiplication AA−1                                                                                                          approaches to                                               
                                                                      ⊤                                                                                                                                 
and observing that we recover I4.                                                                                                           solving systems of linear equations.                        
                                                                      b ⇐⇒ x = (A                                                                                                                       
2.3.4 Algorithms for Solving a System of Linear Equations                                                                                   Gaussian elimination plays an important role when  computing
                                                                      ⊤A)                                                                   determinants  (Section  4.1),  checking  whether  a  set  of
In the following, we briefly discuss approaches to solving a                                                                                vectors is linearly independent (Section 2.5), computing the
system of linear equations of the form Ax = b. We  make  the          −1A                                                                   inverse of a matrix (Section 2.2.2),                        
assumption that  a  solution  exists.  Should  there  be  no                                                                                                                                            
solution, we need to resort to approximate                            ⊤                                                                     computing  the  rank  of  a  matrix  (Section  2.6.2),   and
                                                                                                                                            determining a basis                                         
solutions, which we do not cover in this chapter. One way to          b (2.59)                                                                                                                          
solve the approximate  problem  is  using  the  approach  of                                                                                of a vector space (Section 2.6.1). Gaussian  elimination  is
linear regression, which we                                           Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          an intuitive and                                            
                                                                      Feedback: https://mml-book.com.                                                                                                   
discuss in detail in Chapter 9.                                                                                                             constructive way to solve a system of linear equations  with
                                                                      2.4 Vector Spaces 35                                                  thousands of                                                
In special cases, we may be able to determine the inverse A                                                                                                                                             
                                                                      and use the Moore-Penrose pseudo-inverse (A                           variables. However, for systems with millions of  variables,
−1                                                                                                                                          it is impractical  as  the  required  number  of  arithmetic
                                                                      ⊤A)                                                                   operations scales cubically in the                          
, such                                                                                                                                                                                                  
                                                                      −1A                                                                   number of simultaneous equations.                           
that the solution of Ax = b is given as x = A                                                                                                                                                           
                                                                      ⊤                                                                     In practice, systems of many  linear  equations  are  solved
−1                                                                                                                                          indirectly, by either stationary iterative methods, such  as
                                                                      to determine the Moore-Penrose                                        the Richardson method, the Jacobi  method,  the  Gauß-Seidel
b. However, this is                                                                                                                         method, and the successive over-relaxation                  
                                                                      pseudo-inverse solution (2.59) that solves  Ax  =  b,  which                                                                      
only possible if A is a square matrix and invertible,  which          also corresponds to the minimum norm least-squares solution.          method,  or  Krylov  subspace  methods,  such  as  conjugate
is often not the                                                      A disadvantage of this approach is that                               gradients,  generalized  minimal  residual,  or  biconjugate
                                                                                                                                            gradients. We refer to the books                            
case. Otherwise, under mild assumptions (i.e.,  A  needs  to          it requires many computations for the matrix-matrix  product                                                                      
have linearly                                                         and computing the inverse of A                                        by Stoer and Burlirsch (2002), Strang (2003), and Liesen and
                                                                                                                                            Mehrmann                                                    
independent columns) we can use the transformation                    ⊤A. Moreover, for reasons of numerical precision it                                                                               
                                                                                                                                            (2015) for further details.                                 
Ax = b ⇐⇒ A                                                           is generally not  recommended  to  compute  the  inverse  or                                                                      
                                                                      pseudo-inverse.                                                       Let x∗ be a solution of Ax  =  b.  The  key  idea  of  these

%%%

iterative methods                                                     and we will start by introducing the  concept  of  a  group,          4. Inverse element: ∀x ∈ G ∃y ∈ G : x ⊗ y = e and y ⊗ x = e,
                                                                      which is a set                                                        where e is                                                  
is to set up an iteration of the form                                                                                                                                                                   
                                                                      of elements and an operation defined on these elements  that          the neutral element. We often write x                       
x                                                                     keeps some                                                                                                                        
                                                                                                                                            −1                                                          
(k+1) = Cx(k) + d (2.60)                                              structure of the set intact.                                                                                                      
                                                                                                                                            to denote the inverse element                               
for suitable C and d that reduces the residual error ∥x               ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                      
                                                                      by Cambridge University Press (2020).                                 of x.                                                       
(k+1)−x∗∥ in every                                                                                                                                                                                      
                                                                      36 Linear Algebra                                                     Remark. The inverse element is defined with respect  to  the
iteration and converges to x∗. We will introduce norms  ∥  ·                                                                                operation ⊗                                                 
∥, which allow                                                        2.4.1 Groups                                                                                                                      
                                                                                                                                            and does not necessarily mean 1                             
us to compute similarities between vectors, in Section 3.1.           Groups play an important role in computer  science.  Besides                                                                      
                                                                      providing a                                                           x                                                           
2.4 Vector Spaces                                                                                                                                                                                       
                                                                      fundamental framework  for  operations  on  sets,  they  are          . ♢                                                         
Thus far, we have looked at systems of linear equations  and          heavily used in                                                                                                                   
how to solve                                                                                                                                Abelian group If additionally ∀x, y ∈ G : x ⊗ y  =  y  ⊗  x,
                                                                      cryptography, coding theory, and graphics.                            then G = (G, ⊗) is an Abelian                               
them (Section 2.3). We saw that systems of linear  equations                                                                                                                                            
can be compactly represented  using  matrix-vector  notation          Definition 2.7 (Group). Consider a set G and an operation  ⊗          group (commutative).                                        
(2.10). In the following,                                             : G ×G → G                                                                                                                        
                                                                                                                                            Example 2.10 (Groups)                                       
we will have  a  closer  look  at  vector  spaces,  i.e.,  a          group defined on G. Then G := (G, ⊗) is called  a  group  if                                                                      
structured space in which                                             the following hold:                                                   Let us have a look at some examples of sets with  associated
                                                                                                                                            operations                                                  
vectors live.                                                         closure                                                                                                                           
                                                                                                                                            and see whether they are groups:                            
In  the   beginning   of   this   chapter,   we   informally          1. Closure of G under ⊗: ∀x, y ∈ G : x ⊗ y ∈ G associativity                                                                      
characterized vectors as                                                                                                                    (Z, +) is an Abelian group.                                 
                                                                      2. Associativity: ∀x, y, z ∈ G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)                                                                        
objects that can be  added  together  and  multiplied  by  a                                                                                N (N0, +) is not a  group:  Although  (N0,  +)  possesses  a
scalar, and they                                                      neutral element                                                       neutral element 0 := N ∪ {0}                                
                                                                                                                                                                                                        
remain objects of the  same  type.  Now,  we  are  ready  to          inverse element 3. Neutral element: ∃e ∈ G ∀x ∈ G : x ⊗ e  =          (0), the inverse elements are missing.                      
formalize this,                                                       x and e ⊗ x = x                                                                                                                   
                                                                                                                                            (Z, ·) is not a group: Although (Z, ·)  contains  a  neutral

%%%

element (1), the                                                      –  Closure  and  associativity  follow  directly  from   the          R). However, general linear group                           
                                                                      definition of matrix                                                                                                              
inverse elements for any z ∈ Z, z ̸= ±1, are missing.                                                                                       since matrix multiplication is not commutative, the group is
                                                                      multiplication.                                                       not Abelian.                                                
(R, ·) is not a group since 0 does not  possess  an  inverse                                                                                                                                            
element.                                                              – Neutral element: The identity matrix  In  is  the  neutral          2.4.2 Vector Spaces                                         
                                                                      element with                                                                                                                      
(R\{0}, ·) is Abelian.                                                                                                                      When we discussed groups, we looked  at  sets  G  and  inner
                                                                      respect to matrix multiplication “·” in (Rn×n                         operations on                                               
(Rn                                                                                                                                                                                                     
                                                                      , ·).                                                                 G, i.e., mappings G × G → G that only operate on elements in
, +),(Z                                                                                                                                     G. In the                                                   
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
n                                                                     Feedback: https://mml-book.com.                                       following, we will consider sets  that  in  addition  to  an
                                                                                                                                            inner operation +                                           
, +), n ∈ N are Abelian if + is defined componentwise, i.e.,          2.4 Vector Spaces 37                                                                                                              
                                                                                                                                            also contain an outer operation ·, the multiplication  of  a
(x1, · · · , xn) + (y1, · · · , yn) = (x1 + y1, · · · , xn +          – Inverse element: If the inverse  exists  (A  is  regular),          vector x ∈ G by                                             
yn). (2.61)                                                           then A                                                                                                                            
                                                                                                                                            a scalar λ ∈ R. We can think of the  inner  operation  as  a
Then, (x1, · · · , xn)                                                −1                                                                    form of addition,                                           
                                                                                                                                                                                                        
−1                                                                    is the                                                                and the outer operation as a form of scaling. Note that  the
                                                                                                                                            inner/outer                                                 
:= (−x1, · · · , −xn) is the inverse element and                      inverse element of A ∈ Rn×n                                                                                                       
                                                                                                                                            operations have nothing to do with inner/outer products.    
e = (0, · · · , 0) is the neutral element.                            , and in exactly this case (Rn×n                                                                                                  
                                                                                                                                            Definition 2.9 (Vector Space). A real-valued vector space  V
(Rm×n                                                                 , ·) is a                                                             = (V, +, ·) is vector space                                 
                                                                                                                                                                                                        
,  +),  the  set  of  m  ×  n-matrices  is   Abelian   (with          group, called the general linear group.                               a set V with two operations                                 
componentwise                                                                                                                                                                                           
                                                                      Definition 2.8 (General Linear Group). The  set  of  regular          + : V × V → V (2.62)                                        
addition as defined in (2.61)).                                       (invertible)                                                                                                                      
                                                                                                                                            · : R × V → V (2.63)                                        
Let us have a closer look at (Rn×n                                    matrices A ∈ Rn×n                                                                                                                 
                                                                                                                                            where                                                       
, ·), i.e., the set of n×n-matrices with                              is a group with respect to matrix multiplication as                                                                               
                                                                                                                                            1. (V, +) is an Abelian group                               
matrix    multiplication    as    defined     in     (2.13).          defined in (2.13) and is called general linear  group  GL(n,                                                                      

%%%

2. Distributivity:                                                    38 Linear Algebra                                                     – Addition: A + B =                                         
                                                                                                                                                                                                        
1. ∀λ ∈ R, x, y ∈ V : λ · (x + y) = λ · x + λ · y                     (which we usually do), we can use the matrix  multiplication                                                                     
                                                                      as defined                                                                                                                        
2. ∀λ, ψ ∈ R, x ∈ V : (λ + ψ) · x = λ · x + ψ · x                                                                                                                                                      
                                                                      in (2.13). However, then the dimensions of  the  vectors  do                                                                      
3. Associativity (outer operation): ∀λ, ψ  ∈  R,  x  ∈  V  :          not match. Only                                                                                                                  
λ·(ψ·x) = (λψ)·x                                                                                                                                                                                        
                                                                      the following multiplications for vectors are defined: ab⊤ ∈          a11 + b11 · · · a1n + b1n                                   
4. Neutral element with respect to the outer operation: ∀x ∈          Rn×n                                                                                                                              
V : 1·x = x                                                                                                                                 .                                                           
                                                                      outer product (outer                                                                                                              
The elements x ∈ V are called vectors. The  neutral  element                                                                                .                                                           
of (V, +) is vector                                                   product), a                                                                                                                       
                                                                                                                                            .                                                           
the zero vector 0 = [0, . . . , 0]⊤, and the inner operation          ⊤b ∈ R (inner/scalar/dot product). ♢                                                                                              
+ is called vector vector addition                                                                                                          .                                                           
                                                                      Example 2.11 (Vector Spaces)                                                                                                      
addition. The elements λ ∈ R  are  called  scalars  and  the                                                                                .                                                           
outer operation scalar                                                Let us have a look at some important examples:                                                                                    
                                                                                                                                            .                                                           
· is a multiplication by scalars. Note that a scalar product          V = Rn                                                                                                                            
is something multiplication by                                                                                                              am1 + bm1 · · · amn + bmn                                   
                                                                      , n ∈ N  is  a  vector  space  with  operations  defined  as                                                                      
different, and we will get to this in Section 3.2 scalars             follows:                                                                                                                         
                                                                                                                                                                                                        
.                                                                     – Addition: x+y = (x1, . . . ,  xn)+(y1,  .  .  .  ,  yn)  =                                                                     
                                                                      (x1+y1, . . . , xn+yn)                                                                                                            
Remark. A “vector multiplication” ab, a, b ∈ Rn                                                                                                                                                        
                                                                      for all x, y ∈ Rn                                                                                                                 
,  is  not  defined.  Theoretically,  we  could  define   an                                                                                is defined elementwise for all A, B ∈ V                     
element-wise multiplication, such that c = ab                         – Multiplication by scalars: λx = λ(x1, . . . , xn) =  (λx1,                                                                      
                                                                      . . . , λxn) for                                                      – Multiplication by scalars: λA =                           
with cj = aj bj . This “array multiplication” is  common  to                                                                                                                                            
many programming languages but makes mathematically  limited          all λ ∈ R, x ∈ Rn                                                                                                                
sense using the standard rules for matrix multiplication: By                                                                                                                                            
treating vectors as n × 1 matrices                                    V = Rm×n                                                                                                                         
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          , m, n ∈ N is a vector space with                                                                                                
by      Cambridge       University       Press       (2020).                                                                                                                                            

%%%

λa11 · · · λa1n                                                       simplify                                                               . (2.64)                                                  
                                                                                                                                                                                                        
.                                                                     notation. ♢                                                           This  simplifies  the  notation   regarding   vector   space
                                                                                                                                            operations. However,                                        
.                                                                     Remark. The vector spaces Rn                                                                                                      
                                                                                                                                            we do distinguish between Rn×1 and R1×n                     
.                                                                     , Rn×1                                                                                                                            
                                                                                                                                            row vector (the row vectors) to avoid confusion with  matrix
.                                                                     , R1×n are only different in the way                                  multiplication. By default, we write x to  denote  a  column
                                                                                                                                            vector, and a row vector is denoted by x                    
.                                                                     we write vectors. In the  following,  we  will  not  make  a                                                                      
                                                                      distinction between                                                   ⊤ transpose , the transpose of x. ♢                         
.                                                                                                                                                                                                       
                                                                      Rn and Rn×1                                                           Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
λam1 · · · λamn                                                                                                                             Feedback: https://mml-book.com.                             
                                                                      column vector , which allows us to write n-tuples as  column                                                                      
                                                                     vectors                                                               2.4 Vector Spaces 39                                        
                                                                                                                                                                                                        
                                                                     x =                                                                   2.4.3 Vector Subspaces                                      
                                                                                                                                                                                                        
                                                                                                                                          In  the  following,  we  will  introduce  vector  subspaces.
                                                                                                                                            Intuitively, they are                                       
as defined in                                                                                                                                                                                          
                                                                                                                                            sets  contained  in  the  original  vector  space  with  the
Section 2.2. Remember that Rm×n                                                                                                            property that when                                          
                                                                                                                                                                                                        
is equivalent to Rmn                                                  x1                                                                    we perform vector space operations on elements  within  this
                                                                                                                                            subspace, we                                                
.                                                                     .                                                                                                                                 
                                                                                                                                            will never leave it.  In  this  sense,  they  are  “closed”.
V = C, with the standard definition of addition  of  complex          .                                                                     Vector subspaces are a                                      
numbers.                                                                                                                                                                                                
                                                                      .                                                                     key idea  in  machine  learning.  For  example,  Chapter  10
Remark. In the following, we will denote a vector space  (V,                                                                                demonstrates how                                            
+, ·) by V                                                            xn                                                                                                                                
                                                                                                                                            to use vector subspaces for dimensionality reduction.       
when + and · are the standard  vector  addition  and  scalar                                                                                                                                           
multiplication.                                                                                                                             Definition 2.10 (Vector Subspace). Let V = (V, +,  ·)  be  a
                                                                                                                                           vector space                                                
Moreover, we will use the notation x ∈ V for vectors in V to                                                                                                                                            

%%%

and U ⊆ V, U ̸= ∅. Then U =  (U,  +,  ·)  is  called  vector          (with the usual inner/                                                subspace.                                                   
subspace of V (or vector subspace                                                                                                                                                                       
                                                                      outer operations). In A  and  C,  the  closure  property  is          0 0 0 0                                                     
linear subspace) if U is a  vector  space  with  the  vector          violated; B does                                                                                                                  
space operations + linear subspace                                                                                                          A                                                           
                                                                      not contain 0.                                                                                                                    
and · restricted to U ×U and R×U. We write U ⊆ V to denote a                                                                                B                                                           
subspace                                                              The solution set of a homogeneous system of linear equations                                                                      
                                                                      Ax = 0                                                                C                                                           
U of V .                                                                                                                                                                                                
                                                                      with n unknowns x = [x1, . . . , xn]                                  D                                                           
If U ⊆ V and V is a vector space, then U naturally  inherits                                                                                                                                            
many properties directly from V because they hold for all  x          ⊤ is a subspace of Rn                                                 ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
∈ V, and in particular for                                                                                                                  by Cambridge University Press (2020).                       
                                                                      .                                                                                                                                 
all x ∈ U ⊆ V. This includes the Abelian  group  properties,                                                                                40 Linear Algebra                                           
the  distributivity,  the  associativity  and  the   neutral          The solution of an inhomogeneous system of linear  equations                                                                      
element. To determine whether                                         Ax =                                                                  Remark. Every subspace U ⊆ (Rn                              
                                                                                                                                                                                                        
(U, +, ·) is a subspace of V we still do need to show                 b, b ̸= 0 is not a subspace of Rn                                     , +, ·) is the solution space of  a  homogeneous  system  of
                                                                                                                                            linear equations Ax = 0 for x ∈ Rn                          
1. U ̸= ∅, in particular: 0 ∈ U                                       .                                                                                                                                 
                                                                                                                                            . ♢                                                         
2. Closure of U:                                                      The intersection of arbitrarily many subspaces is a subspace                                                                      
                                                                      itself.                                                               2.5 Linear Independence                                     
a. With respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx ∈                                                                                                                                            
U.                                                                    Figure 2.1 Not all                                                    In the following, we will have a close look at what  we  can
                                                                                                                                            do with vectors                                             
b. With respect to the inner operation: ∀x, y ∈ U : x + y  ∈          subsets of R2 are                                                                                                                 
U.                                                                                                                                          (elements of the vector space). In particular,  we  can  add
                                                                      subspaces. In A and                                                   vectors together                                            
Example 2.12 (Vector Subspaces)                                                                                                                                                                         
                                                                      C, the closure                                                        and  multiply  them  with  scalars.  The  closure   property
Let us have a look at some examples:                                                                                                        guarantees that we                                          
                                                                      property is violated;                                                                                                             
For every vector space V  ,  the  trivial  subspaces  are  V                                                                                end up with another vector in the same vector space.  It  is
itself and {0}.                                                       B does not contain                                                    possible to find                                            
                                                                                                                                                                                                        
Only example D in Figure 2.1 is a subspace of R2                      0. Only D is a                                                        a set of vectors with which we can represent every vector in
                                                                                                                                            the                                                   vector

%%%

space by adding them together and scaling them. This set  of          set of vectors to                                                     Example 2.13 (Linearly Dependent Vectors)                   
vectors is                                                                                                                                                                                              
                                                                      represent 0, i.e., linear combinations of vectors x1, . .  .          A geographic example may help  to  clarify  the  concept  of
a basis, and we will discuss them in Section  2.6.1.  Before          , xk, where not all                                                   linear independence. A person in Nairobi (Kenya)  describing
we get there,                                                                                                                               where Kigali (Rwanda) is                                    
                                                                      coefficients λi                                                                                                                   
we  will  need  to  introduce   the   concepts   of   linear                                                                                might say ,“You can get to Kigali  by  first  going  506  km
combinations and linear                                               in (2.65) are 0.                                                      Northwest to Kampala (Uganda) and then 374  km  Southwest.”.
                                                                                                                                            This is sufficient information                              
independence.                                                         Definition 2.12 (Linear (In)dependence). Let us  consider  a                                                                      
                                                                      vector space                                                          Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
Definition 2.11  (Linear  Combination).  Consider  a  vector                                                                                Feedback: https://mml-book.com.                             
space V and a                                                         V with k ∈ N and x1, . . .  ,  xk  ∈  V  .  If  there  is  a                                                                      
                                                                      non-trivial linear combination, such that 0 =                         2.5 Linear Independence 41                                  
finite number of vectors x1, . . . , xk ∈ V . Then, every  v                                                                                                                                            
∈ V of the form                                                       Pk                                                                    to describe the location of Kigali  because  the  geographic
                                                                                                                                            coordinate system may be considered a two-dimensional vector
v = λ1x1 + · · · + λkxk =                                             i=1 λixi with at least one λi ̸= 0, the vectors                       space (ignoring altitude                                    
                                                                                                                                                                                                        
X                                                                     linearly dependent x1, . . . , xk are linearly dependent. If          and the Earth’s curved surface). The person may add, “It  is
                                                                      only the trivial solution exists, i.e.,                               about 751 km                                                
k                                                                                                                                                                                                       
                                                                      linearly λ1 = . . . = λk = 0 the vectors x1, . . . , xk  are          West of here.” Although this last statement is true,  it  is
i=1                                                                   linearly independent.                                                 not necessary to                                            
                                                                                                                                                                                                        
λixi ∈ V (2.65)                                                       independent                                                           find Kigali given the previous information (see  Figure  2.2
                                                                                                                                            for  an  illustration).  In  this  example,  the   “506   km
linear combination with λ1, . . . ,  λk  ∈  R  is  a  linear          Linear independence is one of the most important concepts in          Northwest” vector (blue) and the                            
combination of the vectors x1, . . . , xk.                            linear                                                                                                                            
                                                                                                                                            “374 km Southwest” vector (purple) are linearly independent.
The 0-vector can always be written as the linear combination          algebra. Intuitively, a set of linearly independent  vectors          This means                                                  
of k vectors x1, . . . , xk because 0 =                               consists of vectors                                                                                                               
                                                                                                                                            the Southwest vector cannot be described  in  terms  of  the
Pk                                                                    that have no redundancy, i.e., if we  remove  any  of  those          Northwest vector, and vice versa. However, the third “751 km
                                                                      vectors from                                                          West” vector (black) is a                                   
i=1 0xi                                                                                                                                                                                                 
                                                                      the  set,  we  will  lose  something.  Throughout  the  next          linear combination of the other two vectors,  and  it  makes
is always true. In the following,                                     sections, we will                                                     the set of vectors linearly dependent.  Equivalently,  given
                                                                                                                                            “751 km West” and “374 km                                   
we are interested in non-trivial linear  combinations  of  a          formalize           this           intuition           more.                                                                      

%%%

Southwest” can  be  linearly  combined  to  obtain  “506  km          The vectors {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k},  k                                                                     
Northwest”.                                                           ⩾ 2, are linearly                                                                                                                 
                                                                                                                                            1 3 0                                                       
Figure 2.2                                                            dependent if and only if (at least) one of them is a  linear                                                                      
                                                                      combination                                                           0 0 2                                                      
Geographic example                                                                                                                                                                                      
                                                                      of the others. In particular, if one vector is a multiple of          (2.66)                                                      
(with crude                                                           another vector,                                                                                                                   
                                                                                                                                            tells us that the first and third columns are pivot columns.
approximations to                                                     i.e., xi = λxj , λ ∈ R then the set {x1, . . . , xk : xi  ̸=          The second column is a non-pivot column because it is  three
                                                                      0, i = 1, . . . , k}                                                  times the first                                             
cardinal directions)                                                                                                                                                                                    
                                                                      is linearly dependent.                                                column.                                                     
of linearly                                                                                                                                                                                             
                                                                      A practical way of checking whether vectors x1, . . . , xk ∈          All column vectors are linearly independent if and  only  if
dependent vectors                                                     V are linearly                                                        all columns                                                 
                                                                                                                                                                                                        
in a                                                                  independent  is  to  use  Gaussian  elimination:  Write  all          are pivot columns.  If  there  is  at  least  one  non-pivot
                                                                      vectors as columns                                                    column, the columns                                         
two-dimensional                                                                                                                                                                                         
                                                                      of a matrix A and perform  Gaussian  elimination  until  the          (and, therefore, the  corresponding  vectors)  are  linearly
space (plane).                                                        matrix is in                                                          dependent.                                                  
                                                                                                                                                                                                        
506 km Northwest  751  km  West  374  km  Southwest  374  km          row  echelon  form  (the   reduced   row-echelon   form   is          ♢                                                           
SouthwestKampala Nairobi Kigali                                       unnecessary here):                                                                                                                
                                                                                                                                            Example 2.14                                                
Remark. The following properties  are  useful  to  find  out          ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                      
whether vectors                                                       by Cambridge University Press (2020).                                 Consider R4 with                                            
                                                                                                                                                                                                        
are linearly independent:                                             42 Linear Algebra                                                     x1 =                                                        
                                                                                                                                                                                                        
k  vectors  are  either  linearly  dependent   or   linearly          – The pivot columns indicate the vectors, which are linearly                                                                     
independent. There                                                    independent of the vectors on the left. Note that  there  is                                                                      
                                                                      an ordering of vectors when the matrix is built.                                                                                 
is no third option.                                                                                                                                                                                     
                                                                      –  The  non-pivot  columns  can  be  expressed   as   linear                                                                     
If at least one of the vectors x1, . . . , xk is 0 then they          combinations of                                                                                                                   
are linearly dependent. The same holds if  two  vectors  are                                                                                                                                           
identical.                                                            the  pivot  columns  on  their  left.  For   instance,   the                                                                      
                                                                      row-echelon                                             form          1                                                           

%%%

2                                                                     , x3 =                                                                1                                                           
                                                                                                                                                                                                        
−3                                                                                                                                         2                                                           
                                                                                                                                                                                                        
4                                                                                                                                          −3                                                          
                                                                                                                                                                                                        
                                                                                                                                          4                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
                                                                     −2                                                                                                                               
                                                                                                                                                                                                        
, x2 =                                                                1                                                                                                                                
                                                                                                                                                                                                        
                                                                     1                                                                     + λ2                                                        
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                     . (2.67)                                                              1                                                           
                                                                                                                                                                                                        
0                                                                     To check whether they are linearly dependent, we follow  the          1                                                           
                                                                      general approach and solve                                                                                                        
2                                                                                                                                           0                                                           
                                                                      λ1x1 + λ2x2 + λ3x3 = λ1                                                                                                           
                                                                                                                                           2                                                           
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
+ λ3                                                                                                                                                                                                  
                                                                                                                                                                                                        
                                                                     1 1 −1                                                                . (2.69)                                                    
                                                                                                                                                                                                        
                                                                     2 1 −2                                                                Here,  every  column  of  the  matrix  is  a  pivot  column.
                                                                                                                                            Therefore, there is no                                      
                                                                     −3 0 1                                                                                                                            
                                                                                                                                            non-trivial solution, and we require λ1 = 0, λ2 = 0, λ3 =  0
                                                                     4 2 1                                                                 to solve the                                                
                                                                                                                                                                                                        
−1                                                                                                                                         equation system. Hence, the vectors x1, x2, x3 are  linearly
                                                                                                                                            independent.                                                
−2                                                                                                                                                                                                     
                                                                                                                                            Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
1                                                                                                                                          Feedback: https://mml-book.com.                             
                                                                                                                                                                                                        
1                                                                      ⇝ · · · ⇝                                                           2.5 Linear Independence 43                                  
                                                                                                                                                                                                        
                                                                                                                                          Remark.  Consider  a  vector  space  V   with   k   linearly
                                                                                                                                            independent vectors                                         
                                                                                                                                                                                                      
                                                                                                                                            b1, . . . , bk and m linear combinations                    
                                                                                                                                                                                                      
                                                                                                                                            x1 =                                                        
 = 0 (2.68)                                                                                                                                                                                           
                                                                                                                                            X                                                           
for λ1, . . . , λ3. We write the vectors xi                           1 1 −1                                                                                                                            
                                                                                                                                            k                                                           
, i = 1, 2, 3, as the columns of a                                    0 1 0                                                                                                                             
                                                                                                                                            i=1                                                         
matrix and apply elementary row operations until we identify          0 0 1                                                                                                                             
the pivot                                                                                                                                   λi1bi                                                       
                                                                      0 0 0                                                                                                                             
columns:                                                                                                                                    ,                                                           
                                                                                                                                                                                                       
                                                                                                                                           .                                                           
                                                                                                                                                                                                       
                                                                                                                                           .                                                           

%%%

.                                                                                                                                          Remark. In a vector space V , m  linear  combinations  of  k
                                                                                                                                            vectors x1, . . . , xk                                      
xm =                                                                   , j = 1, . . . , m , (2.71)                                                                                                     
                                                                                                                                            are linearly dependent if m > k. ♢                          
X                                                                     in a more compact form.                                                                                                           
                                                                                                                                            Example 2.15                                                
k                                                                     We want to test  whether  x1,  .  .  .  ,  xm  are  linearly                                                                      
                                                                      independent. For this                                                 Consider a set of linearly independent vectors b1,  b2,  b3,
i=1                                                                                                                                         b4 ∈ Rn and                                                 
                                                                      purpose, we follow the general approach of testing when Pm                                                                        
λimbi                                                                                                                                       x1 = b1 − 2b2 + b3 − b4                                     
                                                                      j=1 ψjxj = 0.                                                                                                                     
.                                                                                                                                           x2 = −4b1 − 2b2 + 4b4                                       
                                                                      With (2.71), we obtain                                                                                                            
(2.70)                                                                                                                                      x3 = 2b1 + 3b2 − b3 − 3b4                                   
                                                                      Xm                                                                                                                                
Defining B = [b1, . . . , bk] as the  matrix  whose  columns                                                                                x4 = 17b1 − 10b2 + 11b3 + b4                                
are the linearly                                                      j=1                                                                                                                               
                                                                                                                                            . (2.73)                                                    
independent vectors b1, . . . , bk, we can write                      ψjxj =                                                                                                                            
                                                                                                                                            Are the vectors x1, . . . , x4 ∈ Rn                         
xj = Bλj , λj =                                                       Xm                                                                                                                                
                                                                                                                                            linearly independent? To answer this                        
                                                                     j=1                                                                                                                               
                                                                                                                                            question, we investigate whether the column vectors         
                                                                     ψjBλj = B                                                                                                                         
                                                                                                                                                                                                       
                                                                     Xm                                                                                                                                
                                                                                                                                                                                                     
λ1j                                                                   j=1                                                                                                                               
                                                                                                                                                                                                     
.                                                                     ψjλj . (2.72)                                                                                                                     
                                                                                                                                                                                                       
.                                                                     This means that {x1, . . . , xm} are linearly independent if                                                                      
                                                                      and only if the                                                                                                                  
.                                                                                                                                                                                                       
                                                                      column vectors {λ1, . . . , λm} are linearly independent.                                                                        
λkj                                                                                                                                                                                                     
                                                                      ♢                                                                                                                                
                                                                                                                                                                                                       

%%%

1                                                                                                                                          −10                                                         
                                                                                                                                                                                                        
−2                                                                    ,                                                                     11                                                          
                                                                                                                                                                                                        
1                                                                                                                                          1                                                           
                                                                                                                                                                                                        
−1                                                                                                                                                                                                    
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     2                                                                                                                                
                                                                                                                                                                                                        
                                                                     3                                                                                                                                
                                                                                                                                                                                                        
,                                                                     −1                                                                                                                             
                                                                                                                                                                                                        
                                                                     −3                                                                                                                             
                                                                                                                                                                                                        
                                                                                                                                          (2.74)                                                      
                                                                                                                                                                                                        
                                                                                                                                          ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                                                                                            by Cambridge University Press (2020).                       
                                                                                                                                                                                                      
                                                                                                                                            44 Linear Algebra                                           
−4                                                                                                                                                                                                     
                                                                                                                                            are linearly independent. The reduced  row-echelon  form  of
−2                                                                    ,                                                                     the corresponding linear equation  system  with  coefficient
                                                                                                                                            matrix                                                      
0                                                                                                                                                                                                      
                                                                                                                                            A =                                                         
4                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                     17                                                                                                                                
                                                                                                                                                                                                       

%%%

1 −4 2 17                                                                                                                                  combinations of vectors in A is                             
                                                                                                                                                                                                        
−2 −2 3 −10                                                                                                                                span called the span of A. If A spans the vector space  V  ,
                                                                                                                                            we write V = span[A]                                        
1 0 −1 11                                                             . (2.76)                                                                                                                          
                                                                                                                                            or V = span[x1, . . . , xk].                                
−1 4 −3 1                                                             We see that the  corresponding  linear  equation  system  is                                                                      
                                                                      non-trivially solvable: The  last  column  is  not  a  pivot          Generating  sets  are  sets  of  vectors  that  span  vector
                                                                     column, and x4 = −7x1−15x2−18x3.                                      (sub)spaces, i.e.,                                          
                                                                                                                                                                                                        
                                                                     Therefore, x1, . . . , x4 are linearly dependent as  x4  can          every vector can be represented as a linear  combination  of
                                                                      be expressed as a                                                     the vectors                                                 
                                                                                                                                                                                                       
                                                                      linear combination of x1, . . . , x3.                                 in the generating set. Now, we will  be  more  specific  and
                                                                                                                                           characterize the                                            
                                                                      2.6 Basis and Rank                                                                                                                
(2.75)                                                                                                                                      smallest generating set that spans a vector (sub)space.     
                                                                      In a vector space V , we are particularly interested in sets                                                                      
is given as                                                           of vectors A that                                                     Definition 2.14 (Basis). Consider a vector space V = (V,  +,
                                                                                                                                            ·) and A ⊆                                                  
                                                                     possess the property that any vector v ∈ V can  be  obtained                                                                      
                                                                      by a linear                                                           minimal V. A generating set A of  V  is  called  minimal  if
                                                                                                                                           there exists no smaller set                                 
                                                                      combination of vectors  in  A.  These  vectors  are  special                                                                      
                                                                     vectors, and in the                                                   A˜ ⊊ A ⊆  V  that  spans  V  .  Every  linearly  independent
                                                                                                                                            generating set of V                                         
                                                                     following, we will characterize them.                                                                                             
                                                                                                                                            basis is minimal and is called a basis of V .               
1 0 0 −7                                                              2.6.1 Generating Set and Basis                                                                                                    
                                                                                                                                            Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
0 1 0 −15                                                             Definition 2.13 (Generating Set and Span). Consider a vector          Feedback: https://mml-book.com.                             
                                                                      space V =                                                                                                                         
0 0 1 −18                                                                                                                                   2.6 Basis and Rank 45                                       
                                                                      (V, +, ·) and set of vectors A = {x1, . . . , xk}  ⊆  V.  If                                                                      
0 0 0 0                                                               every vector v ∈                                                      Let V = (V, +, ·) be a vector space and B ⊆ V, B ̸= ∅. Then,
                                                                                                                                            the                                                         
                                                                     V can be expressed as a linear combination of x1, .  .  .  ,                                                                      
                                                                      xk, A is called a                                                     following statements are equivalent: A basis is a minimal   
                                                                                                                                                                                                       
                                                                      generating set generating set of V . The set of  all  linear          generating              set              and               a

%%%

maximal linearly                                                      , ψi ∈ R, bi ∈ B it follows that λi = ψi                              0                                                           
                                                                                                                                                                                                        
independent set of                                                    , i = 1, . . . , k.                                                                                                              
                                                                                                                                                                                                        
vectors.                                                              Example 2.16                                                           ,                                                         
                                                                                                                                                                                                        
B is a basis of V .                                                   In R3                                                                                                                            
                                                                                                                                                                                                        
B is a minimal generating set.                                        , the canonical/standard basis is canonical basis                                                                                
                                                                                                                                                                                                        
B is a maximal linearly independent set of vectors  in  V  ,          B =                                                                   0                                                           
i.e., adding any                                                                                                                                                                                        
                                                                                                                                           0                                                           
other vector to this set will make it linearly dependent.                                                                                                                                               
                                                                                                                                           1                                                           
Every vector x ∈ V is a linear combination of  vectors  from                                                                                                                                            
B, and every                                                                                                                                                                                          
                                                                                                                                                                                                        
linear combination is unique, i.e., with                                                                                                                                                              
                                                                                                                                                                                                        
x =                                                                                                                                                                                                   
                                                                                                                                                                                                        
X                                                                     1                                                                                                                                
                                                                                                                                                                                                        
k                                                                     0                                                                                                                                
                                                                                                                                                                                                        
i=1                                                                   0                                                                     . (2.78)                                                    
                                                                                                                                                                                                        
λibi =                                                                                                                                     Different bases in R3 are                                   
                                                                                                                                                                                                        
X                                                                      ,                                                                   B1 =                                                        
                                                                                                                                                                                                        
k                                                                                                                                                                                                     
                                                                                                                                                                                                        
i=1                                                                                                                                                                                                   
                                                                                                                                                                                                        
ψibi (2.77)                                                           0                                                                                                                                
                                                                                                                                                                                                        
and λi                                                                1                                                                                                                                
                                                                                                                                                                                                        

%%%

                                                                                                                                           ,                                                         
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                     , B2 =                                                                −2.2                                                        
                                                                                                                                                                                                        
                                                                                                                                          −1.3                                                        
                                                                                                                                                                                                        
 ,                                                                                                                                        3.5                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                     0.5                                                                                                                              
                                                                                                                                                                                                        
0                                                                     0.8                                                                                                                              
                                                                                                                                                                                                        
                                                                     0.4                                                                   . (2.79)                                                    
                                                                                                                                                                                                        
 ,                                                                                                                                        The set                                                     
                                                                                                                                                                                                        
                                                                      ,                                                                   A =                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                   
                                                                                                                                                                                                        
1                                                                     1.8                                                                                                                            
                                                                                                                                                                                                        
1                                                                     0.3                                                                                                                              
                                                                                                                                                                                                        
                                                                     0.3                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        

%%%

                                                                                                                                          :                                                           
                                                                                                                                                                                                        
1                                                                                                                                          For instance, the vector [1, 0, 0, 0]⊤ cannot be obtained by
                                                                                                                                            a linear combination of elements in A.                      
2                                                                     ,                                                                                                                                 
                                                                                                                                            Remark. Every vector  space  V  possesses  a  basis  B.  The
3                                                                                                                                          preceding examples show that there can be many  bases  of  a
                                                                                                                                            vector space V , i.e., there is                             
4                                                                                                                                                                                                      
                                                                                                                                            no unique basis. However, all bases possess the same  number
                                                                                                                                          of elements,                                                
                                                                                                                                                                                                        
                                                                                                                                          the basis vectors. ♢ basis vector                           
                                                                                                                                                                                                        
                                                                     1                                                                     We only consider finite-dimensional vector  spaces  V  .  In
                                                                                                                                            this case, the                                              
                                                                     1                                                                                                                                 
                                                                                                                                            dimension of V is the number of basis vectors of V , and  we
,                                                                     0                                                                     write dim(V ). dimension                                    
                                                                                                                                                                                                        
                                                                     −4                                                                    If U ⊆ V is a subspace of V , then  dim(U)  ⩽  dim(V  )  and
                                                                                                                                            dim(U) =                                                    
                                                                                                                                                                                                      
                                                                                                                                            ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                                                                                          by Cambridge University Press (2020).                       
                                                                                                                                                                                                        
                                                                                                                                          46 Linear Algebra                                           
                                                                                                                                                                                                        
2                                                                                                                                          dim(V ) if and only if U = V . Intuitively, the dimension of
                                                                                                                                            a vector space                                              
−1                                                                                                                                                                                                     
                                                                                                                                            can be thought of as the number of independent directions in
0                                                                                                                                        this vector                                                 
                                                                                                                                                                                                        
2                                                                                                                                        The dimension of a space.                                   
                                                                                                                                                                                                        
                                                                     (2.80)                                                                vector space                                                
                                                                                                                                                                                                        
                                                                     is linearly independent, but not a generating  set  (and  no          corresponds to the                                          
                                                                      basis)                         of                         R4                                                                      

%%%

number of its basis                                                   For a vector subspace U ⊆ R5                                          , x2 =                                                      
                                                                                                                                                                                                        
vectors.                                                              , spanned by the vectors                                                                                                         
                                                                                                                                                                                                        
Remark. The dimension of a vector space is  not  necessarily          x1 =                                                                                                                             
the number                                                                                                                                                                                              
                                                                                                                                                                                                      
of elements in a vector. For instance, the vector space V  =                                                                                                                                            
span[                                                                                                                                                                                                
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          2                                                           
                                                                                                                                                                                                        
] is                                                                                                                                       −1                                                          
                                                                                                                                                                                                        
one-dimensional, although the  basis  vector  possesses  two          1                                                                     1                                                           
elements. ♢                                                                                                                                                                                             
                                                                      2                                                                     2                                                           
Remark. A basis of a subspace U = span[x1, . . . , xm] ⊆ Rn                                                                                                                                             
                                                                      −1                                                                    −2                                                          
can be found                                                                                                                                                                                            
                                                                      −1                                                                                                                               
by executing the following steps:                                                                                                                                                                       
                                                                      −1                                                                                                                               
1. Write the spanning vectors as columns of a matrix A                                                                                                                                                  
                                                                                                                                                                                                      
2. Determine the row-echelon form of A.                                                                                                                                                                 
                                                                                                                                                                                                      
3. The spanning vectors associated with  the  pivot  columns                                                                                                                                            
are a basis of                                                                                                                                                                                        
                                                                                                                                                                                                        
U.                                                                                                                                                                                                    
                                                                                                                                                                                                        
♢                                                                                                                                          , x3 =                                                      
                                                                                                                                                                                                        
Example 2.17 (Determining a Basis)                                                                                                                                                                    
                                                                                                                                                                                                        

%%%

                                                                                                                                          Therefore, we need to solve                                 
                                                                                                                                                                                                        
                                                                                                                                          X                                                           
                                                                                                                                                                                                        
                                                                                                                                          4                                                           
                                                                                                                                                                                                        
                                                                     −1                                                                    i=1                                                         
                                                                                                                                                                                                        
                                                                     8                                                                     λixi = 0 , (2.82)                                           
                                                                                                                                                                                                        
3                                                                     −5                                                                    which leads to a homogeneous system of equations with matrix
                                                                                                                                                                                                        
−4                                                                    −6                                                                    x1, x2, x3, x4                                              
                                                                                                                                                                                                        
3                                                                     1                                                                     =                                                           
                                                                                                                                                                                                        
5                                                                                                                                                                                                     
                                                                                                                                                                                                        
−3                                                                                                                                                                                                    
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     ∈ R                                                                   1 2 3 −1                                                    
                                                                                                                                                                                                        
                                                                     5                                                                     2 −1 −4 8                                                   
                                                                                                                                                                                                        
, x4 =                                                                , (2.81)                                                              −1 1 3 −5                                                   
                                                                                                                                                                                                        
                                                                     we are interested in finding out which vectors x1, . .  .  ,          −1 2 5 −6                                                   
                                                                      x4 are a basis for U.                                                                                                             
                                                                                                                                           −1 −2 −3 1                                                  
                                                                      For this, we need to check whether  x1,  .  .  .  ,  x4  are                                                                      
                                                                     linearly independent.                                                                                                            
                                                                                                                                                                                                        

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
. (2.83)                                                              ⇝ · · · ⇝                                                                                                                         
                                                                                                                                            2.6 Basis and Rank 47                                       
With the basic transformation rules for  systems  of  linear                                                                                                                                           
equations, we                                                                                                                               Since the pivot columns indicate which  set  of  vectors  is
                                                                                                                                           linearly independent, we see from the row-echelon form  that
obtain the row-echelon form                                                                                                                 x1, x2, x4 are linearly independent (because the  system  of
                                                                                                                                           linear equations λ1x1 + λ2x2 + λ4x4 = 0                     
                                                                                                                                                                                                       
                                                                                                                                           can only be solved with λ1 = λ2 = λ4 = 0).  Therefore,  {x1,
                                                                                                                                           x2, x4} is a                                                
                                                                                                                                                                                                       
                                                                                                                                           basis of U.                                                 
                                                                                                                                                                                                       
                                                                                                                                           2.6.2 Rank                                                  
                                                                      1 2 3 −1                                                                                                                          
                                                                                                                                           The number of linearly independent columns of a matrix  A  ∈
                                                                      0 1 2 −2                                                              Rm×n                                                        
                                                                                                                                                                                                       
                                                                      0 0 0 1                                                               equals the number of linearly independent rows and is called
1 2 3 −1                                                                                                                                    the rank rank                                               
                                                                      0 0 0 0                                                                                                                           
2 −1 −4 8                                                                                                                                   of A and is denoted by rk(A).                               
                                                                      0 0 0 0                                                                                                                           
−1 1 3 −5                                                                                                                                   Remark. The rank of a matrix has some important properties: 
                                                                                                                                                                                                       
−1 2 5 −6                                                                                                                                   rk(A) = rk(A                                                
                                                                                                                                                                                                       
−1 −2 −3 1                                                                                                                                  ⊤                                                           
                                                                                                                                                                                                       
                                                                                                                                           ),  i.e.,   the   column   rank   equals   the   row   rank.

%%%

The columns of A ∈ Rm×n                                               the subspace of solutions for Ax = 0 possesses dimension n −          A has two linearly independent rows/columns so that rk(A)  =
                                                                      rk(A). Later, we will call this subspace the kernel  or  the          2.                                                          
span a subspace U ⊆ Rm with dim(U) =                                  null kernel                                                                                                                       
                                                                                                                                            ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
rk(A). Later we will call this subspace the image or  range.          null space space.                                                     by Cambridge University Press (2020).                       
A basis of                                                                                                                                                                                              
                                                                      A matrix A ∈ Rm×n has full  rank  if  its  rank  equals  the          48 Linear Algebra                                           
U can be found by applying  Gaussian  elimination  to  A  to          largest possible full rank                                                                                                        
identify the                                                                                                                                A =                                                         
                                                                      rank for a matrix of the same dimensions.  This  means  that                                                                      
pivot columns.                                                        the rank of                                                                                                                      
                                                                                                                                                                                                        
The rows of A ∈ Rm×n                                                  a full-rank matrix is the lesser of the number of  rows  and                                                                     
                                                                      columns, i.e.,                                                                                                                    
span a subspace W ⊆ Rn with dim(W) =                                                                                                        1 2 1                                                       
                                                                      rk(A) = min(m, n). A matrix is said to be rank deficient  if                                                                      
rk(A). A basis of  W  can  be  found  by  applying  Gaussian          it does not rank deficient                                            −2 −3 1                                                     
elimination to                                                                                                                                                                                          
                                                                      have full rank.                                                       3 5 0                                                       
A                                                                                                                                                                                                       
                                                                      ♢                                                                                                                                
⊤                                                                                                                                                                                                       
                                                                      Example 2.18 (Rank)                                                    .                                                         
.                                                                                                                                                                                                       
                                                                      A =                                                                   We use Gaussian elimination to determine the rank:          
For all A ∈ Rn×n                                                                                                                                                                                        
                                                                                                                                                                                                      
it holds that A is regular (invertible) if and only if                                                                                                                                                  
                                                                                                                                                                                                      
rk(A) = n.                                                                                                                                                                                              
                                                                      1 0 1                                                                 1 2 1                                                       
For all A ∈ Rm×n and all b ∈ Rm it  holds  that  the  linear                                                                                                                                            
equation                                                              0 1 1                                                                 −2 −3 1                                                     
                                                                                                                                                                                                        
system Ax = b can be solved if and only if rk(A) =  rk(A|b),          0 0 0                                                                 3 5 0                                                       
where                                                                                                                                                                                                   
                                                                                                                                                                                                      
A|b denotes the augmented system.                                                                                                                                                                       
                                                                      .                                                                     ⇝ · · · ⇝                                                 
For                 A                 ∈                 Rm×n                                                                                                                                            

%%%

                                                                     Φ(x + y) = Φ(x) + Φ(y) (2.85)                                         briefly introduce special mappings.                         
                                                                                                                                                                                                        
                                                                     Φ(λx) = λΦ(x) (2.86)                                                  Definition 2.16 (Injective, Surjective, Bijective). Consider
                                                                                                                                            a mapping Φ :                                               
1 2 1                                                                 for all x, y ∈ V and λ ∈ R. We can  summarize  this  in  the                                                                      
                                                                      following                                                             V → W, where V, W can be arbitrary sets. Then Φ is called   
0 1 3                                                                                                                                                                                                   
                                                                      definition:                                                           injective                                                   
0 0 0                                                                                                                                                                                                   
                                                                      Definition 2.15 (Linear Mapping). For vector spaces V, W,  a          Injective if ∀x, y ∈ V : Φ(x) = Φ(y) =⇒ x = y.              
                                                                     mapping                                                                                                                           
                                                                                                                                            surjective                                                  
 . (2.84)                                                            linear mapping Φ : V → W is  called  a  linear  mapping  (or                                                                      
                                                                      vector space homomorphism/                                            Surjective if Φ(V) = W. bijective                           
Here, we see that the number of  linearly  independent  rows                                                                                                                                            
and columns                                                           vector space                                                          Bijective if it is injective and surjective.                
                                                                                                                                                                                                        
is 2, such that rk(A) = 2.                                            homomorphism                                                          Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
2.7 Linear Mappings                                                   linear transformation) if                                                                                                         
                                                                                                                                            2.7 Linear Mappings 49                                      
In the following, we will study mappings  on  vector  spaces          linear                                                                                                                            
that preserve                                                                                                                               If Φ is surjective, then every element in W can be “reached”
                                                                      transformation                                                        from V                                                      
their structure, which will allow us to define  the  concept                                                                                                                                            
of a coordinate.                                                      ∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y). (2.87)              using Φ. A bijective Φ can be “undone”, i.e., there exists a
                                                                                                                                            mapping Ψ :                                                 
In the beginning of the chapter, we said  that  vectors  are          It turns out  that  we  can  represent  linear  mappings  as                                                                      
objects that can be                                                   matrices (Section 2.7.1). Recall that we can also collect  a          W → V so that Ψ ◦ Φ(x) = x. This mapping Ψ  is  then  called
                                                                      set of vectors as columns of a                                        the inverse                                                 
added together and multiplied by a scalar, and the resulting                                                                                                                                            
object is still                                                       matrix. When working with matrices, we have to keep in  mind          of Φ and normally denoted by Φ                              
                                                                      what the                                                                                                                          
a vector. We wish to preserve this  property  when  applying                                                                                −1                                                          
the mapping:                                                          matrix represents: a  linear  mapping  or  a  collection  of                                                                      
                                                                      vectors. We will see                                                  .                                                           
Consider two real vector spaces V, W. A mapping Φ :  V  →  W                                                                                                                                            
preserves                                                             more about linear mappings in Chapter 4. Before we continue,          With these definitions, we introduce the  following  special
                                                                      we will                                                               cases of linear                                             
the     structure     of     the     vector     space     if                                                                                                                                            

%%%

mappings between vector spaces V and W:                               = Φ x1                                                              into the set  of  complex  numbers  with  the  corresponding
                                                                                                                                            addition. Note that we only showed linearity,  but  not  the
isomorphism                                                           x2                                                                    bijection.                                                  
                                                                                                                                                                                                        
Isomorphism: Φ : V → W linear and bijective endomorphism               + Φ y1                                                           Theorem   2.17   (Theorem    3.59    in    Axler    (2015)).
                                                                                                                                            Finite-dimensional vector                                   
Endomorphism: Φ : V → V linear automorphism                           y2                                                                                                                                
                                                                                                                                            spaces V and W are isomorphic if  and  only  if  dim(V  )  =
Automorphism: Φ : V → V linear and bijective                                                                                              dim(W).                                                     
                                                                                                                                                                                                        
We define idV : V → V , x 7→ x as the  identity  mapping  or          Φ                                                                     Theorem 2.17 states that there exists  a  linear,  bijective
identity identity mapping                                                                                                                   mapping between two vector spaces  of  the  same  dimension.
                                                                                                                                           Intuitively, this means                                     
identity                                                                                                                                                                                                
                                                                      λ                                                                     that vector spaces of the same dimension  are  kind  of  the
automorphism                                                                                                                                same thing, as                                              
                                                                                                                                                                                                       
automorphism in V .                                                                                                                         they can be transformed into each  other  without  incurring
                                                                      x1                                                                    any loss.                                                   
Example 2.19 (Homomorphism)                                                                                                                                                                             
                                                                      x2                                                                    Theorem 2.17 also gives us the justification to treat Rm×n  
The mapping Φ : R2 → C, Φ(x) = x1 + ix2, is a homomorphism:                                                                                                                                             
                                                                       = λx1 + λix2 = λ(x1 + ix2) = λΦ                                    (the vector                                                 
Φ                                                                                                                                                                                                       
                                                                      x1                                                                  space of m ×  n-matrices)  and  Rmn  (the  vector  space  of
x1                                                                                                                                        vectors of length                                           
                                                                      x2                                                                                                                                
x2                                                                                                                                          mn) the same, as their dimensions are mn, and there exists a
                                                                       .                                                                  linear, bijective  mapping  that  transforms  one  into  the
                                                                                                                                           other.                                                      
                                                                      (2.88)                                                                                                                            
+                                                                                                                                           Remark. Consider vector spaces V, W, X. Then:               
                                                                      This also justifies why complex numbers can  be  represented                                                                      
                                                                     as tuples in                                                          For linear mappings Φ : V → W and Ψ : W → X, the mapping    
                                                                                                                                                                                                        
y1                                                                    R2                                                                    Ψ ◦ Φ : V → X is also linear.                               
                                                                                                                                                                                                        
y2                                                                    : There is a bijective  linear  mapping  that  converts  the          If Φ : V → W is an isomorphism, then Φ                      
                                                                      elementwise addition of tuples in R2                                                                                              
 = (x1 + y1) +  i(x2  +  y2)  =  x1  +  ix2  +  y1  +  iy2                                                                                −1                                                          

%%%

: W → V is an isomorphism, too.                                       R, are                                                                x = α1b1 + . . . + αnbn (2.90)                              
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          linear, too.                                                          coordinate of x with respect to B. Then α1, . . . ,  αn  are
by Cambridge University Press (2020).                                                                                                       the coordinates of x with                                   
                                                                      ♢                                                                                                                                 
50 Linear Algebra                                                                                                                           respect to B, and the vector                                
                                                                      2.7.1 Matrix Representation of Linear Mappings                                                                                    
Figure 2.1 Two                                                                                                                              α =                                                         
                                                                      Any n-dimensional vector space is isomorphic to Rn                                                                                
different coordinate                                                                                                                                                                                   
                                                                      (Theorem 2.17). We                                                                                                                
systems defined by                                                                                                                                                                                     
                                                                      consider a basis {b1, . . . , bn} of an n-dimensional vector                                                                      
two sets of basis                                                     space V . In the                                                                                                                 
                                                                                                                                                                                                        
vectors. A vector x                                                   following, the order of the basis vectors will be important.          α1                                                          
                                                                      Therefore, we                                                                                                                     
has different                                                                                                                               .                                                           
                                                                      write                                                                                                                             
coordinate                                                                                                                                  .                                                           
                                                                      B = (b1, . . . , bn) (2.89)                                                                                                       
representations                                                                                                                             .                                                           
                                                                      ordered basis and call this n-tuple an ordered basis of V .                                                                       
depending on which                                                                                                                          αn                                                          
                                                                      Remark (Notation). We are at the point where notation gets a                                                                      
coordinate system is                                                  bit tricky.                                                                                                                      
                                                                                                                                                                                                        
chosen.                                                               Therefore, we summarize some parts here. B = (b1, .  .  .  ,                                                                     
                                                                      bn) is an ordered                                                                                                                 
x x                                                                                                                                          ∈ R                                                       
                                                                      basis, B = {b1, . . . , bn} is an (unordered) basis, and B =                                                                      
e1                                                                    [b1, . . . , bn] is a                                                 n                                                           
                                                                                                                                                                                                        
e2                                                                    matrix whose columns are the vectors b1, . . . , bn. ♢                (2.91)                                                      
                                                                                                                                                                                                        
b1                                                                    Definition 2.18 (Coordinates). Consider a vector space V and          coordinate  vector  is  the   coordinate   vector/coordinate
                                                                      an ordered                                                            representation of x with respect to the                     
b2                                                                                                                                                                                                      
                                                                      basis B = (b1, . . . , bn) of V . For any x ∈ V we obtain  a          coordinate                                                  
If Φ : V → W, Ψ : V → W are linear, then Φ + Ψ and λΦ,  λ  ∈          unique       representation       (linear       combination)                                                                      

%%%

representation                                                        Let us have a look  at  a  geometric  vector  x  ∈  R2  with          we will obtain the coordinates 1                            
                                                                      coordinates [2, 3]⊤ Figure 2.2                                                                                                    
ordered basis B.                                                                                                                            2                                                           
                                                                      Different coordinate                                                                                                              
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                                [−1, 5]⊤ to represent the same vector with                  
Feedback: https://mml-book.com.                                       representations of a                                                                                                              
                                                                                                                                            respect to (b1, b2) (see Figure 2.2).                       
2.7 Linear Mappings 51                                                vector x, depending                                                                                                               
                                                                                                                                            Remark. For an n-dimensional vector space V and  an  ordered
A basis effectively defines  a  coordinate  system.  We  are          on the choice of                                                      basis B                                                     
familiar with the                                                                                                                                                                                       
                                                                      basis.                                                                of V , the mapping Φ : Rn → V , Φ(ei) = bi                  
Cartesian coordinate system  in  two  dimensions,  which  is                                                                                                                                            
spanned by the                                                        e1                                                                    , i = 1, . . . , n, is linear                               
                                                                                                                                                                                                        
canonical basis vectors e1, e2. In this coordinate system, a          e2 b2                                                                 (and because of Theorem 2.17 an isomorphism), where (e1, . .
vector x ∈ R2                                                                                                                               . , en) is                                                  
                                                                      b1                                                                                                                                
has a representation that tells us how to  linearly  combine                                                                                the standard basis of Rn                                    
e1 and e2 to                                                          x = −                                                                                                                             
                                                                                                                                            .                                                           
obtain  x.  However,  any  basis  of  R2  defines  a   valid          1                                                                                                                                 
coordinate system,                                                                                                                          ♢                                                           
                                                                      2b1 +                                                                                                                             
and the same vector x  from  before  may  have  a  different                                                                                Now we are ready to  make  an  explicit  connection  between
coordinate representation in the (b1, b2) basis.  In  Figure          5                                                                     matrices and                                                
2.1, the coordinates of x with                                                                                                                                                                          
                                                                      2b2                                                                   linear mappings between finite-dimensional vector spaces.   
respect to the standard basis (e1, e2) is [2, 2]⊤.  However,                                                                                                                                            
with respect to                                                       x = 2e1 + 3e2                                                         Definition 2.19  (Transformation  Matrix).  Consider  vector
                                                                                                                                            spaces V, W                                                 
the basis (b1, b2) the  same  vector  x  is  represented  as          with respect to the standard basis (e1, e2) of R2                                                                                 
[1.09, 0.72]⊤, i.e.,                                                                                                                        with corresponding (ordered) bases B = (b1, . . . , bn)  and
                                                                      . This means, we can write                                            C = (c1, . . . , cm).                                       
x = 1.09b1 + 0.72b2. In  the  following  sections,  we  will                                                                                                                                            
discover how to                                                       x = 2e1 + 3e2.  However,  we  do  not  have  to  choose  the          Moreover, we consider a linear mapping Φ : V → W.  For  j  ∈
                                                                      standard basis to                                                     {1, . . . , n},                                             
obtain this representation.                                                                                                                                                                             
                                                                      represent this vector. If we use the basis vectors b1 =  [1,          Φ(bj ) = α1jc1 + · · · + αmjcm =                            
Example                                                 2.20          −1]⊤,           b2           =            [1,            1]⊤                                                                      

%%%

Xm                                                                    yˆ = AΦxˆ . (2.94)                                                                                                               
                                                                                                                                                                                                        
i=1                                                                   This means that the transformation matrix can be used to map                                                                     
                                                                      coordinates                                                                                                                       
αijci (2.92)                                                                                                                                1 2 0                                                       
                                                                      with respect to an ordered basis in V  to  coordinates  with                                                                      
is the unique representation of Φ(bj ) with  respect  to  C.          respect to an                                                         −1 1 3                                                      
Then, we call the                                                                                                                                                                                       
                                                                      ordered basis in W.                                                   3 7 1                                                       
m × n-matrix AΦ, whose elements are given by                                                                                                                                                            
                                                                      Example 2.21 (Transformation Matrix)                                  −1 2 4                                                      
AΦ(i, j) = αij , (2.93)                                                                                                                                                                                 
                                                                      Consider a homomorphism Φ : V → W and ordered bases B =                                                                          
the transformation matrix of Φ (with respect to the  ordered                                                                                                                                            
bases B of V transformation                                           (b1, . . . , b3) of V and C = (c1, . . . , c4) of W. With                                                                        
                                                                                                                                                                                                        
and C of W matrix ).                                                  Φ(b1) = c1 − c2 + 3c3 − c4                                                                                                       
                                                                                                                                                                                                        
The coordinates of Φ(bj ) with respect to the ordered  basis          Φ(b2) = 2c1 + c2 + 7c3 + 2c4                                                                                                     
C of W                                                                                                                                                                                                  
                                                                      Φ(b3) = 3c2 + c3 + 4c4                                                , (2.96)                                                    
are the j-th column  of  AΦ.  Consider  (finite-dimensional)                                                                                                                                            
vector spaces                                                         (2.95)                                                                where the αj , j = 1, 2, 3, are the  coordinate  vectors  of
                                                                                                                                            Φ(bj ) with respect                                         
V, W with ordered bases B, C and a linear mapping Φ : V →  W          the transformation matrix                                                                                                         
with                                                                                                                                        to C.                                                       
                                                                      P                                                                                                                                 
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                Example 2.22 (Linear Transformations of Vectors)            
by Cambridge University Press (2020).                                 AΦ with respect to B and C satisfies Φ(bk) =                                                                                      
                                                                                                                                            Figure 2.3 Three                                            
52 Linear Algebra                                                     4                                                                                                                                 
                                                                                                                                            examples of linear                                          
transformation matrix AΦ. If xˆ is the coordinate vector  of          i=1 αikci for k = 1, . . . , 3 and is given as                                                                                    
x ∈ V with                                                                                                                                  transformations of                                          
                                                                      AΦ = [α1, α2, α3] =                                                                                                               
respect to B and yˆ the coordinate vector of y =  Φ(x)  ∈  W                                                                                the vectors shown                                           
with respect                                                                                                                                                                                           
                                                                                                                                            as dots in (a);                                             
to C, then                                                                                                                                                                                             
                                                                                                                                            (b)              Rotation              by               45◦;

%%%

(c) Stretching of the                                                 sin( π                                                                Figure 2.3 gives three examples of linear transformations of
                                                                                                                                            a set of vectors. Figure 2.3(a) shows 400 vectors in R2     
horizontal                                                            4                                                                                                                                 
                                                                                                                                            , each of which is represented                              
coordinates by 2;                                                     ) cos( π                                                                                                                          
                                                                                                                                            by a dot at  the  corresponding  (x1,  x2)-coordinates.  The
(d) Combination of                                                    4                                                                     vectors are arranged in a square. When we use matrix  A1  in
                                                                                                                                            (2.97) to linearly transform                                
reflection, rotation                                                  )                                                                                                                                 
                                                                                                                                            each of these vectors,  we  obtain  the  rotated  square  in
and stretching.                                                                                                                            Figure 2.3(b). If we                                        
                                                                                                                                                                                                        
(a) Original data. (b) Rotation by 45◦.  (c)  Stretch  along          , A2 =                                                                apply the linear mapping represented by A2,  we  obtain  the
the                                                                                                                                         rectangle in                                                
                                                                                                                                                                                                       
horizontal axis.                                                                                                                            Figure 2.3(c) where each x1-coordinate is  stretched  by  2.
                                                                      2 0                                                                   Figure 2.3(d)                                               
(d) General linear                                                                                                                                                                                      
                                                                      0 1                                                                  shows the original square from Figure 2.3(a)  when  linearly
mapping.                                                                                                                                    transformed                                                 
                                                                      , A3 =                                                                                                                            
We consider three linear transformations of a set of vectors                                                                                using  A3,  which  is  a  combination  of  a  reflection,  a
in R2 with                                                            1                                                                     rotation, and a stretch.                                    
                                                                                                                                                                                                        
the transformation matrices                                           2                                                                     2.7.2 Basis Change                                          
                                                                                                                                                                                                        
A1 =                                                                                                                                       In the  following,  we  will  have  a  closer  look  at  how
                                                                                                                                            transformation matrices                                     
                                                                     3 −1                                                                                                                              
                                                                                                                                            of a linear mapping Φ : V → W change if we change the  bases
cos( π                                                                1 −1                                                                  in V and                                                    
                                                                                                                                                                                                        
4                                                                                                                                          W. Consider two ordered bases                               
                                                                                                                                                                                                        
) − sin( π                                                            . (2.97)                                                              B = (b1, . . . , bn), B˜ = (˜b1, . . . ,                    
                                                                                                                                                                                                        
4                                                                     Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          ˜bn) (2.98)                                                 
                                                                      Feedback: https://mml-book.com.                                                                                                   
)                                                                                                                                           of V and two ordered bases                                  
                                                                      2.7             Linear              Mappings              53                                                                      

%%%

C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.99)                  Example 2.23 (Basis Change)                                           54 Linear Algebra                                           
                                                                                                                                                                                                        
of W. Moreover, AΦ ∈ Rm×n                                             Consider a transformation matrix                                      we obtain a diagonal transformation matrix                  
                                                                                                                                                                                                        
is the transformation matrix of the linear                            A =                                                                   A˜ =                                                        
                                                                                                                                                                                                        
mapping Φ : V → W with respect to the bases B and C, and A˜                                                                                                                                           
                                                                                                                                                                                                        
Φ ∈ Rm×n                                                              2 1                                                                   3 0                                                         
                                                                                                                                                                                                        
is the corresponding transformation mapping with respect  to          1 2                                                                  0 1                                                        
B˜ and C˜.                                                                                                                                                                                              
                                                                      (2.100)                                                               (2.102)                                                     
In the following, we will  investigate  how  A  and  A˜  are                                                                                                                                            
related, i.e., how/                                                   with respect to the canonical basis in R2                             with respect to B, which is easier to work with than A.     
                                                                                                                                                                                                        
whether we can transform AΦ into A˜                                   . If we define a new basis                                            In the following, we will look at  mappings  that  transform
                                                                                                                                            coordinate                                                  
Φ if we choose to perform a basis                                     B = (                                                                                                                            
                                                                                                                                            vectors with respect to one basis  into  coordinate  vectors
change from B, C to B, ˜ C˜.                                          1                                                                     with respect to                                             
                                                                                                                                                                                                        
Remark.   We   effectively    get    different    coordinate          1                                                                     a different basis. We will state our main result  first  and
representations of the                                                                                                                      then provide an                                             
                                                                                                                                                                                                       
identity mapping idV . In the context of  Figure  2.2,  this                                                                                explanation.                                                
would mean to                                                         ,                                                                                                                                 
                                                                                                                                            Theorem 2.20 (Basis Change). For a linear mapping Φ : V → W,
map coordinates with respect to (e1,  e2)  onto  coordinates                                                                               ordered                                                     
with respect to                                                                                                                                                                                         
                                                                      1                                                                     bases                                                       
(b1, b2) without changing the  vector  x.  By  changing  the                                                                                                                                            
basis and correspondingly the representation of vectors, the          −1                                                                    B = (b1, . . . , bn), B˜ = (˜b1, . . . ,                    
transformation matrix with                                                                                                                                                                              
                                                                                                                                           ˜bn) (2.103)                                                
respect to this new basis can  have  a  particularly  simple                                                                                                                                            
form that allows                                                      ) (2.101)                                                             of V and                                                    
                                                                                                                                                                                                        
for straightforward computation. ♢                                    ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.104)       
                                                                      by      Cambridge       University       Press       (2020).                                                                      

%%%

of W, and a transformation matrix AΦ of Φ with respect to  B          sijbi                                                                 with respect to                                             
and C, the                                                                                                                                                                                              
                                                                      , j = 1, . . . , n . (2.106)                                          C. Note that both S and T are regular.                      
corresponding transformation matrix A˜                                                                                                                                                                  
                                                                      Similarly, we write the new basis  vectors  C˜  of  W  as  a          We are going to look  at  Φ(˜bj  )  from  two  perspectives.
Φ with respect to the bases B˜ and C˜                                 linear combination                                                    First, applying the                                         
                                                                                                                                                                                                        
is given as                                                           of the basis vectors of C, which yields                               mapping Φ, we get that for all j = 1, . . . , n             
                                                                                                                                                                                                        
A˜                                                                    c˜k = t1kc1 + · · · + tmkcm =                                         Φ(˜bj ) = Xm                                                
                                                                                                                                                                                                        
Φ = T                                                                 Xm                                                                    k=1                                                         
                                                                                                                                                                                                        
−1AΦS . (2.105)                                                       l=1                                                                   a˜kjc˜k                                                     
                                                                                                                                                                                                        
Here, S ∈ Rn×n                                                        tlkcl                                                                 | {z }                                                      
                                                                                                                                                                                                        
is the transformation matrix of idV that maps coordinates             , k = 1, . . . , m . (2.107)                                          ∈W                                                          
                                                                                                                                                                                                        
with respect to B˜ onto coordinates with respect to B, and T          We define S = ((sij )) ∈ Rn×n as the  transformation  matrix          (2.107) =                                                   
∈ Rm×m is the                                                         that maps                                                                                                                         
                                                                                                                                            Xm                                                          
transformation matrix of  idW  that  maps  coordinates  with          coordinates with respect to B˜ onto coordinates with respect                                                                      
respect to C˜ onto                                                    to B and                                                              k=1                                                         
                                                                                                                                                                                                        
coordinates with respect to C.                                        T = ((tlk)) ∈ Rm×m as the transformation  matrix  that  maps          a˜kjXm                                                      
                                                                      coordinates                                                                                                                       
Proof Following Drumm and Weil  (2001),  we  can  write  the                                                                                l=1                                                         
vectors of                                                            with respect to C˜ onto coordinates with respect  to  C.  In                                                                      
                                                                      particular, the jth                                                   tlkcl =                                                     
the new basis B˜ of V as a linear combination of  the  basis                                                                                                                                            
vectors of B,                                                         column of S is the coordinate  representation  of  ˜bj  with          Xm                                                          
                                                                      respect to B and                                                                                                                  
such that                                                                                                                                   l=1 Xm                                                      
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
˜bj = s1jb1 + · · · + snjbn =                                         Feedback: https://mml-book.com.                                       k=1                                                         
                                                                                                                                                                                                        
Xn                                                                    2.7 Linear Mappings 55                                                tlka˜kj!                                                    
                                                                                                                                                                                                        
i=1                                                                   the kth column of T is the coordinate representation of  c˜k          cl                                                          

%%%

, (2.108)                                                             =                                                                     , (2.111)                                                   
                                                                                                                                                                                                        
where we first expressed the new basis vectors c˜k  ∈  W  as          Xm                                                                    such that                                                   
linear combinations of the basis vectors cl  ∈  W  and  then                                                                                                                                            
swapped the order of                                                  l=1 Xn                                                                A˜                                                          
                                                                                                                                                                                                        
summation.                                                            i=1                                                                   Φ = T                                                       
                                                                                                                                                                                                        
Alternatively, when  we  express  the  ˜bj  ∈  V  as  linear          alisij!                                                               −1AΦS , (2.112)                                             
combinations of                                                                                                                                                                                         
                                                                      cl                                                                    which proves Theorem 2.20.                                  
bj ∈ V , we arrive at                                                                                                                                                                                   
                                                                      , j = 1, . . . , n , (2.109b)                                         Theorem 2.20 tells us that with a basis change in  V  (B  is
Φ(˜bj )                                                                                                                                     replaced with                                               
                                                                      where we exploited the linearity of Φ. Comparing (2.108) and                                                                      
(2.106) = Φ Xn                                                        (2.109b),                                                             B˜) and W (C is replaced with C˜), the transformation matrix
                                                                                                                                            AΦ of a                                                     
i=1                                                                   it follows for all j = 1, . . . , n and l = 1, .  .  .  ,  m                                                                      
                                                                      that                                                                  linear mapping Φ : V → W is replaced by an equivalent matrix
sijbi                                                                                                                                       A˜                                                          
                                                                      Xm                                                                                                                                
!                                                                                                                                           Φ with                                                      
                                                                      k=1                                                                                                                               
=                                                                                                                                           A˜                                                          
                                                                      tlka˜kj =                                                                                                                         
Xn                                                                                                                                          Φ = T                                                       
                                                                      Xn                                                                                                                                
i=1                                                                                                                                         −1AΦS. (2.113)                                              
                                                                      i=1                                                                                                                               
sijΦ(bi) =                                                                                                                                  Figure   2.2   illustrates   this   relation:   Consider   a
                                                                      alisij (2.110)                                                        homomorphism Φ : V → W                                      
Xn                                                                                                                                                                                                      
                                                                      and, therefore,                                                       and ordered bases B, B˜ of V and C, C˜ of W. The mapping ΦCB
i=1                                                                                                                                         is an instantiation of Φ and maps basis vectors  of  B  onto
                                                                      T A˜                                                                  linear combinations of                                      
sijXm                                                                                                                                                                                                   
                                                                      Φ = AΦS ∈ R                                                           basis vectors of C. Assume that we know  the  transformation
l=1                                                                                                                                         matrix AΦ                                                   
                                                                      m×n                                                                                                                               
alicl                                               (2.109a)                                                                                of ΦCB with respect to the  ordered  bases  B,  C.  When  we

%%%

perform a basis                                                       bases in the                                                          ΦCB                                                         
                                                                                                                                                                                                        
change from B to B˜ in V and from C  to  C˜  in  W,  we  can          subscripts. The                                                       ΦC˜B˜                                                       
determine the                                                                                                                                                                                           
                                                                      corresponding                                                         ΨBB˜ ΞCC˜ = Ξ−1 S T CC˜                                     
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                                                                            
by Cambridge University Press (2020).                                 transformation                                                        −1                                                          
                                                                                                                                                                                                        
56 Linear Algebra                                                     matrices are in red.                                                  A˜ Φ                                                        
                                                                                                                                                                                                        
Figure 2.2 For a                                                      V W                                                                   AΦ                                                          
                                                                                                                                                                                                        
homomorphism                                                          B                                                                     Vector spaces                                               
                                                                                                                                                                                                        
Φ : V → W and                                                         B˜ C˜                                                                 Ordered bases                                               
                                                                                                                                                                                                        
ordered bases B, B˜                                                   C                                                                     corresponding transformation matrix A˜                      
                                                                                                                                                                                                        
of V and C, C˜ of W                                                   Φ                                                                     Φ as follows: First, we find the  matrix  representation  of
                                                                                                                                            the linear mapping ΨBB˜ : V → V that maps  coordinates  with
(marked in blue),                                                     ΦCB                                                                   respect to the new basis B˜ onto  the  (unique)  coordinates
                                                                                                                                            with                                                        
we can express the                                                    ΦC˜B˜                                                                                                                             
                                                                                                                                            respect to the “old” basis B (in  V  ).  Then,  we  use  the
mapping ΦC˜B˜ with                                                    ΨBB˜ S T ΞCC˜                                                         transformation matrix AΦ of  ΦCB  :  V  →  W  to  map  these
                                                                                                                                            coordinates onto the coordinates                            
respect to the bases                                                  A˜ Φ                                                                                                                              
                                                                                                                                            with respect to C in W. Finally, we  use  a  linear  mapping
B, ˜ C˜ equivalently as                                               AΦ                                                                    ΞCC˜ : W → W                                                
                                                                                                                                                                                                        
a composition of the                                                  V W                                                                   to map the coordinates with respect to  C  onto  coordinates
                                                                                                                                            with respect to                                             
homomorphisms                                                         B                                                                                                                                 
                                                                                                                                            C˜. Therefore, we can express the linear mapping ΦC˜B˜ as  a
ΦC˜B˜ =                                                               B˜ C˜                                                                 composition of                                              
                                                                                                                                                                                                        
ΞCC˜ ◦ ΦCB ◦ ΨBB˜                                                     C                                                                     linear mappings that involve the “old” basis:               
                                                                                                                                                                                                        
with respect to the                                                   Φ                                                                     ΦC˜B˜ = ΞCC˜ ◦ ΦCB ◦ ΨBB˜ = Ξ−1                             
                                                                                                                                                                                                        

%%%

CC˜ ◦ ΦCB ◦ ΨBB˜ . (2.114)                                            overall transformation matrix is AΨ◦Φ = AΨAΦ. ♢                       then AΦ : B → C, A˜                                         
                                                                                                                                                                                                        
Concretely, we use ΨBB˜ = idV and ΞCC˜ =  idW  ,  i.e.,  the          In light of this remark, we can look at basis  changes  from          Φ : B˜ → C˜, S : B˜ → B, T : C˜ → C and                     
identity mappings                                                     the perspective of composing linear mappings:                                                                                     
                                                                                                                                            T                                                           
that map vectors onto themselves,  but  with  respect  to  a          AΦ is the transformation matrix of a linear mapping ΦCB :  V                                                                      
different basis.                                                      → W                                                                   −1                                                          
                                                                                                                                                                                                        
Definition 2.21 (Equivalence). Two matrices A, A˜ ∈ Rm×n              with respect to the bases B, C.                                       : C → C˜, and                                               
                                                                                                                                                                                                        
equivalent are equivalent                                             A˜                                                                    B˜ → C˜ = B˜ → B→ C → C˜ (2.115)                            
                                                                                                                                                                                                        
if there exist regular matrices S ∈ Rn×n and T ∈ Rm×m,  such          Φ is the transformation matrix of the linear mapping ΦC˜B˜ :          A˜                                                          
that                                                                  V → W                                                                                                                             
                                                                                                                                            Φ = T                                                       
A˜ = T                                                                with respect to the bases B, ˜ C˜.                                                                                                
                                                                                                                                            −1AΦS . (2.116)                                             
−1AS.                                                                 S is the transformation matrix of a linear mapping ΨBB˜ :  V                                                                      
                                                                      → V                                                                   Note that the execution order in (2.116) is  from  right  to
Definition 2.22 (Similarity). Two matrices A, A˜ ∈ Rn×n                                                                                     left because vectors are multiplied at the  right-hand  side
                                                                      (automorphism) that represents B˜ in terms of B. Normally, Ψ          so that x 7→ Sx 7→ AΦ(Sx) 7→                                
similar are similar if                                                = idV is                                                                                                                          
                                                                                                                                            T                                                           
there exists a regular matrix S ∈ Rn×n with A˜ = S                    the identity mapping in V .                                                                                                       
                                                                                                                                            −1                                                          
−1AS                                                                  Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       AΦ(Sx)                                                      
Remark. Similar matrices  are  always  equivalent.  However,                                                                                                                                            
equivalent matrices are not necessarily similar. ♢                    2.7 Linear Mappings 57                                                                                                           
                                                                                                                                                                                                        
Remark. Consider vector spaces V, W, X. From the remark that          T is the transformation matrix of a linear mapping ΞCC˜ :  W          = A˜                                                        
follows                                                               → W                                                                                                                               
                                                                                                                                            Φx.                                                         
Theorem 2.17, we already know that for linear mappings Φ : V          (automorphism) that represents C˜ in terms of C. Normally, Ξ                                                                      
→ W                                                                   = idW is                                                              Example 2.24 (Basis Change)                                 
                                                                                                                                                                                                        
and Ψ : W → X the mapping Ψ ◦ Φ : V → X is also linear. With          the identity mapping in W.                                            Consider a linear mapping Φ : R3 → R4  whose  transformation
                                                                                                                                            matrix is                                                   
transformation matrices  AΦ  and  AΨ  of  the  corresponding          If we (informally) write down the  transformations  just  in                                                                      
mappings,                                                the          terms                       of                        bases,          AΦ                                                         =

%%%

                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                      ,                                                                   0                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1 2 0                                                                 0                                                                                                                                
                                                                                                                                                                                                        
−1 1 3                                                                1                                                                                                                                
                                                                                                                                                                                                        
3 7 1                                                                 0                                                                                                                                
                                                                                                                                                                                                        
−1 2 4                                                                                                                                                                                                
                                                                                                                                                                                                        
                                                                      ,                                                                   ,                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
(2.117)                                                               0                                                                                                                                
                                                                                                                                                                                                        
with respect to the standard bases                                    1                                                                     0                                                           
                                                                                                                                                                                                        
B = (                                                                                                                                      1                                                           
                                                                                                                                                                                                        
                                                                     ), C = (                                                             0                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        

%%%

                                                                     0                                                                     0                                                           
                                                                                                                                                                                                        
,                                                                     0                                                                     1                                                           
                                                                                                                                                                                                        
                                                                     1                                                                     1                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                           ,                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                     ). (2.118)                                                            1                                                           
                                                                                                                                                                                                        
1                                                                     We seek the transformation matrix A˜                                  0                                                           
                                                                                                                                                                                                        
0                                                                     Φ of Φ with respect to the new bases                                  1                                                           
                                                                                                                                                                                                        
                                                                     B˜ = (                                                                                                                           
                                                                                                                                                                                                        
                                                                                                                                          ) ∈ R                                                      
                                                                                                                                                                                                        
                                                                                                                                          3                                                           
                                                                                                                                                                                                        
                                                                     1                                                                     , C˜ = (                                                    
                                                                                                                                                                                                        
,                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                      ,                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
0                                                                                                                                          1                                                           
                                                                                                                                                                                                        

%%%

0                                                                                                                                          1                                                           
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                     1                                                                     ). (2.119)                                                  
                                                                                                                                                                                                        
,                                                                     1                                                                     Then,                                                       
                                                                                                                                                                                                        
                                                                     0                                                                     S =                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          1 0 1                                                       
                                                                                                                                                                                                        
1                                                                                                                                          1 1 0                                                       
                                                                                                                                                                                                        
0                                                                     ,                                                                     0 1 1                                                       
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                           , T =                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
,                                                                     0                                                                     1 1 0 1                                                     
                                                                                                                                                                                                        

%%%

1 0 1 0                                                               A˜                                                                                                                               
                                                                                                                                                                                                        
0 1 1 0                                                               Φ = T                                                                 3 2 1                                                       
                                                                                                                                                                                                        
0 0 0 1                                                               −1AΦS =                                                               0 4 2                                                       
                                                                                                                                                                                                        
                                                                     1                                                                     10 8 4                                                      
                                                                                                                                                                                                        
                                                                     2                                                                     1 6 3                                                       
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
, (2.120)                                                                                                                                                                                             
                                                                                                                                                                                                        
where the ith column of S is the  coordinate  representation                                                                                                                                          
of ˜bi                                                                                                                                                                                                  
                                                                      1 1 −1 −1                                                             (2.121a)                                                    
in                                                                                                                                                                                                      
                                                                      1 −1 1 −1                                                             =                                                           
terms of the basis vectors of B. Since  B  is  the  standard                                                                                                                                            
basis, the coordinate representation is  straightforward  to          −1 1 1 1                                                                                                                         
find. For a general basis B,                                                                                                                                                                            
                                                                      0 0 0 2                                                                                                                          
we would need to solve a linear equation system to find  the                                                                                                                                            
λi such that                                                                                                                                                                                          
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                                                                          
by Cambridge University Press (2020)                                                                                                                                                                    
                                                                                                                                           −4 −4 −2                                                    
58 Linear Algebra                                                                                                                                                                                       
                                                                                                                                           6 0 0                                                       
P3                                                                                                                                                                                                      
                                                                                                                                           4 8 4                                                       
i=1 λibi = ˜bj , j = 1, . . . , 3. Similarly, the jth column                                                                                                                                            
of T is the coordinate representation of c˜j in terms of the                                                                               1 6 3                                                       
basis vectors of C.                                                                                                                                                                                     
                                                                                                                                                                                                      
Therefore,                     we                     obtain                                                                                                                                            

%%%

                                                                     range                                                                 and image of a                                              
                                                                                                                                                                                                        
                                                                     Im(Φ) := Φ(V ) = {w ∈ W|∃v ∈ V : Φ(v) = w} . (2.123)                  linear mapping                                              
                                                                                                                                                                                                        
                                                                     domain We also call V and W also the domain and codomain  of          Φ : V → W.                                                  
                                                                      Φ, respectively.                                                                                                                  
. (2.121b)                                                                                                                                  Im(Φ)                                                       
                                                                      codomain                                                                                                                          
In Chapter 4, we will be able to exploit the  concept  of  a                                                                                0W                                                          
basis change                                                          Intuitively, the kernel is the set of vectors v ∈ V  that  Φ                                                                      
                                                                      maps onto the                                                         ker(Φ)                                                      
to find a basis with respect  to  which  the  transformation                                                                                                                                            
matrix  of  an  endomorphism  has  a   particularly   simple          neutral element 0W ∈ W. The image is the set of vectors w  ∈          0V                                                          
(diagonal) form. In Chapter 10, we                                    W that                                                                                                                            
                                                                                                                                            Φ : V → W V W                                               
will  look  at  a  data  compression  problem  and  find   a          can be “reached” by Φ from any vector in V . An illustration                                                                      
convenient basis onto                                                 is given in                                                           Φ is injective (one-to-one) if and only if ker(Φ) = {0}.    
                                                                                                                                                                                                        
which  we  can  project  the  data  while   minimizing   the          Figure 2.2.                                                           ♢                                                           
compression loss.                                                                                                                                                                                       
                                                                      Remark. Consider a linear mapping Φ : V → W, where V, W  are          Remark (Null Space and Column Space). Let us  consider  A  ∈
2.7.3 Image and Kernel                                                vector                                                                Rm×n and                                                    
                                                                                                                                                                                                        
The  image  and  kernel  of  a  linear  mapping  are  vector          spaces.                                                               a linear mapping Φ : Rn → Rm, x 7→ Ax.                      
subspaces  with  certain  important   properties.   In   the                                                                                                                                            
following, we will characterize them                                  It always holds that Φ(0V  )  =  0W  and,  therefore,  0V  ∈          For A = [a1, . . . , an], where ai are the columns of A,  we
                                                                      ker(Φ). In                                                            obtain                                                      
more carefully.                                                                                                                                                                                         
                                                                      particular, the null space is never empty.                            Im(Φ) = {Ax : x ∈ R                                         
Definition 2.23 (Image and Kernel).                                                                                                                                                                     
                                                                      Im(Φ) ⊆ W is a subspace of W, and ker(Φ) ⊆ V is  a  subspace          n                                                           
kernel For Φ : V → W, we define the kernel/null space                 of V .                                                                                                                            
                                                                                                                                            } =                                                         
null space                                                            Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       (Xn                                                         
ker(Φ) := Φ−1                                                                                                                                                                                           
                                                                      2.7 Linear Mappings 59                                                i=1                                                         
(0W ) = {v ∈ V : Φ(v) = 0W } (2.122)                                                                                                                                                                    
                                                                      Figure 2.2 Kernel                                                     xiai                                                        
image            and             the             image/range                                                                                                                                            

%%%

: x1, . . . , xn ∈ R                                                  ♢                                                                                                                                
                                                                                                                                                                                                        
)                                                                     Example 2.25 (Image and Kernel of a Linear Mapping)                   1 2 −1 0                                                    
                                                                                                                                                                                                        
(2.124a)                                                              The mapping                                                           1 0 0 1                                                    
                                                                                                                                                                                                        
= span[a1, . . . , an] ⊆ R                                            Φ : R                                                                                                                            
                                                                                                                                                                                                        
m , (2.124b)                                                          4 → R                                                                                                                            
                                                                                                                                                                                                        
i.e., the image is the span of the columns of A, also called          2                                                                                                                                
the column column space                                                                                                                                                                                 
                                                                      ,                                                                                                                                
space. Therefore, the column space (image) is a subspace  of                                                                                                                                            
Rm, where                                                                                                                                  x1                                                          
                                                                                                                                                                                                        
m is the “height” of the matrix.                                                                                                           x2                                                          
                                                                                                                                                                                                        
rk(A) = dim(Im(Φ)).                                                                                                                        x3                                                          
                                                                                                                                                                                                        
The kernel/null space ker(Φ) is the general solution to  the                                                                               x4                                                          
homogeneous system of linear equations Ax = 0  and  captures                                                                                                                                            
all possible                                                          x1                                                                                                                               
                                                                                                                                                                                                        
linear combinations of the elements in Rn                             x2                                                                                                                               
                                                                                                                                                                                                        
that produce 0 ∈ Rm.                                                  x3                                                                                                                               
                                                                                                                                                                                                        
The kernel is a subspace of Rn                                        x4                                                                     =                                                         
                                                                                                                                                                                                        
, where n is the “width” of the matrix.                                                                                                                                                               
                                                                                                                                                                                                        
The kernel focuses on the relationship  among  the  columns,                                                                               x1 + 2x2 − x3                                               
and we can                                                                                                                                                                                              
                                                                                                                                           x1 + x4                                                     
use it to determine whether/how we can express a column as a                                                                                                                                            
linear                                                                                                                                                                                                
                                                                                                                                                                                                        
combination of other columns.                                         7→                                                                    (2.125a)                                                    
                                                                                                                                                                                                        

%%%

©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          1                                                                                                                                
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                                                                                           0                                                           
60 Linear Algebra                                                                                                                                                                                       
                                                                      (2.125b)                                                              1                                                           
= x1                                                                                                                                                                                                    
                                                                      is linear. To determine Im(Φ), we can take the span  of  the                                                                     
                                                                     columns of the                                                                                                                    
                                                                                                                                            ] . (2.126)                                                 
1                                                                     transformation matrix and obtain                                                                                                  
                                                                                                                                            To compute the kernel (null space) of Φ, we need to solve Ax
1                                                                     Im(Φ) = span[                                                        = 0, i.e.,                                                  
                                                                                                                                                                                                        
                                                                     1                                                                     we need to solve a homogeneous equation system. To do  this,
                                                                                                                                            we use                                                      
+ x2                                                                  1                                                                                                                                 
                                                                                                                                            Gaussian elimination to transform A into reduced row-echelon
                                                                                                                                          form:                                                       
                                                                                                                                                                                                        
2                                                                     ,                                                                                                                                
                                                                                                                                                                                                        
0                                                                                                                                          1 2 −1 0                                                    
                                                                                                                                                                                                        
                                                                     2                                                                     1 0 0 1                                                    
                                                                                                                                                                                                        
+ x3                                                                  0                                                                     ⇝ · · · ⇝                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
−1                                                                    ,                                                                     1 0 0 1                                                     
                                                                                                                                                                                                        
0                                                                                                                                          0 1 −                                                       
                                                                                                                                                                                                        
                                                                     −1                                                                    1                                                           
                                                                                                                                                                                                        
+ x4                                                                  0                                                                     2 −                                                         
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
0                                                                     ,                                                                     2                                                           

%%%

                                                                     Overall, this gives us the kernel (null space) as                     −1                                                          
                                                                                                                                                                                                        
. (2.127)                                                             ker(Φ) = span[                                                        1                                                           
                                                                                                                                                                                                        
This matrix is in reduced row-echelon form, and we  can  use                                                                               2                                                           
the Minus1 Trick to compute  a  basis  of  the  kernel  (see                                                                                                                                            
Section 2.3.3). Alternatively,                                                                                                             0                                                           
                                                                                                                                                                                                        
we can express the non-pivot columns (columns 3  and  4)  as                                                                               1                                                           
linear combinations of the pivot columns (columns 1 and  2).                                                                                                                                            
The third column a3 is                                                                                                                                                                                
                                                                                                                                                                                                        
equivalent to −                                                       0                                                                                                                                
                                                                                                                                                                                                        
1                                                                     1                                                                                                                                
                                                                                                                                                                                                        
2                                                                     2                                                                                                                                
                                                                                                                                                                                                        
times the second column a2. Therefore, 0 = a3+                        1                                                                     ] . (2.128)                                                 
                                                                                                                                                                                                        
1                                                                     0                                                                     rank-nullity                                                
                                                                                                                                                                                                        
2                                                                                                                                          theorem Theorem  2.24  (Rank-Nullity  Theorem).  For  vector
                                                                                                                                            spaces V, W and a linear mapping Φ : V → W it holds that    
a2. In                                                                                                                                                                                                 
                                                                                                                                            dim(ker(Φ)) + dim(Im(Φ)) = dim(V ). (2.129)                 
the same way, we see that a4 = a1−                                                                                                                                                                     
                                                                                                                                            fundamental The rank-nullity theorem is also referred to  as
1                                                                                                                                          the fundamental theorem                                     
                                                                                                                                                                                                        
2                                                                     ,                                                                     theorem of linear                                           
                                                                                                                                                                                                        
a2 and, therefore, 0 = a1−                                                                                                                 mappings                                                    
                                                                                                                                                                                                        
1                                                                                                                                          of  linear  mappings  (Axler,  2015,  theorem   3.22).   The
                                                                                                                                            following are direct                                        
2                                                                                                                                                                                                      
                                                                                                                                            consequences of Theorem 2.24:                               
a2−a4.                                                                                                                                                                                                 
                                                                                                                                            If dim(Im(Φ)) < dim(V ), then ker(Φ) is  non-trivial,  i.e.,

%%%

the kernel                                                            Remark. In the machine learning literature, the  distinction          Therefore, an affine subspace is  not  a  (linear)  subspace
                                                                      between linear                                                        (vector subspace)                                           
contains more than 0V and dim(ker(Φ)) ⩾ 1.                                                                                                                                                              
                                                                      and affine is sometimes  not  clear  so  that  we  can  find          of V for x0 ∈/ U.                                           
If AΦ is the transformation matrix of Φ with respect  to  an          references to affine                                                                                                              
ordered basis                                                                                                                               Examples of affine subspaces are points, lines,  and  planes
                                                                      spaces/mappings as linear spaces/mappings. ♢                          in R3                                                       
and  dim(Im(Φ))  <  dim(V  ),  then  the  system  of  linear                                                                                                                                            
equations AΦx =                                                       2.8.1 Affine Subspaces                                                , which                                                     
                                                                                                                                                                                                        
0 has infinitely many solutions.                                      Definition 2.25 (Affine Subspace). Let V be a vector  space,          do not (necessarily) go through the origin.                 
                                                                      x0 ∈ V and                                                                                                                        
If  dim(V  )  =  dim(W),  then   the   following   three-way                                                                                Remark. Consider two affine subspaces L = x0 + U  and  L˜  =
equivalence holds:                                                    U ⊆ V a subspace. Then the subset                                     x˜0 + U˜ of a                                               
                                                                                                                                                                                                        
– Φ is injective                                                      L = x0 + U := {x0 + u : u ∈ U} (2.130a)                               vector space V . Then, L ⊆ L˜ if and only if U ⊆ U˜ and x0 −
                                                                                                                                            x˜0 ∈ U˜.                                                   
– Φ is surjective                                                     = {v ∈ V |∃u ∈ U : v = x0 + u} ⊆ V (2.130b)                                                                                       
                                                                                                                                            Affine subspaces are often described by parameters: Consider
– Φ is bijective                                                      is called affine subspace or linear manifold of  V  .  U  is          a k-dimensional affine space L = x0 + U of V . If (b1, . . .
                                                                      called direction or affine subspace                                   , bk) is an ordered basis of                                
since Im(Φ) ⊆ W.                                                                                                                                                                                        
                                                                      linear manifold                                                       U, then every element x ∈ L can be uniquely described as    
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                       direction                                                             x = x0 + λ1b1 + . . . + λkbk , (2.131)                      
                                                                                                                                                                                                        
2.8 Affine Spaces 61                                                  direction space, and x0 is called support point. In  Chapter          where λ1, . . . , λk ∈  R.  This  representation  is  called
                                                                      12, we refer to                                                       parametric equation parametric equation                     
2.8 Affine Spaces                                                                                                                                                                                       
                                                                      direction space                                                       of L with directional vectors b1, . . . , bk and  parameters
In the following, we will have a closer look at spaces  that                                                                                λ1, . . . , λk. ♢ parameters                                
are offset from                                                       support point                                                                                                                     
                                                                                                                                            Example 2.26 (Affine Subspaces)                             
the  origin,  i.e.,  spaces  that  are  no   longer   vector          such a subspace as a hyperplane.                                                                                                  
subspaces. Moreover, we                                                                                                                     One-dimensional affine subspaces are called lines and can be
                                                                      hyperplane                                                            written line                                                
will briefly discuss properties of  mappings  between  these                                                                                                                                            
affine spaces,                                                        Note that the definition of an affine subspace excludes 0 if          as y = x0 + λb1, where λ ∈ R and U = span[b1] ⊆ Rn          
                                                                      x0 ∈/ U.                                                                                                                          
which          resemble           linear           mappings.                                                                                is      a      onedimensional      subspace      of       Rn

%%%

. This means that a line is defined by a  support  point  x0          U of Rn                                                               subspaces).                                                 
and a vector b1 that defines the direction. See Figure 2.2                                                                                                                                              
                                                                      . This means that a hyperplane is defined by a support point          For A ∈ Rm×n and x ∈ Rm,  the  solution  of  the  system  of
for an illustration.                                                                                                                        linear equations Aλ = x is either the empty set or an affine
                                                                      x0 and (n − 1) linearly independent vectors b1, . . . , bn−1          subspace of Rn of                                           
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          that span the                                                                                                                     
by Cambridge University Press (2020).                                                                                                       dimension n − rk(A). In  particular,  the  solution  of  the
                                                                      direction space. In R2                                                linear equation                                             
62 Linear Algebra                                                                                                                                                                                       
                                                                      , a line is also a hyperplane. In R3                                  λ1b1 + . . . + λnbn = x, where (λ1, . . . , λn) ̸= (0, . . .
Two-dimensional affine subspaces of Rn                                                                                                      , 0), is a hyperplane                                       
                                                                      , a plane is also                                                                                                                 
plane are called planes. The parametric equation for  planes                                                                                in Rn                                                       
is y = x0 + λ1b1 + λ2b2, where λ1, λ2 ∈ R                             a hyperplane.                                                                                                                     
                                                                                                                                            .                                                           
and U = span[b1, b2] ⊆ Rn                                             Figure 2.2 Lines                                                                                                                  
                                                                                                                                            In Rn                                                       
. This means that a plane is defined by a                             are affine subspaces.                                                                                                             
                                                                                                                                            , every k-dimensional affine subspace is the solution of  an
support point x0 and two linearly independent vectors b1, b2          Vectors y on a line                                                   inhomogeneous system of linear equations Ax = b, where  A  ∈
that span                                                                                                                                   Rm×n                                                        
                                                                      x0 + λb1 lie in an                                                                                                                
the direction space.                                                                                                                        , b ∈                                                       
                                                                      affine subspace L                                                                                                                 
In Rn                                                                                                                                       Rm and rk(A) = n − k. Recall that for  homogeneous  equation
                                                                      with support point                                                    systems                                                     
hyperplane , the (n − 1)-dimensional  affine  subspaces  are                                                                                                                                            
called hyperplanes,                                                   x0 and direction b1.                                                  Ax = 0 the solution was a vector subspace, which we can also
                                                                                                                                            think of                                                    
and the corresponding parametric equation is y = x0 +                 0                                                                                                                                 
                                                                                                                                            as a special affine space with support point x0 = 0. ♢      
Pn−1                                                                  x0                                                                                                                                
                                                                                                                                            2.8.2 Affine Mappings                                       
i=1 λibi                                                              b1                                                                                                                                
                                                                                                                                            Similar to linear mappings between vector spaces,  which  we
,                                                                     y                                                                     discussed                                                   
                                                                                                                                                                                                        
where  b1,  .  .  .  ,  bn−1  form  a  basis  of  an  (n   −          L = x0 + λb1                                                          in Section 2.7, we can define affine  mappings  between  two
1)-dimensional subspace                                                                                                                     affine spaces.                                              
                                                                      Remark (Inhomogeneous systems of linear equations and affine                                                                      

%%%

Linear and affine mappings are closely  related.  Therefore,          ′                                                                     lengths and distances, which  we  will  use  for  orthogonal
many properties                                                                                                                             projections. Projections turn out to be key in many  machine
                                                                      : W → X is                                                            learning algorithms, such as                                
that we already know from linear mappings,  e.g.,  that  the                                                                                                                                            
composition of                                                        affine.                                                               linear regression and principal component analysis, both  of
                                                                                                                                            which we will                                               
linear mappings is a linear mapping, also  hold  for  affine          Affine mappings keep the geometric structure invariant. They                                                                      
mappings.                                                             also preserve the dimension and parallelism.                          cover in Chapters 9 and 10, respectively.                   
                                                                                                                                                                                                        
Definition 2.26 (Affine Mapping). For two vector  spaces  V,          2.9 Further Reading                                                   ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
W, a linear                                                                                                                                 by Cambridge University Press (2020).                       
                                                                      There  are  many  resources  for  learning  linear  algebra,                                                                      
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          including the textbooks  by  Strang  (2003),  Golan  (2007),          64 Linear Algebra                                           
Feedback: https://mml-book.com.                                       Axler (2015), and Liesen and                                                                                                      
                                                                                                                                            Exercises                                                   
2.9 Further Reading 63                                                Mehrmann (2015). There are  also  several  online  resources                                                                      
                                                                      that we mentioned in the introduction to  this  chapter.  We          2.1 We consider (R\{−1}, ⋆), where                          
mapping Φ : V → W, and a ∈ W, the mapping                             only covered Gaussian elimination here, but there  are  many                                                                      
                                                                      other approaches for solving systems of                               a ⋆ b := ab + a + b, a, b ∈ R\{−1} (2.134)                  
ϕ : V → W (2.132)                                                                                                                                                                                       
                                                                      linear equations, and we refer to numerical  linear  algebra          a. Show that (R\{−1}, ⋆) is an Abelian group.               
x 7→ a + Φ(x) (2.133)                                                 textbooks by                                                                                                                      
                                                                                                                                            b. Solve                                                    
is an affine mapping from V to W. The vector a is called the          Stoer and Burlirsch (2002), Golub and Van Loan  (2012),  and                                                                      
translation affine mapping                                            Horn and                                                              3 ⋆ x ⋆ x = 15                                              
                                                                                                                                                                                                        
vector of ϕ. translation vector                                       Johnson (2013) for an in-depth discussion.                            in the Abelian group (R\{−1}, ⋆),  where  ⋆  is  defined  in
                                                                                                                                            (2.134).                                                    
Every affine mapping ϕ : V → W is also the composition of  a          In this book, we distinguish between the  topics  of  linear                                                                      
linear                                                                algebra (e.g.,                                                        2.2 Let n be in N\{0}. Let k, x  be  in  Z.  We  define  the
                                                                                                                                            congruence class k¯ of the                                  
mapping Φ : V → W and a translation τ : W →  W  in  W,  such          vectors, matrices, linear independence,  basis)  and  topics                                                                      
that                                                                  related to the                                                        integer k as the set                                        
                                                                                                                                                                                                        
ϕ = τ ◦ Φ. The mappings Φ and τ are uniquely determined.              geometry of a vector space. In Chapter 3, we will  introduce          k = {x ∈ Z | x − k = 0 (modn)}                              
                                                                      the inner                                                                                                                         
The composition ϕ                                                                                                                           = {x ∈ Z | ∃a ∈ Z: (x − k = n · a)} .                       
                                                                      product, which induces a norm. These concepts  allow  us  to                                                                      
′ ◦ ϕ of affine mappings ϕ : V → W, ϕ                                 define angles,                                                        We now define Z/nZ (sometimes written Zn) as the set of  all
                                                                                                                                            congruence                                                  

%%%

classes modulo n. Euclidean division implies that  this  set          group if and                                                                                                                     
is a finite set containing n elements:                                                                                                                                                                  
                                                                      only if n ∈ N\{0} is prime.                                           x, y, z ∈ R                                                 
Zn = {0, 1, . . . , n − 1}                                                                                                                                                                              
                                                                      2.3 Consider the set G of 3 × 3 matrices defined as follows:                                                                     
For all a, b ∈ Zn, we define                                                                                                                                                                            
                                                                      G =                                                                                                                              
a ⊕ b := a + b                                                                                                                                                                                          
                                                                                                                                                                                                      
a. Show that (Zn, ⊕) is a group. Is it Abelian?                                                                                                                                                         
                                                                                                                                           We define · as the standard matrix multiplication.          
b. We now define another operation ⊗ for all a and b  in  Zn                                                                                                                                            
as                                                                                                                                         Is (G, ·) a group? If  yes,  is  it  Abelian?  Justify  your
                                                                                                                                            answer.                                                     
a ⊗ b = a × b , (2.135)                                                                                                                                                                                
                                                                                                                                            2.4 Compute the following matrix products, if possible:     
where a × b represents the usual multiplication in Z.                                                                                                                                                  
                                                                                                                                            Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
Let n = 5. Draw the times table of the  elements  of  Z5\{0}          1 x z                                                                 Feedback: https://mml-book.com.                             
under ⊗, i.e.,                                                                                                                                                                                          
                                                                      0 1 y                                                                 Exercises 65                                                
calculate the products a ⊗ b for all a and b in Z5\{0}.                                                                                                                                                 
                                                                      0 0 1                                                                 a.                                                          
Hence, show that Z5\{0} is closed under ⊗  and  possesses  a                                                                                                                                            
neutral                                                                                                                                                                                               
                                                                                                                                                                                                        
element for ⊗. Display the inverse of all elements in Z5\{0}           ∈ R                                                                                                                            
under ⊗.                                                                                                                                                                                                
                                                                      3×3                                                                   1 2                                                         
Conclude that (Z5\{0}, ⊗) is an Abelian group.                                                                                                                                                          
                                                                                                                                           4 5                                                         
c. Show that (Z8\{0}, ⊗) is not a group.                                                                                                                                                                
                                                                                                                                           7 8                                                         
d. We recall  that  the  B´ezout  theorem  states  that  two                                                                                                                                            
integers a and b are                                                                                                                                                                                  
                                                                                                                                                                                                        
relatively prime (i.e., gcd(a, b) = 1) if and only if  there                                                                                                                                          
exist two integers                                                                                                                                                                                      
                                                                                                                                                                                                      
u and v such that au + bv = 1. Show that (Zn\{0},  ⊗)  is  a                                                                                                                                            

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 0                                                                 c.                                                                                                                               
                                                                                                                                                                                                        
0 1 1                                                                                                                                                                                                 
                                                                                                                                                                                                        
1 0 1                                                                                                                                                                                                 
                                                                                                                                                                                                        
                                                                     1 1 0                                                                                                                            
                                                                                                                                                                                                        
                                                                     0 1 1                                                                 0 3                                                         
                                                                                                                                                                                                        
b.                                                                    1 0 1                                                                 1 −1                                                        
                                                                                                                                                                                                        
                                                                                                                                          2 1                                                         
                                                                                                                                                                                                        
                                                                                                                                          5 2                                                         
                                                                                                                                                                                                        
1 2 3                                                                                                                                                                                                 
                                                                                                                                                                                                        
4 5 6                                                                                                                                                                                                 
                                                                                                                                                                                                        
7 8 9                                                                 1 2 3                                                                                                                            
                                                                                                                                                                                                        
                                                                     4 5 6                                                                                                                            
                                                                                                                                                                                                        
                                                                     7 8 9                                                                 e.                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 0                                                                 d.                                                                                                                               
                                                                                                                                                                                                        
0 1 1                                                                                                                                                                                                 
                                                                                                                                                                                                        
1 0 1                                                                 1 2 1 2                                                               0 3                                                         
                                                                                                                                                                                                        
                                                                     4 1 −1 −4                                                             1 −1                                                        
                                                                                                                                                                                                        

%%%

2 1                                                                   2 −1 1 3                                                              A =                                                         
                                                                                                                                                                                                        
5 2                                                                   5 2 −4 2                                                                                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          1 −1 0 0 1                                                  
                                                                                                                                                                                                        
                                                                     , b =                                                                 1 1 0 −3 0                                                  
                                                                                                                                                                                                        
1 2 1 2                                                                                                                                    2 −1 0 1 −1                                                 
                                                                                                                                                                                                        
4 1 −1 −4                                                                                                                                  −1 2 0 −2 −1                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
2.5 Find the set S of all solutions in x  of  the  following                                                                                                                                          
inhomogeneous linear                                                                                                                                                                                    
                                                                      1                                                                                                                                
systems Ax = b, where A and b are defined as follows:                                                                                                                                                   
                                                                      −2                                                                                                                               
a.                                                                                                                                                                                                      
                                                                      4                                                                     , b =                                                       
A =                                                                                                                                                                                                     
                                                                      6                                                                                                                                
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                           3                                                           
1 1 −1 −1                                                                                                                                                                                               
                                                                      b.                                                                    6                                                           
2                  5                  −7                  −5                                                                                                                                            

%%%

5                                                                                                                                                                                                     
                                                                                                                                                                                                        
−1                                                                     .                                                                   and P3                                                      
                                                                                                                                                                                                        
                                                                     ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          i=1 xi = 1.                                                 
                                                                      by Cambridge University Press (2020).                                                                                             
                                                                                                                                           2.8 Determine the inverses  of  the  following  matrices  if
                                                                      66 Linear Algebra                                                     possible:                                                   
                                                                                                                                                                                                       
                                                                      2.7 Find all solutions in x =                                         a.                                                          
                                                                                                                                                                                                       
                                                                                                                                           A =                                                         
2.6 Using Gaussian elimination, find all  solutions  of  the                                                                                                                                            
inhomogeneous equation system Ax = b with                                                                                                                                                             
                                                                                                                                                                                                        
A =                                                                   x1                                                                                                                               
                                                                                                                                                                                                        
                                                                     x2                                                                    2 3 4                                                       
                                                                                                                                                                                                        
                                                                     x3                                                                    3 4 5                                                       
                                                                                                                                                                                                        
0 1 0 0 1 0                                                                                                                                4 5 6                                                       
                                                                                                                                                                                                        
0 0 0 1 1 0                                                            ∈ R3 of the equation system Ax = 12x,                                                                                          
                                                                                                                                                                                                        
0 1 0 0 0 1                                                           where                                                                                                                            
                                                                                                                                                                                                        
                                                                     A =                                                                   b.                                                          
                                                                                                                                                                                                        
 , b =                                                                                                                                    A =                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     6 4 3                                                                                                                            
                                                                                                                                                                                                        
2                                                                     6 0 9                                                                                                                            
                                                                                                                                                                                                        
−1                                                                    0 8 0                                                                                                                            
                                                                                                                                                                                                        
1                                                                                                                                          1                  0                   1                   0

%%%

0 1 1 0                                                               C = {(ξ1, ξ2, ξ3) ∈ R3                                                 , x3 =                                                    
                                                                                                                                                                                                        
1 1 0 1                                                               | ξ1 − 2ξ2 + 3ξ3 = γ}                                                                                                            
                                                                                                                                                                                                        
1 1 1 0                                                               d. D = {(ξ1, ξ2, ξ3) ∈ R3                                                                                                        
                                                                                                                                                                                                        
                                                                     | ξ2 ∈ Z}                                                             3                                                           
                                                                                                                                                                                                        
                                                                     2.10 Are the following sets of vectors linearly independent?          −3                                                          
                                                                                                                                                                                                        
                                                                     a.                                                                    8                                                           
                                                                                                                                                                                                        
                                                                     x1 =                                                                                                                             
                                                                                                                                                                                                        
2.9 Which of the following sets are subspaces of R3                                                                                                                                                   
                                                                                                                                                                                                        
?                                                                                                                                          b.                                                          
                                                                                                                                                                                                        
a. A = {(λ, λ + µ                                                     2                                                                     x1 =                                                        
                                                                                                                                                                                                        
3                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
, λ − µ                                                               3                                                                                                                                
                                                                                                                                                                                                        
3                                                                                                                                                                                                     
                                                                                                                                                                                                        
) | λ, µ ∈ R}                                                          , x2 =                                                                                                                         
                                                                                                                                                                                                        
b. B = {(λ                                                                                                                                                                                            
                                                                                                                                                                                                        
2                                                                                                                                                                                                     
                                                                                                                                                                                                        
, −λ                                                                  1                                                                     1                                                           
                                                                                                                                                                                                        
2                                                                     1                                                                     2                                                           
                                                                                                                                                                                                        
, 0) | λ ∈ R}                                                         −2                                                                    1                                                           
                                                                                                                                                                                                        
c. Let γ be in R.                                                                                                                          0                                                           
                                                                                                                                                                                                        

%%%

0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          2.11 Write                                                  
                                                                                                                                                                                                        
                                                                                                                                          y =                                                         
                                                                                                                                                                                                        
                                                                     , x3 =                                                                                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
, x2 =                                                                                                                                     1                                                           
                                                                                                                                                                                                        
                                                                                                                                          −2                                                          
                                                                                                                                                                                                        
                                                                                                                                          5                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1                                                                     as linear combination of                                    
                                                                                                                                                                                                        
                                                                     0                                                                     x1 =                                                        
                                                                                                                                                                                                        
1                                                                     0                                                                                                                                
                                                                                                                                                                                                        
1                                                                     1                                                                                                                                
                                                                                                                                                                                                        
0                                                                     1                                                                     1                                                           
                                                                                                                                                                                                        
1                                                                                                                                          1                                                           
                                                                                                                                                                                                        
1                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        

%%%

 , x2 =                                                                                                                                   −1                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
2                                                                     1                                                                                                                                
                                                                                                                                                                                                        
3                                                                     1                                                                     ,                                                           
                                                                                                                                                                                                        
                                                                     −3                                                                                                                               
                                                                                                                                                                                                        
 , x3 =                                                              1                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
2                                                                                                                                          −1                                                          
                                                                                                                                                                                                        
−1                                                                                                                                         1                                                           
                                                                                                                                                                                                        
1                                                                     ,                                                                     −1                                                          
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                                                                                          
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                                                                                                                                                      
Exercises 67                                                                                                                                                                                            
                                                                      2                                                                                                                                
2.12 Consider two subspaces of R4                                                                                                                                                                       
                                                                      −1                                                                    ] , U2 = span[                                              
:                                                                                                                                                                                                       
                                                                      0                                                                                                                                
U1                          =                          span[                                                                                                                                            

%%%

                                                                                                                                          homogeneous equation system A1x = 0 and U2 is  the  solution
                                                                                                                                            space of the                                                
                                                                                                                                                                                                      
                                                                                                                                            homogeneous equation system A2x = 0 with                    
                                                                                                                                                                                                      
                                                                                                                                            A1 =                                                        
−1                                                                                                                                                                                                     
                                                                                                                                                                                                       
−2                                                                    ,                                                                                                                                 
                                                                                                                                                                                                       
2                                                                                                                                                                                                      
                                                                                                                                                                                                       
1                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                            1 0 1                                                       
                                                                                                                                                                                                      
                                                                                                                                            1 −2 −1                                                     
                                                                     −3                                                                                                                                
                                                                                                                                            2 1 3                                                       
                                                                     6                                                                                                                                 
                                                                                                                                            1 0 1                                                       
,                                                                     −2                                                                                                                                
                                                                                                                                                                                                       
                                                                     −1                                                                                                                                
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                            , A2 =                                                      
2                                                                                                                                                                                                      
                                                                                                                                                                                                       
−2                                                                    ] .                                                                                                                               
                                                                                                                                                                                                       
0                                                                     Determine a basis of U1 ∩ U2.                                                                                                     
                                                                                                                                                                                                       
0                                                                     2.13 Consider two subspaces U1  and  U2,  where  U1  is  the                                                                      
                                                                      solution              space              of              the                                                                     

%%%

3 −3 0                                                                1 −2 −1                                                               .                                                           
                                                                                                                                                                                                        
1 2 3                                                                 2 1 3                                                                 a. Determine the dimension of U1, U2                        
                                                                                                                                                                                                        
7 −5 2                                                                1 0 1                                                                 b. Determine bases of U1 and U2                             
                                                                                                                                                                                                        
3 −1 2                                                                                                                                     c. Determine a basis of U1 ∩ U2                             
                                                                                                                                                                                                        
                                                                                                                                          2.15 Let F = {(x, y, z) ∈ R3                                
                                                                                                                                                                                                        
                                                                                                                                          | x+y−z = 0} and G = {(a−b, a+b, a−3b) | a, b ∈ R}.         
                                                                                                                                                                                                        
                                                                                                                                          a. Show that F and G are subspaces of R3                    
                                                                                                                                                                                                        
                                                                     , A2 =                                                                .                                                           
                                                                                                                                                                                                        
.                                                                                                                                          b. Calculate F ∩ G without resorting to any basis vector.   
                                                                                                                                                                                                        
a. Determine the dimension of U1, U2.                                                                                                      c. Find one basis for F and one for G, calculate  F∩G  using
                                                                                                                                            the basis vectors                                           
b. Determine bases of U1 and U2.                                                                                                                                                                       
                                                                                                                                            previously found and check your  result  with  the  previous
c. Determine a basis of U1 ∩ U2.                                                                                                           question.                                                   
                                                                                                                                                                                                        
2.14 Consider two subspaces U1 and U2, where U1  is  spanned          3 −3 0                                                                2.16 Are the following mappings linear?                     
by the columns of                                                                                                                                                                                       
                                                                      1 2 3                                                                 a. Let a, b ∈ R.                                            
A1 and U2 is spanned by the columns of A2 with                                                                                                                                                          
                                                                      7 −5 2                                                                Φ : L                                                       
A1 =                                                                                                                                                                                                    
                                                                      3 −1 2                                                                1                                                           
                                                                                                                                                                                                       
                                                                                                                                           ([a, b]) → R                                                
                                                                                                                                                                                                       
                                                                                                                                           f 7→ Φ(f) = Z b                                             
                                                                                                                                                                                                       
                                                                                                                                           a                                                           
                                                                                                                                                                                                       
                                                                                                                                           f(x)dx ,                                                    
1                            0                             1                                                                                                                                            

%%%

where L                                                               Φ : R                                                                 3 → R                                                       
                                                                                                                                                                                                        
1                                                                     3 → R                                                                 4                                                           
                                                                                                                                                                                                        
([a, b]) denotes the set of integrable functions on [a, b].           2                                                                     Φ                                                           
                                                                                                                                                                                                        
b.                                                                    x 7→                                                                                                                             
                                                                                                                                                                                                        
Φ : C                                                                                                                                                                                                 
                                                                                                                                                                                                        
1 → C                                                                 1 2 3                                                                                                                            
                                                                                                                                                                                                        
0                                                                     1 4 3                                                                                                                           
                                                                                                                                                                                                        
f 7→ Φ(f) = f                                                         x                                                                     x1                                                          
                                                                                                                                                                                                        
′                                                                     e. Let θ be in [0, 2π[ and                                            x2                                                          
                                                                                                                                                                                                        
,                                                                     Φ : R                                                                 x3                                                          
                                                                                                                                                                                                        
where for k ⩾ 1, C                                                    2 → R                                                                                                                            
                                                                                                                                                                                                        
k denotes the set of  k  times  continuously  differentiable          2                                                                                                                                
functions, and C                                                                                                                                                                                        
                                                                      x 7→                                                                                                                             
0 denotes the set of continuous functions.                                                                                                                                                              
                                                                                                                                            =                                                         
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                                                                            
by Cambridge University Press (2020).                                 cos(θ) sin(θ)                                                                                                                    
                                                                                                                                                                                                        
68 Linear Algebra                                                     − sin(θ) cos(θ)                                                                                                                  
                                                                                                                                                                                                        
c.                                                                                                                                                                                                    
                                                                                                                                                                                                        
Φ : R → R                                                             x                                                                                                                                
                                                                                                                                                                                                        
x 7→ Φ(x) = cos(x)                                                    2.17 Consider the linear mapping                                      3x1 + 2x2 + x3                                              
                                                                                                                                                                                                        
d.                                                                    Φ : R                                                                 x1 + x2 + x3                                                
                                                                                                                                                                                                        

%%%

x1 − 3x2                                                              1 1 0                                                                                                                            
                                                                                                                                                                                                        
2x1 + 3x2 + x3                                                        1 −1 0                                                                ,                                                          
                                                                                                                                                                                                        
                                                                     1 1 1                                                                                                                            
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                      .                                                                   1                                                           
                                                                                                                                                                                                        
                                                                     a. Determine ker(Φ) and Im(Φ).                                        0                                                           
                                                                                                                                                                                                        
Find the transformation matrix AΦ.                                    b. Determine the transformation matrix A˜ Φ with respect  to          0                                                           
                                                                      the basis                                                                                                                         
Determine rk(AΦ).                                                                                                                                                                                      
                                                                      B = (                                                                                                                             
Compute the kernel and image of Φ. What are dim(ker(Φ))  and                                                                                ),                                                         
dim(Im(Φ))?                                                                                                                                                                                            
                                                                                                                                            i.e., perform a basis change toward the new basis B.        
2.18  Let  E  be  a  vector  space.  Let  f  and  g  be  two                                                                                                                                           
automorphisms on E such that                                                                                                                2.20 Let us consider b1, b2, b                              
                                                                      1                                                                                                                                 
f ◦ g = idE (i.e., f ◦ g is the identity mapping idE).  Show                                                                                ′                                                           
that ker(f) =                                                         1                                                                                                                                 
                                                                                                                                            1                                                           
ker(g ◦ f), Im(g) = Im(g ◦ f) and  that  ker(f)  ∩  Im(g)  =          1                                                                                                                                 
{0E}.                                                                                                                                       , b                                                         
                                                                                                                                                                                                       
2.19  Consider  an  endomorphism  Φ  :   R3   →   R3   whose                                                                                ′                                                           
transformation matrix                                                 ,                                                                                                                                
                                                                                                                                            2                                                           
(with respect to the standard basis in R3                                                                                                                                                              
                                                                                                                                            , 4 vectors of R2                                           
) is                                                                                                                                                                                                   
                                                                                                                                            expressed in the standard basis                             
AΦ =                                                                  1                                                                                                                                 
                                                                                                                                            of R2                                                       
                                                                     2                                                                                                                                 
                                                                                                                                            as                                                          
                                                                     1                                                                                                                                 

%%%

b1 =                                                                                                                                       B                                                           
                                                                                                                                                                                                        
                                                                     1                                                                     ′                                                           
                                                                                                                                                                                                        
2                                                                     1                                                                     to B.                                                       
                                                                                                                                                                                                        
1                                                                                                                                          c. We consider c1, c2, c3, three vectors of  R3  defined  in
                                                                                                                                            the standard basis                                          
                                                                     and let us define two ordered bases B = (b1, b2) and B                                                                            
                                                                                                                                            of R3                                                       
, b2 =                                                                ′ = (b                                                                                                                            
                                                                                                                                            as                                                          
                                                                     ′                                                                                                                                 
                                                                                                                                            c1 =                                                        
−1                                                                    1                                                                                                                                 
                                                                                                                                                                                                       
−1                                                                    , b                                                                                                                               
                                                                                                                                                                                                       
                                                                     ′                                                                                                                                 
                                                                                                                                            1                                                           
, b                                                                   2                                                                                                                                 
                                                                                                                                            2                                                           
′                                                                     ) of R2                                                                                                                           
                                                                                                                                            −1                                                          
1 =                                                                   .                                                                                                                                 
                                                                                                                                                                                                       
                                                                     Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       , c2 =                                                     
2                                                                                                                                                                                                       
                                                                      Exercises 69                                                                                                                     
−2                                                                                                                                                                                                      
                                                                      a. Show that B and B                                                                                                             
                                                                                                                                                                                                       
                                                                      ′                                                                     0                                                           
, b                                                                                                                                                                                                     
                                                                      are two bases of R2                                                   −1                                                          
′                                                                                                                                                                                                       
                                                                      and draw those basis vectors.                                         2                                                           
2 =                                                                                                                                                                                                     
                                                                      b. Compute the matrix P 1 that performs a basis change  from                                                                     

%%%

, c3 =                                                               ′                                                                     and C                                                       
                                                                                                                                                                                                        
                                                                     3                                                                     ′                                                           
                                                                                                                                                                                                        
                                                                     ) the standard basis of R3                                            .                                                           
                                                                                                                                                                                                        
1                                                                     . Determine                                                           f. Let us consider the vector x ∈ R2 whose coordinates in B 
                                                                                                                                                                                                        
0                                                                     the matrix P 2 that performs the basis change from C to C             ′                                                           
                                                                                                                                                                                                        
−1                                                                    ′                                                                     are [2, 3]⊤.                                                
                                                                                                                                                                                                        
                                                                     .                                                                     In other words, x = 2b                                      
                                                                                                                                                                                                        
                                                                     d. We consider a homomorphism Φ : R2 −→ R3                            ′                                                           
                                                                                                                                                                                                        
and we define C = (c1, c2, c3).                                       , such that                                                           1 + 3b                                                      
                                                                                                                                                                                                        
(i) Show that C is a basis of R3                                      Φ(b1 + b2) = c2 + c3                                                  ′                                                           
                                                                                                                                                                                                        
, e.g., by using determinants (see                                    Φ(b1 − b2) = 2c1 − c2 + 3c3                                           2                                                           
                                                                                                                                                                                                        
Section 4.1).                                                         where B = (b1, b2) and C = (c1, c2, c3) are ordered bases of          .                                                           
                                                                      R2                                                                                                                                
(ii) Let us call C                                                                                                                          (i) Calculate the coordinates of x in B.                    
                                                                      and R3                                                                                                                            
′ = (c                                                                                                                                      (ii)  Based  on  that,  compute  the  coordinates  of   Φ(x)
                                                                      ,                                                                     expressed in C.                                             
′                                                                                                                                                                                                       
                                                                      respectively.                                                         (iii) Then, write Φ(x) in terms of c                        
1                                                                                                                                                                                                       
                                                                      Determine the transformation matrix AΦ of Φ with respect  to          ′                                                           
, c                                                                   the ordered bases B and C.                                                                                                        
                                                                                                                                            1                                                           
′                                                                     e. Determine A′                                                                                                                   
                                                                                                                                            , c                                                         
2                                                                     , the transformation matrix of Φ with respect to the bases                                                                        
                                                                                                                                            ′                                                           
, c                                                                   B′                                                                                                                                
                                                                                                                                            2                                                           

%%%

, c                                                                   metrics capture                                                       projection Angles Rotations                                 
                                                                                                                                                                                                        
′                                                                     the intuitive notions of similarity and distances, which  we          Chapter 4                                                   
                                                                      use to develop                                                                                                                    
3                                                                                                                                           Matrix                                                      
                                                                      the support vector machine in Chapter 12. We will  then  use                                                                      
.                                                                     the concepts                                                          decomposition                                               
                                                                                                                                                                                                        
(iv) Use the representation of x in B                                 of lengths and angles between vectors to discuss  orthogonal          Chapter 10                                                  
                                                                      projections,                                                                                                                      
′                                                                                                                                           Dimensionality                                              
                                                                      which will play a central role  when  we  discuss  principal                                                                      
and the matrix A′                                                     component analysis in Chapter 10 and regression via  maximum          reduction                                                   
                                                                      likelihood estimation in                                                                                                          
to find this                                                                                                                                Chapter 9                                                   
                                                                      Chapter 9. Figure 3.1 gives an overview of how  concepts  in                                                                      
result directly.                                                      this chapter                                                          Regression                                                  
                                                                                                                                                                                                        
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          are related and how they are connected to other chapters  of          Chapter 12                                                  
by Cambridge University Press (2020).                                 the book.                                                                                                                         
                                                                                                                                            Classification                                              
3                                                                     Figure 3.1 A mind                                                                                                                 
                                                                                                                                            induces                                                     
Analytic Geometry                                                     map of the concepts                                                                                                               
                                                                                                                                            70                                                          
In Chapter 2, we studied vectors, vector spaces, and  linear          introduced in this                                                                                                                
mappings at                                                                                                                                 This material is published by Cambridge University Press  as
                                                                      chapter, along with                                                   Mathematics for Machine Learning by                         
a general but abstract level. In this chapter, we  will  add                                                                                                                                            
some geometric interpretation and intuition to all of  these          when they are used                                                    Marc Peter Deisenroth, A. Aldo Faisal, and  Cheng  Soon  Ong
concepts. In particular, we                                                                                                                 (2020). This version is free to view                        
                                                                      in other parts of the                                                                                                             
will look at geometric vectors and compute their lengths and                                                                                and download for personal use only. Not for re-distribution,
distances                                                             book.                                                                 re-sale, or use in derivative works.                        
                                                                                                                                                                                                        
or angles between two vectors. To be able  to  do  this,  we          Inner product                                                         ©by M. P. Deisenroth, A. A. Faisal, and  C.  S.  Ong,  2021.
equip the vector space with an inner  product  that  induces                                                                                https://mml-book.com.                                       
the geometry of the vector                                            Norm                                                                                                                              
                                                                                                                                            3.1 Norms 71                                                
space. Inner products  and  their  corresponding  norms  and          Lengths                                           Orthogonal                                                                      

%%%

Figure 3.1 For                                                        ∥ · ∥ : V → R , (3.1)                                                 .                                                           
                                                                                                                                                                                                        
different norms, the                                                  x 7→ ∥x∥ , (3.2)                                                      Recall that for a vector x ∈ Rn we denote  the  elements  of
                                                                                                                                            the vector using                                            
red lines indicate                                                    which assigns each vector x its length ∥x∥ ∈  R,  such  that                                                                      
                                                                      for all λ ∈ R length                                                  a subscript, that is, xi                                    
the set of vectors                                                                                                                                                                                      
                                                                      and x, y ∈ V the following hold:                                      is the i                                                    
with norm 1. Left:                                                                                                                                                                                      
                                                                      absolutely                                                            th element of the vector x.                                 
Manhattan norm;                                                                                                                                                                                         
                                                                      Absolutely homogeneous: ∥λx∥ = homogeneous |λ|∥x∥                     Example 3.1 (Manhattan Norm)                                
Right: Euclidean                                                                                                                                                                                        
                                                                      Triangle inequality: ∥x + y∥ ⩽ ∥x∥ + ∥y∥ triangle inequality          The Manhattan norm on Rn                                    
distance.                                                                                                                                                                                               
                                                                      Positive definite: positive definite ∥x∥ ⩾ 0 and ∥x∥ = 0  ⇐⇒          is defined for x ∈ Rn as Manhattan norm                     
1                                                                     x = 0                                                                                                                             
                                                                                                                                            ∥x∥1 := Xn                                                  
1 1                                                                   Figure 3.2 Triangle                                                                                                               
                                                                                                                                            i=1                                                         
1                                                                     inequality.                                                                                                                       
                                                                                                                                            |xi                                                         
kxk1 = 1 kxk2 = 1                                                     a b                                                                                                                               
                                                                                                                                            | , (3.3)                                                   
3.1 Norms                                                             c ≤ a + b                                                                                                                         
                                                                                                                                            where | · | is the absolute value. The left panel of  Figure
When we think of  geometric  vectors,  i.e.,  directed  line          In geometric terms, the triangle inequality states that  for          3.1 shows all                                               
segments that start                                                   any triangle,                                                                                                                     
                                                                                                                                            vectors x ∈ R2 with ∥x∥1 = 1. The  Manhattan  norm  is  also
at the origin, then intuitively the length of  a  vector  is          the sum of the lengths of any two sides must be greater than          called ℓ1 ℓ1 norm                                           
the distance of the                                                   or equal                                                                                                                          
                                                                                                                                            norm.                                                       
“end” of this directed line segment from the origin. In  the          to the length of the remaining side; see Figure 3.2  for  an                                                                      
following, we                                                         illustration.                                                         ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                                                                                            by Cambridge University Press (2020).                       
will discuss the notion of the length of vectors  using  the          Definition 3.1 is in terms  of  a  general  vector  space  V                                                                      
concept of a norm.                                                    (Section 2.4), but                                                    72 Analytic Geometry                                        
                                                                                                                                                                                                        
Definition 3.1 (Norm). A norm on  a  vector  space  V  is  a          in this book we  will  only  consider  a  finite-dimensional          Example 3.2 (Euclidean Norm)                                
function                                                norm          vector                       space                        Rn                                                                      

%%%

The Euclidean norm of x ∈ Rn Euclidean norm is defined as             vectors are orthogonal to each other.                                 mapping with two arguments, and it is linear in             
                                                                                                                                                                                                        
∥x∥2 :=                                                               3.2.1 Dot Product                                                     each argument, i.e., when we look at a vector space  V  then
                                                                                                                                            it holds that                                               
vuutXn                                                                We may already be familiar with a particular type  of  inner                                                                      
                                                                      product, the                                                          for all x, y, z ∈ V, λ, ψ ∈ R that                          
i=1                                                                                                                                                                                                     
                                                                      scalar product/dot product in Rn                                      Ω(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z) (3.6)                   
x                                                                                                                                                                                                       
                                                                      scalar product , which is given by                                    Ω(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z). (3.7)                  
2                                                                                                                                                                                                       
                                                                      dot product                                                           Here, (3.6) asserts that Ω is linear in the first  argument,
i =                                                                                                                                         and (3.7) asserts                                           
                                                                      x                                                                                                                                 
√                                                                                                                                           that Ω is linear in the second argument (see also (2.87)).  
                                                                      ⊤y =                                                                                                                              
x⊤x (3.4)                                                                                                                                   Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                      Xn                                                                    Feedback: https://mml-book.com.                             
Euclidean distance and computes the Euclidean distance of  x                                                                                                                                            
from the origin. The right panel                                      i=1                                                                   3.2 Inner Products 73                                       
                                                                                                                                                                                                        
of Figure 3.1 shows all vectors x ∈ R2 with ∥x∥2  =  1.  The          xiyi                                                                  Definition 3.2. Let V be a vector space and Ω : V × V → R be
Euclidean                                                                                                                                   a bilinear                                                  
                                                                      . (3.5)                                                                                                                           
ℓ2 norm norm is also called ℓ2 norm.                                                                                                        mapping that takes two vectors and maps  them  onto  a  real
                                                                      We will refer to this particular inner product  as  the  dot          number. Then                                                
Remark. Throughout this book, we will use the Euclidean norm          product in this                                                                                                                   
(3.4) by                                                                                                                                    Ω is called symmetric if Ω(x, y) = Ω(y, x) for all x, y ∈  V
                                                                      book. However, inner products are more general concepts with          , i.e., the symmetric                                       
default if not stated otherwise. ♢                                    specific                                                                                                                          
                                                                                                                                            order of the arguments does not matter.                     
3.2 Inner Products                                                    properties, which we will now introduce.                                                                                          
                                                                                                                                            Ω is called positive definite if positive definite          
Inner products  allow  for  the  introduction  of  intuitive          3.2.2 General Inner Products                                                                                                      
geometrical concepts, such as the length of a vector and the                                                                                ∀x ∈ V \{0} : Ω(x, x) > 0 , Ω(0, 0) = 0 . (3.8)             
angle or distance between                                             Recall the linear mapping from Section  2.7,  where  we  can                                                                      
                                                                      rearrange the                                                         Definition 3.3. Let V be a vector space and Ω : V × V → R be
two vectors.  A  major  purpose  of  inner  products  is  to                                                                                a bilinear                                                  
determine whether                                                     bilinear  mapping  mapping  with  respect  to  addition  and                                                                      
                                                                      multiplication with a scalar. A  bilinear  mapping  Ω  is  a          mapping that takes two vectors and maps  them  onto  a  real

%%%

number. Then                                                          Symmetric, positive definite matrices play an important role          *Xn                                                         
                                                                      in machine                                                                                                                        
A positive definite, symmetric bilinear mapping Ω : V ×V → R                                                                                i=1                                                         
is called                                                             learning, and they are defined via  the  inner  product.  In                                                                      
                                                                      Section 4.3, we                                                       ψibi                                                        
an inner product on V . We typically write ⟨x, y⟩ instead of                                                                                                                                            
Ω(x, y). inner product                                                will return to symmetric, positive definite matrices in  the          ,                                                           
                                                                      context of matrix                                                                                                                 
The pair (V,⟨·, ·⟩) is called  an  inner  product  space  or                                                                                Xn                                                          
(real) vector space inner product space                               decompositions. The idea of symmetric positive  semidefinite                                                                      
                                                                      matrices is                                                           j=1                                                         
vector space with                                                                                                                                                                                       
                                                                      key in the definition of kernels (Section 12.4).                      λjbj                                                        
inner product                                                                                                                                                                                           
                                                                      Consider an n-dimensional  vector  space  V  with  an  inner          +                                                           
with inner product. If we use the  dot  product  defined  in          product ⟨·, ·⟩ :                                                                                                                  
(3.5), we call                                                                                                                              =                                                           
                                                                      V × V → R (see Definition 3.3) and an ordered basis B = (b1,                                                                      
(V,⟨·, ·⟩) a Euclidean vector space.                                  . . . , bn) of                                                        Xn                                                          
                                                                                                                                                                                                        
Euclidean vector                                                      V . Recall from Section 2.6.1 that any vectors x, y ∈ V  can          i=1                                                         
                                                                      be written as                                                                                                                     
We will refer to these spaces as  inner  product  spaces  in                                                                                Xn                                                          
this book. space                                                      linear combinations of the basis vectors so that x =                                                                              
                                                                                                                                            j=1                                                         
Example 3.3 (Inner Product That Is Not the Dot Product)               Pn                                                                                                                                
                                                                                                                                            ψi ⟨bi                                                      
Consider V = R2                                                       i=1 ψibi ∈ V and                                                                                                                  
                                                                                                                                            , bj ⟩ λj = xˆ                                              
. If we define                                                        y =                                                                                                                               
                                                                                                                                            ⊤Ayˆ , (3.10)                                               
⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2x2y2 (3.9)                          Pn                                                                                                                                
                                                                                                                                            where Aij := ⟨bi                                            
then ⟨·, ·⟩ is an inner product but different from  the  dot          j=1 λjbj ∈ V for suitable ψi                                                                                                      
product. The proof                                                                                                                          , bj ⟩ and xˆ, yˆ are  the  coordinates  of  x  and  y  with
                                                                      , λj ∈ R. Due to the bilinearity of the                               respect                                                     
will be an exercise.                                                                                                                                                                                    
                                                                      inner product, it holds for all x, y ∈ V that                         to the basis B. This implies that the inner product  ⟨·,  ·⟩
3.2.3 Symmetric, Positive Definite Matrices                                                                                                 is uniquely determined through A. The symmetry of the  inner
                                                                      ⟨x,                           y⟩                           =          product         also          means          that          A

%%%

©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          9 6                                                                   2                                                           
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      6 5                                                                  2 = (3x1 + 2x2)                                             
74 Analytic Geometry                                                                                                                                                                                    
                                                                      , A2 =                                                                2 + x                                                       
is symmetric. Furthermore, the positive definiteness of  the                                                                                                                                            
inner product                                                                                                                              2                                                           
                                                                                                                                                                                                        
implies that                                                          9 6                                                                   2 > 0 (3.13b)                                               
                                                                                                                                                                                                        
∀x ∈ V \{0} : x                                                       6 3                                                                  for all x ∈ V \{0}. In contrast, A2  is  symmetric  but  not
                                                                                                                                            positive definite                                           
⊤Ax > 0 . (3.11)                                                      . (3.12)                                                                                                                          
                                                                                                                                            because x                                                   
Definition 3.4  (Symmetric,  Positive  Definite  Matrix).  A          A1 is positive definite because it is symmetric and                                                                               
symmetric matrix                                                                                                                            ⊤A2x = 9x                                                   
                                                                      x                                                                                                                                 
A ∈ Rn×n                                                                                                                                    2                                                           
                                                                      ⊤A1x =                                                                                                                            
symmetric,  positive  that  satisfies   (3.11)   is   called                                                                                1 + 12x1x2 + 3x                                             
symmetric, positive definite, or                                      x1 x2                                                                                                                             
                                                                                                                                            2                                                           
definite just positive definite. If only ⩾ holds in  (3.11),                                                                                                                                           
then A is called symmetric,                                                                                                                 2 = (3x1 + 2x2)                                             
                                                                      9 6                                                                                                                               
positive definite                                                                                                                           2 − x                                                       
                                                                      6 5 x1                                                                                                                          
symmetric, positive                                                                                                                         2                                                           
                                                                      x2                                                                                                                                
semidefinite                                                                                                                                2                                                           
                                                                                                                                                                                                       
positive semidefinite.                                                                                                                      can be less                                                 
                                                                      (3.13a)                                                                                                                           
Example 3.4 (Symmetric, Positive Definite Matrices)                                                                                         than 0, e.g., for x = [2, −3]⊤.                             
                                                                      = 9x                                                                                                                              
Consider the matrices                                                                                                                       If A ∈ Rn×n                                                 
                                                                      2                                                                                                                                 
A1 =                                                                                                                                        is symmetric, positive definite, then                       
                                                                      1 + 12x1x2 + 5x                                                                                                                   
                                                                                                                                           ⟨x,                 y⟩                 =                  xˆ

%%%

⊤Ayˆ (3.14)                                                           is the ith vector of the standard basis in Rn                         (V,⟨·,  ·⟩)  the  induced  norm  ∥   ·   ∥   satisfies   the
                                                                                                                                            Cauchy-Schwarz inequality Cauchy-Schwarz                    
defines an inner product with respect to an ordered basis B,          .                                                                                                                                 
where xˆ and                                                                                                                                inequality                                                  
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
yˆ are the coordinate representations  of  x,  y  ∈  V  with          Feedback: https://mml-book.com.                                       | ⟨x, y⟩ | ⩽ ∥x∥∥y∥ . (3.17)                                
respect to B.                                                                                                                                                                                           
                                                                      3.3 Lengths and Distances 75                                          ♢                                                           
Theorem 3.5. For a  real-valued,  finite-dimensional  vector                                                                                                                                            
space V and an                                                        3.3 Lengths and Distances                                             Example 3.5 (Lengths of Vectors Using Inner Products)       
                                                                                                                                                                                                        
ordered basis B of V , it holds that ⟨·, ·⟩ : V × V →  R  is          In Section 3.1, we already discussed norms that we  can  use          In geometry, we are often interested in lengths of  vectors.
an inner product if                                                   to compute                                                            We can now use                                              
                                                                                                                                                                                                        
and only if there  exists  a  symmetric,  positive  definite          the length of a vector. Inner products and norms are closely          an inner product to compute them using (3.16). Let us take x
matrix A ∈ Rn×n with                                                  related in the                                                        = [1, 1]⊤ ∈                                                 
                                                                                                                                                                                                        
⟨x, y⟩ = xˆ                                                           sense that any inner product induces a norm Inner products            R2                                                          
                                                                                                                                                                                                        
⊤Ayˆ . (3.15)                                                         induce norms.                                                         . If we use the dot  product  as  the  inner  product,  with
                                                                                                                                            (3.16) we obtain                                            
The following properties hold if A ∈ Rn×n                             ∥x∥ := q                                                                                                                          
                                                                                                                                            ∥x∥ =                                                       
is symmetric and positive                                             ⟨x, x⟩ (3.16)                                                                                                                     
                                                                                                                                            √                                                           
definite:                                                             in a natural way,  such  that  we  can  compute  lengths  of                                                                      
                                                                      vectors using the inner product. However, not every norm  is          x⊤x =                                                       
The null space (kernel) of A consists only of 0 because x             induced by an inner product. The                                                                                                  
                                                                                                                                            √                                                           
⊤Ax > 0 for                                                           Manhattan norm (3.3) is an  example  of  a  norm  without  a                                                                      
                                                                      corresponding                                                         1                                                           
all x ̸= 0. This implies that Ax ̸= 0 if x ̸= 0.                                                                                                                                                        
                                                                      inner product. In the following, we will focus on norms that          2 + 12 =                                                    
The diagonal elements aii of A are positive because aii = e           are induced                                                                                                                       
                                                                                                                                            √                                                           
⊤                                                                     by inner products and introduce geometric concepts, such  as                                                                      
                                                                      lengths, distances, and angles.                                       2 (3.18)                                                    
i Aei > 0,                                                                                                                                                                                              
                                                                      Remark (Cauchy-Schwarz Inequality).  For  an  inner  product          as the length of x. Let us  now  choose  a  different  inner
where                                                     ei          vector                                                 space          product:                                                    

%%%

⟨x, y⟩ := x                                                           ⟨x, x⟩ = x                                                            d : V × V → R (3.22)                                        
                                                                                                                                                                                                        
⊤                                                                     2                                                                     (x, y) 7→ d(x, y) (3.23)                                    
                                                                                                                                                                                                        
                                                                     1 − x1x2 + x                                                          metric is called a metric.                                  
                                                                                                                                                                                                        
1 −                                                                   2                                                                     Remark. Similar to the length  of  a  vector,  the  distance
                                                                                                                                            between vectors                                             
1                                                                     2 = 1 − 1 + 1 = 1 =⇒ ∥x∥ =                                                                                                        
                                                                                                                                            does not require an inner product: a norm is sufficient.  If
2                                                                     √                                                                     we have a norm                                              
                                                                                                                                                                                                        
−                                                                     1 = 1 , (3.20)                                                        induced by an inner product, the distance may vary depending
                                                                                                                                            on the                                                      
1                                                                     such that x is “shorter” with this inner product  than  with                                                                      
                                                                      the dot product.                                                      choice of the inner product. ♢                              
2                                                                                                                                                                                                       
                                                                      Definition 3.6 (Distance  and  Metric).  Consider  an  inner          A metric d satisfies the following:                         
1                                                                     product space                                                                                                                     
                                                                                                                                            positive definite 1. d is positive definite, i.e., d(x, y) ⩾
                                                                     (V,⟨·, ·⟩). Then                                                      0 for all x, y ∈ V and d(x, y) =                            
                                                                                                                                                                                                        
y = x1y1 −                                                            d(x, y) := ∥x − y∥ =                                                  0 ⇐⇒ x = y .                                                
                                                                                                                                                                                                        
1                                                                     q                                                                     symmetric 2. d is symmetric, i.e., d(x, y) = d(y, x) for all
                                                                                                                                            x, y ∈ V .                                                  
2                                                                     ⟨x − y, x − y⟩ (3.21)                                                                                                             
                                                                                                                                            triangle inequality 3. Triangle inequality: d(x, z)  ⩽  d(x,
(x1y2 + x2y1) + x2y2 . (3.19)                                         is called the distance between x and y for x, y ∈ V . If  we          y) + d(y, z) for all x, y, z ∈ V .                          
                                                                      use the dot distance                                                                                                              
If we compute the norm of a vector, then this inner  product                                                                                Remark. At first glance, the lists of  properties  of  inner
returns smaller                                                       product as the inner product, then the  distance  is  called          products  and  metrics  look  very  similar.   However,   by
                                                                      Euclidean distance. Euclidean distance                                comparing Definition 3.3 with Definition 3.6 we observe that
values than the dot product if x1 and x2 have the same  sign                                                                                ⟨x, y⟩ and d(x, y) behave in opposite directions.           
(and x1x2 >                                                           ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                      
                                                                      by Cambridge University Press (2020).                                 Very similar x and y will result in a large  value  for  the
0); otherwise,  it  returns  greater  values  than  the  dot                                                                                inner product and                                           
product. With this                                                    76 Analytic Geometry                                                                                                              
                                                                                                                                            a small value for the metric. ♢                             
inner             product,             we             obtain          The                                                  mapping                                                                      

%%%

3.4 Angles and Orthogonality                                          inner product spaces between two  vectors  x,  y,  and  this          3.4 Angles and Orthogonality 77                             
                                                                      notion coincides with our                                                                                                         
Figure 3.2 When                                                                                                                             Example 3.6 (Angle between Vectors)                         
                                                                      intuition in R2 and R3                                                                                                            
restricted to [0, π]                                                                                                                        Let us compute the angle between x = [1, 1]⊤ ∈ R2  and  y  =
                                                                      . Assume that x ̸= 0, y ̸= 0. Then                                    [1, 2]⊤ ∈ R2                                                
then f(ω) = cos(ω)                                                                                                                                                                                      
                                                                      −1 ⩽                                                                  ; Figure 3.3 The                                            
returns a unique                                                                                                                                                                                        
                                                                      ⟨x, y⟩                                                                angle ω between                                             
number in the                                                                                                                                                                                           
                                                                      ∥x∥ ∥y∥                                                               two vectors x, y is                                         
interval [−1, 1].                                                                                                                                                                                       
                                                                      ⩽ 1 . (3.24)                                                          computed using the                                          
0 π/2 π                                                                                                                                                                                                 
                                                                      Therefore, there exists a unique ω ∈ [0, π], illustrated  in          inner product.                                              
ω                                                                     Figure 3.2, with                                                                                                                  
                                                                                                                                            y                                                           
−1                                                                    cos ω =                                                                                                                           
                                                                                                                                            x                                                           
0                                                                     ⟨x, y⟩                                                                                                                            
                                                                                                                                            0 1                                                         
1                                                                     ∥x∥ ∥y∥                                                                                                                           
                                                                                                                                            1                                                           
cos(                                                                  . (3.25)                                                                                                                          
                                                                                                                                            ω                                                           
ω                                                                     angle The number ω is the angle between the vectors x and y.                                                                      
                                                                      Intuitively, the                                                      see Figure 3.3, where we use the dot product  as  the  inner
)                                                                                                                                           product. Then                                               
                                                                      angle  between  two  vectors  tells  us  how  similar  their                                                                      
In  addition  to  enabling  the  definition  of  lengths  of          orientations are. For                                                 we get                                                      
vectors, as well as the                                                                                                                                                                                 
                                                                      example, using the dot product, the angle between x and y  =          cos ω =                                                     
distance between two vectors, inner  products  also  capture          4x, i.e., y                                                                                                                       
the geometry                                                                                                                                ⟨x, y⟩                                                      
                                                                      is a scaled version of x, is 0:  Their  orientation  is  the                                                                      
of a vector space  by  defining  the  angle  ω  between  two          same.                                                                 p                                                           
vectors. We use                                                                                                                                                                                         
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          ⟨x, x⟩ ⟨y, y⟩                                               
the Cauchy-Schwarz inequality (3.17) to define angles  ω  in          Feedback:                              https://mml-book.com.                                                                      

%%%

=                                                                     i.e., the vectors  are  unit  vectors,  then  x  and  y  are          ; see Figure 3.1.                                           
                                                                      orthonormal. orthonormal                                                                                                          
x                                                                                                                                           We are interested in determining the angle  ω  between  them
                                                                      An implication of this definition is that  the  0-vector  is          using two                                                   
⊤y                                                                    orthogonal to                                                                                                                     
                                                                                                                                            different inner products. Using the dot product as the inner
p                                                                     every vector in the vector space.                                     product yields                                              
                                                                                                                                                                                                        
x⊤xy⊤y                                                                Remark. Orthogonality is the generalization of  the  concept          an angle ω between x and y of 90◦                           
                                                                      of perpendicularity to bilinear forms that do not have to be                                                                      
=                                                                     the dot product. In our                                               , such that x ⊥ y. However, if we                           
                                                                                                                                                                                                        
3                                                                     context, geometrically, we can think of  orthogonal  vectors          choose the inner product                                    
                                                                      as having a                                                                                                                       
√                                                                                                                                           ⟨x, y⟩ = x                                                  
                                                                      right angle with respect to a specific inner product. ♢                                                                           
10                                                                                                                                          ⊤                                                           
                                                                      Example 3.7 (Orthogonal Vectors)                                                                                                  
, (3.26)                                                                                                                                                                                               
                                                                      Figure 3.1 The                                                                                                                    
and the angle between the two vectors is arccos( √                                                                                          2 0                                                         
                                                                      angle ω between                                                                                                                   
3                                                                                                                                           0 1                                                        
                                                                      two vectors x, y can                                                                                                              
10 ) ≈ 0.32 rad, which                                                                                                                      y , (3.27)                                                  
                                                                      change depending                                                                                                                  
corresponds to about 18◦                                                                                                                    ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
                                                                      on the inner                                                          by Cambridge University Press (2020).                       
.                                                                                                                                                                                                       
                                                                      product.                                                              78 Analytic Geometry                                        
A key feature of the inner product is that it also allows us                                                                                                                                            
to characterize                                                       y x                                                                   we get that the angle ω between x and y is given by         
                                                                                                                                                                                                        
vectors that are orthogonal.                                          −1 0 1                                                                cos ω =                                                     
                                                                                                                                                                                                        
Definition 3.7 (Orthogonality). Two  vectors  x  and  y  are          1                                                                     ⟨x, y⟩                                                      
orthogonal if and orthogonal                                                                                                                                                                            
                                                                      ω                                                                     ∥x∥∥y∥                                                      
only if ⟨x, y⟩ = 0, and we write x ⊥ y. If additionally  ∥x∥                                                                                                                                            
=                 1                  =                  ∥y∥,          Consider two vectors x  =  [1,  1]⊤,  y  =  [−1,  1]⊤  ∈  R2          =                                                          −

%%%

1                                                                     call these matrices                                                   ⊤x = ∥x∥                                                    
                                                                                                                                                                                                        
3                                                                     “orthogonal” but a                                                    2                                                           
                                                                                                                                                                                                        
=⇒ ω ≈ 1.91 rad ≈ 109.5                                               more precise                                                          . (3.31)                                                    
                                                                                                                                                                                                        
◦                                                                     description would                                                     Moreover, the  angle  between  any  two  vectors  x,  y,  as
                                                                                                                                            measured by their                                           
, (3.28)                                                              be “orthonormal”.                                                                                                                 
                                                                                                                                            inner product, is also unchanged when transforming  both  of
and x and y are not orthogonal. Therefore, vectors that  are          Transformations by orthogonal matrices are  special  because          them using                                                  
orthogonal                                                            the length                                                                                                                        
                                                                                                                                            an orthogonal matrix A. Assuming  the  dot  product  as  the
with respect  to  one  inner  product  do  not  have  to  be          of a vector x is not changed when transforming it  using  an          inner product,                                              
orthogonal with respect to a different inner product.                 orthogonal                                                                                                                        
                                                                                                                                            the angle of the images Ax and Ay is given as               
Definition 3.8 (Orthogonal Matrix). A square matrix A ∈ Rn×n          matrix A. For the dot product, we obtain                                                                                          
                                                                                                                                            cos ω =                                                     
is an                                                                 Transformations                                                                                                                   
                                                                                                                                            (Ax)                                                        
orthogonal matrix orthogonal  matrix  if  and  only  if  its          with orthogonal                                                                                                                   
columns are orthonormal so that                                                                                                             ⊤(Ay)                                                       
                                                                      matrices preserve                                                                                                                 
AA⊤ = I = A                                                                                                                                 ∥Ax∥ ∥Ay∥                                                   
                                                                      distances and                                                                                                                     
⊤A , (3.29)                                                                                                                                 =                                                           
                                                                      angles.                                                                                                                           
which implies that                                                                                                                          x                                                           
                                                                      ∥Ax∥                                                                                                                              
A                                                                                                                                           ⊤A                                                          
                                                                      2 = (Ax)                                                                                                                          
−1 = A                                                                                                                                      ⊤Ay                                                         
                                                                      ⊤(Ax) = x                                                                                                                         
⊤                                                                                                                                           q                                                           
                                                                      ⊤A                                                                                                                                
, (3.30)                                                                                                                                    x⊤A                                                         
                                                                      ⊤Ax = x                                                                                                                           
It is convention to i.e., the inverse is obtained by  simply                                                                                ⊤Axy⊤A                                                      
transposing the matrix.                                               ⊤Ix = x                                                                                                                           
                                                                                                                                            ⊤Ay                                                         

%%%

=                                                                     where the basis                                                       basis for a vector space spanned by a set of vectors. Assume
                                                                                                                                            we are given                                                
x                                                                     vectors are orthogonal to each other and where the length of                                                                      
                                                                      each basis                                                            a set {                                                     
⊤y                                                                                                                                                                                                      
                                                                      vector is 1. We will call this  basis  then  an  orthonormal          ˜b1, . . . ,                                                
∥x∥ ∥y∥                                                               basis.                                                                                                                            
                                                                                                                                            ˜bn} of non-orthogonal and unnormalized basis vectors. We   
, (3.32)                                                              Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       concatenate them into a matrix B˜ = [˜b1, . . . ,           
which gives exactly the angle between x and  y.  This  means                                                                                                                                            
that orthogonal matrices A with A                                     3.6 Orthogonal Complement 79                                          ˜bn] and apply Gaussian elimination to the augmented  matrix
                                                                                                                                            (Section 2.3.2) [B˜ B˜                                      
⊤ = A                                                                 Let us introduce this more formally.                                                                                              
                                                                                                                                            ⊤                                                           
−1                                                                    Definition   3.9   (Orthonormal    Basis).    Consider    an                                                                      
                                                                      n-dimensional vector                                                  |B˜ ] to obtain an                                          
preserve both angles and distances. It                                                                                                                                                                  
                                                                      space V and a basis {b1, . . . , bn} of V . If                        orthonormal basis.  This  constructive  way  to  iteratively
turns out that orthogonal  matrices  define  transformations                                                                                build an orthonormal basis {b1, . . . , bn}  is  called  the
that are rotations  (with  the  possibility  of  flips).  In          ⟨bi                                                                   Gram-Schmidt process (Strang, 2003).                        
Section 3.9, we will discuss more                                                                                                                                                                       
                                                                      , bj ⟩ = 0 for i ̸= j (3.33)                                          Example 3.8 (Orthonormal Basis)                             
details about rotations.                                                                                                                                                                                
                                                                      ⟨bi                                                                   The canonical/standard basis for a Euclidean vector space Rn
3.5 Orthonormal Basis                                                                                                                                                                                   
                                                                      , bi⟩ = 1 (3.34)                                                      is an orthonormal basis, where the inner product is the  dot
In Section  2.6.1,  we  characterized  properties  of  basis                                                                                product of vectors.                                         
vectors and found                                                     for all i, j = 1, . . . , n then  the  basis  is  called  an                                                                      
                                                                      orthonormal basis (ONB). orthonormal basis                            In R2                                                       
that in an n-dimensional  vector  space,  we  need  n  basis                                                                                                                                            
vectors, i.e., n                                                      ONB If only (3.33) is satisfied, then the basis is called an          , the vectors                                               
                                                                      orthogonal basis. Note                                                                                                            
vectors that are linearly independent. In Sections  3.3  and                                                                                b1 =                                                        
3.4, we used                                                          orthogonal basis that (3.34) implies that every basis vector                                                                      
                                                                      has length/norm 1.                                                    1                                                           
inner products to compute the  length  of  vectors  and  the                                                                                                                                            
angle between                                                         Recall  from  Section  2.6.1  that  we  can   use   Gaussian          √                                                           
                                                                      elimination to find a                                                                                                             
vectors. In the following, we will discuss the special  case                                                                                2                                                           

%%%

                                                                     Having defined orthogonality, we will  now  look  at  vector          orthogonal                                                  
                                                                      spaces that are                                                                                                                   
1                                                                                                                                           complement U⊥.                                              
                                                                      orthogonal to each other. This will play an  important  role                                                                      
1                                                                     in Chapter 10,                                                        e3                                                          
                                                                                                                                                                                                        
                                                                     when we  discuss  linear  dimensionality  reduction  from  a          e1                                                          
                                                                      geometric perspective.                                                                                                            
, b2 =                                                                                                                                      e2                                                          
                                                                      Consider a D-dimensional vector space V and an M-dimensional                                                                      
1                                                                     subspace U ⊆ V . Then its orthogonal complement U                     w                                                           
                                                                                                                                                                                                        
√                                                                     ⊥ is a (D−M)-dimensional orthogonal                                   U                                                           
                                                                                                                                                                                                        
2                                                                     complement subspace of V and contains all vectors in V  that          uniquely decomposed into                                    
                                                                      are orthogonal to every                                                                                                           
                                                                                                                                           x =                                                         
                                                                      vector in U. Furthermore, U ∩ U                                                                                                   
1                                                                                                                                           X                                                           
                                                                      ⊥ = {0} so that any vector x ∈ V can be                                                                                           
−1                                                                                                                                          M                                                           
                                                                      ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                      
                                                                     by Cambridge University Press (2020).                                 m=1                                                         
                                                                                                                                                                                                        
(3.35)                                                                80 Analytic Geometry                                                  λmbm +                                                      
                                                                                                                                                                                                        
form an orthonormal basis since b                                     Figure 3.1 A plane                                                    D                                                           
                                                                                                                                                                                                        
⊤                                                                     U in a                                                                X−M                                                         
                                                                                                                                                                                                        
1 b2 = 0 and ∥b1∥ = 1 = ∥b2∥.                                         three-dimensional                                                     j=1                                                         
                                                                                                                                                                                                        
We will exploit the  concept  of  an  orthonormal  basis  in          vector space can be                                                   ψjb                                                         
Chapter 12 and                                                                                                                                                                                          
                                                                      described by its                                                      ⊥                                                           
Chapter 10 when  we  discuss  support  vector  machines  and                                                                                                                                            
principal component analysis.                                         normal vector,                                                        j                                                           
                                                                                                                                                                                                        
3.6 Orthogonal Complement                                             which spans its                                                       , λm, ψj ∈ R , (3.36)                                       
                                                                                                                                                                                                        

%%%

where (b1, . . . , bM) is a basis of U and (b                         Thus far, we looked  at  properties  of  inner  products  to          u(x)v(x)dx (3.37)                                           
                                                                      compute lengths,                                                                                                                  
⊥                                                                                                                                           Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                      angles and  distances.  We  focused  on  inner  products  of          Feedback: https://mml-book.com.                             
1                                                                     finite-dimensional                                                                                                                
                                                                                                                                            3.8 Orthogonal Projections 81                               
, . . . , b                                                           vectors. In the following, we will look  at  an  example  of                                                                      
                                                                      inner products of                                                     for lower and upper limits a, b < ∞, respectively.  As  with
⊥                                                                                                                                           our usual inner                                             
                                                                      a different type of vectors: inner products of functions.                                                                         
D−M) is a basis of U                                                                                                                        product, we can define norms and orthogonality by looking at
                                                                      The inner products we discussed  so  far  were  defined  for          the inner                                                   
⊥.                                                                    vectors with a                                                                                                                    
                                                                                                                                            product. If (3.37) evaluates to 0, the functions u and v are
Therefore, the orthogonal complement can  also  be  used  to          finite number of entries. We can think of a vector x ∈ Rn as          orthogonal. To                                              
describe a                                                            a function                                                                                                                        
                                                                                                                                            make the preceding inner product mathematically precise,  we
plane U (two-dimensional subspace)  in  a  three-dimensional          with n function values. The concept of an inner product  can          need to take                                                
vector space.                                                         be generalized                                                                                                                    
                                                                                                                                            care of measures and the definition of integrals, leading to
More specifically, the vector w  with  ∥w∥  =  1,  which  is          to vectors with an infinite  number  of  entries  (countably          the definition of                                           
orthogonal to the                                                     infinite) and also                                                                                                                
                                                                                                                                            a Hilbert  space.  Furthermore,  unlike  inner  products  on
plane U, is the basis vector of U                                     continuous-valued functions (uncountably infinite). Then the          finite-dimensional                                          
                                                                      sum over                                                                                                                          
⊥. Figure 3.1 illustrates this setting. All                                                                                                 vectors, inner  products  on  functions  may  diverge  (have
                                                                      individual components of vectors  (see  Equation  (3.5)  for          infinite value). All                                        
vectors that are orthogonal to w must (by construction)  lie          example) turns                                                                                                                    
in the plane                                                                                                                                this requires diving into some  more  intricate  details  of
                                                                      into an integral.                                                     real and functional                                         
normal vector U. The vector w is called the normal vector of                                                                                                                                            
U.                                                                    An inner product of two functions u : R → R and v :  R  →  R          analysis, which we do not cover in this book.               
                                                                      can be                                                                                                                            
Generally, orthogonal complements can be  used  to  describe                                                                                Example 3.9 (Inner Product of Functions)                    
hyperplanes                                                           defined as the definite integral                                                                                                  
                                                                                                                                            If we choose u = sin(x) and v = cos(x), the integrand f(x) =
in n-dimensional vector and affine spaces.                            ⟨u, v⟩ := Z b                                                         u(x)v(x) Figure 3.2 f(x) =                                  
                                                                                                                                                                                                        
3.7 Inner Product of Functions                                        a                                                                     sin(x) cos(x).                                              
                                                                                                                                                                                                        

%%%

−2.5 0.0 2.5                                                          projecting functions onto this subspace is  the  fundamental          representation.                                             
                                                                      idea behind                                                                                                                       
x                                                                                                                                           data can be represented as vectors, and in this chapter,  we
                                                                      Fourier series. ♢                                                     will discuss                                                
−0.5                                                                                                                                                                                                    
                                                                      In Section 6.4.6, we will have a look at a  second  type  of          some of the fundamental tools  for  data  compression.  More
0.0                                                                   unconventional                                                        specifically, we                                            
                                                                                                                                                                                                        
0.5                                                                   inner products: the inner product of random variables.                can  project  the  original  high-dimensional  data  onto  a
                                                                                                                                            lower-dimensional                                           
sin(                                                                  3.8 Orthogonal Projections                                                                                                        
                                                                                                                                            feature space and work in this  lower-dimensional  space  to
x) cos(                                                               Projections are an important class of linear transformations          learn more                                                  
                                                                      (besides rotations and reflections) and  play  an  important                                                                      
x                                                                     role in graphics,  coding  theory,  statistics  and  machine          about  the  dataset  and  extract  relevant  patterns.   For
                                                                      learning. In machine learning, we often deal                          example, machine                                            
)                                                                                                                                                                                                       
                                                                      with data that is high-dimensional. High-dimensional data is          ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published
of (3.37), is shown in Figure 3.2. We see that this function          often hard                                                            by Cambridge University Press (2020).                       
is odd, i.e.,                                                                                                                                                                                           
                                                                      to analyze  or  visualize.  However,  high-dimensional  data          82 Analytic Geometry                                        
f(−x) = −f(x). Therefore, the integral with limits a = −π, b          quite  often  possesses  the  property  that  only   a   few                                                                      
= π of this                                                           dimensions contain most information,                                  Figure 3.1                                                  
                                                                                                                                                                                                        
product  evaluates  to  0.  Therefore,  sin  and   cos   are          and most other dimensions are not essential to describe  key          Orthogonal                                                  
orthogonal functions.                                                 properties                                                                                                                        
                                                                                                                                            projection (orange                                          
Remark. It also holds that the collection of functions                of the data. When we compress or visualize  high-dimensional                                                                      
                                                                      data, we                                                              dots) of a                                                  
{1, cos(x), cos(2x), cos(3x), . . . } (3.38)                                                                                                                                                            
                                                                      will lose information. To minimize this compression loss, we          two-dimensional                                             
is orthogonal if we integrate from −π to π, i.e.,  any  pair          ideally find                                                                                                                      
of functions are                                                                                                                            dataset (blue dots)                                         
                                                                      the most informative dimensions in the data. As discussed in                                                                      
orthogonal to each other. The  collection  of  functions  in          Chapter 1, “Feature” is a                                             onto a                                                      
(3.38) spans a                                                                                                                                                                                          
                                                                      common expression                                                     one-dimensional                                             
large subspace of the functions that are even  and  periodic                                                                                                                                            
on [−π, π), and                                                       for data                                                              subspace (straight                                          
                                                                                                                                                                                                        

%%%

line).                                                                data retain as much information as possible and minimize the          inner product space (Rn                                     
                                                                      difference/                                                                                                                       
−4 −2 0 2 4                                                                                                                                 ,⟨·,  ·⟩)  onto  subspaces.  We  will  start  with   oneline
                                                                      error  between  the  original  data  and  the  corresponding          dimensional subspaces, which are also called lines.  If  not
x1                                                                    projection. An illustration of such an orthogonal projection          mentioned otherwise, we assume the dot product ⟨x, y⟩ = x   
                                                                      is given in Figure 3.1. Before                                                                                                    
−2                                                                                                                                          ⊤y as the inner product.                                    
                                                                      we detail how to obtain these  projections,  let  us  define                                                                      
−1                                                                    what a projection                                                     3.8.1 Projection onto One-Dimensional Subspaces (Lines)     
                                                                                                                                                                                                        
0                                                                     actually is.                                                          Assume  we  are  given  a  line  (one-dimensional  subspace)
                                                                                                                                            through the origin with basis vector b ∈ Rn                 
1                                                                     Definition 3.10 (Projection). Let V be a vector space and  U                                                                      
                                                                      ⊆ V a                                                                 . The line is a one-dimensional subspace                    
2                                                                                                                                                                                                       
                                                                      projection subspace of V . A linear mapping π :  V  →  U  is          U ⊆ Rn                                                      
x                                                                     called a projection if                                                                                                            
                                                                                                                                            spanned by b. When we project x ∈ Rn onto U, we seek the    
2                                                                     π                                                                                                                                 
                                                                                                                                            vector πU (x) ∈ U that is  closest  to  x.  Using  geometric
learning algorithms, such as  principal  component  analysis          2 = π ◦ π = π.                                                        arguments, let us                                           
(PCA) by Pearson (1901) and Hotelling (1933) and deep neural                                                                                                                                            
networks (e.g., deep                                                  Since linear mappings can  be  expressed  by  transformation          Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
                                                                      matrices (see                                                         Feedback: https://mml-book.com.                             
auto-encoders (Deng et al., 2010)), heavily exploit the idea                                                                                                                                            
of dimensionality reduction. In the following, we will focus          Section 2.7), the preceding definition applies equally to  a          3.8 Orthogonal Projections 83                               
on orthogonal projections,                                            special kind                                                                                                                      
                                                                                                                                            Figure 3.2                                                  
which we will use in Chapter 10  for  linear  dimensionality          projection matrix of transformation matrices, the projection                                                                      
reduction and                                                         matrices P π, which exhibit the                                       Examples of                                                 
                                                                                                                                                                                                        
in Chapter 12 for classification.  Even  linear  regression,          property that P                                                       projections onto                                            
which we discuss                                                                                                                                                                                        
                                                                      2                                                                     one-dimensional                                             
in  Chapter  9,  can   be   interpreted   using   orthogonal                                                                                                                                            
projections. For a given                                              π = P π.                                                              subspaces.                                                  
                                                                                                                                                                                                        
lower-dimensional  subspace,   orthogonal   projections   of          In the following, we will derive orthogonal  projections  of          b                                                           
high-dimensional                                                      vectors in the                                                                                                                    
                                                                                                                                            x                                                           

%%%

πU (x)                                                                the                                                                   ⟨b, x⟩                                                      
                                                                                                                                                                                                        
ω                                                                     coordinate of πU (x)                                                  ∥b∥                                                         
                                                                                                                                                                                                        
(a) Projection of x ∈ R2 onto a subspace U                            with respect to b.                                                    2                                                           
                                                                                                                                                                                                        
with basis vector b.                                                  The projection πU (x) of x onto U must be an  element  of  U          . (3.40)                                                    
                                                                      and, therefore, a multiple of the basis vector b that  spans                                                                      
ω cos ω                                                               U. Hence, πU (x) = λb,                                                In the last step, we exploited the fact that inner  products
                                                                                                                                            are symmetric. If we choose ⟨·, ·⟩ to be the dot product, we
sin ω                                                                 for some λ ∈ R.                                                       obtain                                                      
                                                                                                                                                                                                        
b                                                                     In the following three steps, we determine the coordinate λ,          λ =                                                         
                                                                      the projection                                                                                                                    
x                                                                                                                                           b                                                           
                                                                      πU (x) ∈ U, and the projection matrix P π that maps any x  ∈                                                                      
(b) Projection of a two-dimensional vector                            Rn onto U:                                                            ⊤                                                           
                                                                                                                                                                                                        
x with ∥x∥ = 1 onto a one-dimensional                                 1. Finding the coordinate  λ.  The  orthogonality  condition          x                                                           
                                                                      yields                                                                                                                            
subspace spanned by b.                                                                                                                      b                                                           
                                                                      ⟨x − πU (x), b⟩ = 0 πU (x)=λb ⇐⇒ ⟨x − λb, b⟩ = 0 . (3.39)                                                                         
characterize  some  properties  of  the  projection  πU  (x)                                                                                ⊤                                                           
(Figure 3.2(a) serves                                                 We can now exploit the bilinearity of the inner product  and                                                                      
                                                                      arrive at With a general inner                                        b                                                           
as an illustration):                                                                                                                                                                                    
                                                                      product, we get                                                       =                                                           
The projection πU (x)  is  closest  to  x,  where  “closest”                                                                                                                                            
implies that the                                                      λ = ⟨x, b⟩ if                                                         b                                                           
                                                                                                                                                                                                        
distance ∥x−πU (x)∥ is minimal. It follows that the  segment          ∥b∥ = 1.                                                              ⊤                                                           
πU (x)−x                                                                                                                                                                                                
                                                                      ⟨x, b⟩ − λ ⟨b, b⟩ = 0 ⇐⇒ λ =                                          x                                                           
from πU (x) to x is orthogonal to U, and therefore the basis                                                                                                                                            
vector b of                                                           ⟨x, b⟩                                                                ∥b∥                                                         
                                                                                                                                                                                                        
U. The orthogonality condition yields ⟨πU (x) − x,  b⟩  =  0          ⟨b, b⟩                                                                2                                                           
since angles                                                                                                                                                                                            
                                                                      =                                                                     . (3.41)                                                    
between vectors are defined via the inner product. λ is then                                                                                                                                            

%%%

If ∥b∥ = 1, then the coordinate λ of the projection is given          compute the length of πU (x) by means of Definition 3.1 as            (3.44)                                                      
by b                                                                                                                                                                                                    
                                                                      ∥πU (x)∥ = ∥λb∥ = |λ| ∥b∥ . (3.43)                                    Here, ω is the angle between x and b. This  equation  should
⊤                                                                                                                                           be familiar                                                 
                                                                      Hence, our projection is of length |λ| times the  length  of                                                                      
x.                                                                    b. This also                                                          from trigonometry: If ∥x∥ = 1,  then  x  lies  on  the  unit
                                                                                                                                            circle. It follows                                          
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          adds the intuition that λ is the coordinate of πU  (x)  with                                                                      
by Cambridge University Press (2020).                                 respect to the                                                        The horizontal axis that the projection onto the  horizontal
                                                                                                                                            axis spanned by b is exactly                                
84 Analytic Geometry                                                  basis vector b that spans our one-dimensional subspace U.                                                                         
                                                                                                                                            is a one-dimensional                                        
2. Finding the projection point πU (x) ∈ U. Since πU  (x)  =          If we use the dot product as an inner product, we get                                                                             
λb, we immediately obtain with (3.40) that                                                                                                  subspace.                                                   
                                                                      ∥πU (x)∥                                                                                                                          
πU (x) = λb =                                                                                                                               cos ω, and the length of the corresponding vector πU  (x)  =
                                                                      (3.42) =                                                              |cos ω|. An                                                 
⟨x, b⟩                                                                                                                                                                                                  
                                                                      |b                                                                    illustration is given in Figure 3.2(b).                     
∥b∥                                                                                                                                                                                                     
                                                                      ⊤                                                                     3. Finding the  projection  matrix  P  π.  We  know  that  a
2                                                                                                                                           projection  is  a  linear  mapping  (see  Definition  3.10).
                                                                      x|                                                                    Therefore, there exists a projection                        
b =                                                                                                                                                                                                     
                                                                      ∥b∥                                                                   matrix P π, such that πU (x) = P πx. With the dot product as
b                                                                                                                                           inner                                                       
                                                                      2                                                                                                                                 
⊤                                                                                                                                           product and                                                 
                                                                      ∥b∥                                                                                                                               
x                                                                                                                                           πU (x) = λb = bλ = b                                        
                                                                      (3.25) = | cos ω| ∥x∥ ∥b∥                                                                                                         
∥b∥                                                                                                                                         b                                                           
                                                                      ∥b∥                                                                                                                               
2                                                                                                                                           ⊤                                                           
                                                                      ∥b∥                                                                                                                               
b , (3.42)                                                                                                                                  x                                                           
                                                                      2                                                                                                                                 
where the last equality holds for the dot product  only.  We                                                                                ∥b∥                                                         
can also                                                              = | cos ω| ∥x∥ .                                                                                                                  
                                                                                                                                            2                                                           

%%%

=                                                                     Remark. The projection πU (x) ∈ Rn                                    orthogonal to both                                          
                                                                                                                                                                                                        
bb⊤                                                                   is still an n-dimensional vector and                                  b1 and b2.                                                  
                                                                                                                                                                                                        
∥b∥                                                                   not a scalar. However, we no longer require n coordinates to          0                                                           
                                                                      represent the                                                                                                                     
2                                                                                                                                           x                                                           
                                                                      projection, but only a single one if we want to  express  it                                                                      
x , (3.45)                                                            with respect to                                                       b1                                                          
                                                                                                                                                                                                        
we immediately see that                                               the basis vector b that spans the subspace U: λ. ♢                    b2                                                          
                                                                                                                                                                                                        
P π =                                                                 Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          U                                                           
                                                                      Feedback: https://mml-book.com.                                                                                                   
bb⊤                                                                                                                                         πU (x)                                                      
                                                                      3.8 Orthogonal Projections 85                                                                                                     
∥b∥                                                                                                                                         x − πU (x)                                                  
                                                                      Figure 3.1                                                                                                                        
2                                                                                                                                           Example 3.10 (Projection onto a Line)                       
                                                                      Projection onto a                                                                                                                 
. (3.46)                                                                                                                                    Find the projection matrix P π onto  the  line  through  the
                                                                      two-dimensional                                                       origin spanned                                              
Note that bb⊤                                                                                                                                                                                           
                                                                      subspace U with                                                       by b =                                                      
Projection matrices (and, consequently, P π) is a  symmetric                                                                                                                                            
matrix (of rank                                                       basis b1, b2. The                                                     1 2 2⊤                                                      
                                                                                                                                                                                                        
are always                                                            projection πU (x) of                                                  . b is a direction and a basis of the one-dimensional       
                                                                                                                                                                                                        
symmetric.                                                            x ∈ R3 onto U can                                                     subspace (line through origin).                             
                                                                                                                                                                                                        
1), and ∥b∥                                                           be expressed as a                                                     With (3.46), we obtain                                      
                                                                                                                                                                                                        
2 = ⟨b, b⟩ is a scalar.                                               linear combination                                                    P π =                                                       
                                                                                                                                                                                                        
The projection matrix P π projects any vector x  ∈  Rn  onto          of b1, b2 and the                                                     bb⊤                                                         
the line through                                                                                                                                                                                        
                                                                      displacement vector                                                   b                                                           
the origin with direction b (equivalently,  the  subspace  U                                                                                                                                            
spanned                        by                        b).          x            −             πU             (x)             is          ⊤                                                           

%%%

b                                                                                                                                          1                                                           
                                                                                                                                                                                                        
=                                                                      . (3.47)                                                                                                                       
                                                                                                                                                                                                        
1                                                                     Let us now choose a particular x and see whether it lies  in           =                                                         
                                                                      the subspace                                                                                                                      
9                                                                                                                                           1                                                           
                                                                      spanned by b. For x =                                                                                                             
                                                                                                                                           9                                                           
                                                                      1 1 1⊤                                                                                                                            
                                                                                                                                                                                                      
                                                                      , the projection is                                                                                                               
1                                                                                                                                                                                                      
                                                                      πU (x) = P πx =                                                                                                                   
2                                                                                                                                           5                                                           
                                                                      1                                                                                                                                 
2                                                                                                                                           10                                                          
                                                                      9                                                                                                                                 
                                                                                                                                           10                                                          
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
1 2 2                                                                                                                                        ∈ span[                                                   
                                                                      1 2 2                                                                                                                             
=                                                                                                                                                                                                      
                                                                      2 4 4                                                                                                                             
1                                                                                                                                                                                                      
                                                                      2 4 4                                                                                                                             
9                                                                                                                                           1                                                           
                                                                                                                                                                                                       
                                                                                                                                           2                                                           
                                                                                                                                                                                                       
                                                                                                                                           2                                                           
                                                                                                                                                                                                       
1 2 2                                                                                                                                                                                                  
                                                                                                                                                                                                       
2 4 4                                                                                                                                       ] . (3.48)                                                 
                                                                      1                                                                                                                                 
2 4 4                                                                                                                                       Note that the application of P π to πU (x) does  not  change
                                                                      1                                                                     anything,                                              i.e.,

%%%

P ππU (x) = πU (x). This is expected  because  according  to          onto U is necessarily an element of U. Therefore,  they  can          m , (3.50)                                                  
Definition 3.10,                                                      be represented                                                                                                                    
                                                                                                                                            is closest to x ∈ Rn                                        
we know that a projection matrix P π satisfies P                      ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                      
                                                                      by Cambridge University Press (2020).                                 . As in the 1D case, “closest” means “minimum               
2                                                                                                                                                                                                       
                                                                      86 Analytic Geometry                                                  distance”, which implies that the vector connecting πU (x) ∈
πx = P πx for all x.                                                                                                                        U and                                                       
                                                                      as linear combinations of the basis vectors b1, . . .  ,  bm                                                                      
Remark. With the results from Chapter 4, we can show that πU          of U, such that                                                       x ∈ Rn must  be  orthogonal  to  all  basis  vectors  of  U.
(x) is an                                                                                                                                   Therefore, we                                               
                                                                      πU (x) = Pm                                                                                                                       
eigenvector of P π, and the corresponding eigenvalue is 1. ♢                                                                                obtain m simultaneous conditions (assuming the  dot  product
                                                                      i=1 λibi The basis vectors .                                          as the                                                      
3.8.2 Projection onto General Subspaces                                                                                                                                                                 
                                                                      form the columns of                                                   inner product)                                              
If U is given by a set                                                                                                                                                                                  
                                                                      B ∈ Rn×m, where                                                       ⟨b1, x − πU (x)⟩ = b                                        
of spanning vectors,                                                                                                                                                                                    
                                                                      B = [b1, . . . , bm].                                                 ⊤                                                           
which are not a                                                                                                                                                                                         
                                                                      As in the 1D case, we follow a three-step procedure to  find          1                                                           
basis, make sure                                                      the projection πU (x) and the projection matrix P π:                                                                              
                                                                                                                                            (x − πU (x)) = 0 (3.51)                                     
you determine a                                                       1. Find the coordinates λ1, . . . ,  λm  of  the  projection                                                                      
                                                                      (with respect to the                                                  .                                                           
basis b1, . . . , bm                                                                                                                                                                                    
                                                                      basis of U), such that the linear combination                         .                                                           
before proceeding.                                                                                                                                                                                      
                                                                      πU (x) = Xm                                                           .                                                           
In the following,  we  look  at  orthogonal  projections  of                                                                                                                                            
vectors x ∈ Rn                                                        i=1                                                                   ⟨bm, x − πU (x)⟩ = b                                        
                                                                                                                                                                                                        
onto lower-dimensional subspaces U ⊆ Rn with dim(U) = m ⩾ 1.          λibi = Bλ , (3.49)                                                    ⊤                                                           
An                                                                                                                                                                                                      
                                                                      B = [b1, . . . , bm] ∈ R                                              m(x − πU (x)) = 0 (3.52)                                    
illustration is given in Figure 3.1.                                                                                                                                                                    
                                                                      n×m, λ = [λ1, . . . , λm]                                             which, with πU (x) = Bλ, can be written as                  
Assume that (b1, . . . , bm) is an ordered basis of  U.  Any                                                                                                                                            
projection                      πU                       (x)          ⊤                            ∈                             R          b                                                           

%%%

⊤                                                                     ⊤                                                                     λ = (B                                                      
                                                                                                                                                                                                        
1                                                                     m                                                                     ⊤B)                                                         
                                                                                                                                                                                                        
(x − Bλ) = 0 (3.53)                                                                                                                        −1B                                                         
                                                                                                                                                                                                        
.                                                                                                                                          ⊤                                                           
                                                                                                                                                                                                        
.                                                                                                                                          x . (3.57)                                                  
                                                                                                                                                                                                        
.                                                                                                                                          The matrix (B                                               
                                                                                                                                                                                                        
b                                                                     x − Bλ                                                               ⊤B)                                                         
                                                                                                                                                                                                        
⊤                                                                                                                                          −1B                                                         
                                                                                                                                                                                                        
m(x − Bλ) = 0 (3.54)                                                   = 0 ⇐⇒ B                                                            ⊤                                                           
                                                                                                                                                                                                        
such that we obtain a homogeneous linear equation system              ⊤                                                                     pseudo-inverse is also called the pseudo-inverse of B, which
                                                                                                                                                                                                        
                                                                     (x − Bλ) = 0 (3.55)                                                   can be computed for non-square matrices B. It only  requires
                                                                                                                                            that B                                                      
                                                                     ⇐⇒ B                                                                                                                              
                                                                                                                                            ⊤B                                                          
                                                                     ⊤Bλ = B                                                                                                                           
                                                                                                                                            is positive definite, which is the case if B is  full  rank.
b                                                                     ⊤                                                                     In practical  applications  (e.g.,  linear  regression),  we
                                                                                                                                            often add a “jitter term” ϵI to                             
⊤                                                                     x . (3.56)                                                                                                                        
                                                                                                                                            Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.
1                                                                     normal  equation  The  last  expression  is  called   normal          Feedback: https://mml-book.com.                             
                                                                      equation. Since b1, . . . , bm are a                                                                                              
.                                                                                                                                           3.8 Orthogonal Projections 87                               
                                                                      basis of U and, therefore, linearly independent, B                                                                                
.                                                                                                                                           B                                                           
                                                                      ⊤B ∈ Rm×m is regular and can be inverted. This allows us  to                                                                      
.                                                                     solve for the coefficients/                                           ⊤B to guarantee increased numerical stability  and  positive
                                                                                                                                            definiteness. This “ridge” can be rigorously  derived  using
b                                                                     coordinates                                                           Bayesian inference.                                         
                                                                                                                                                                                                        

%%%

See Chapter 9 for details.                                            ⊤B)                                                                   1                                                           
                                                                                                                                                                                                        
2. Find the projection πU (x) ∈ U.  We  already  established          −1B                                                                   2                                                           
that πU (x) =                                                                                                                                                                                           
                                                                      ⊤                                                                                                                                
Bλ. Therefore, with (3.57)                                                                                                                                                                              
                                                                      as                                                                    ] ⊆ R3 and x =                                             
πU (x) = B(B                                                                                                                                                                                            
                                                                      P π =                                                                                                                            
⊤B)                                                                                                                                                                                                     
                                                                      BB⊤                                                                                                                              
−1B                                                                                                                                                                                                     
                                                                      B⊤B                                                                   6                                                           
⊤                                                                                                                                                                                                       
                                                                      , which is exactly the projection matrix in (3.46). ♢                 0                                                           
x . (3.58)                                                                                                                                                                                              
                                                                      Example 3.11 (Projection onto a Two-dimensional Subspace)             0                                                           
3. Find the projection matrix  P  π.  From  (3.58),  we  can                                                                                                                                            
immediately see                                                       For a subspace U = span[                                                                                                         
                                                                                                                                                                                                        
that the projection matrix that solves P πx = πU (x) must be                                                                                ∈ R3 find the                                             
                                                                                                                                                                                                        
P π = B(B                                                                                                                                  coordinates  λ  of  x  in  terms  of  the  subspace  U,  the
                                                                                                                                            projection point πU (x)                                     
⊤B)                                                                   1                                                                                                                                 
                                                                                                                                            and the projection matrix P π.                              
−1B                                                                   1                                                                                                                                 
                                                                                                                                            First, we see that the  generating  set  of  U  is  a  basis
⊤                                                                     1                                                                     (linear independence) and write the basis vectors of U  into
                                                                                                                                            a matrix B =                                                
. (3.59)                                                                                                                                                                                               
                                                                                                                                                                                                       
Remark. The solution for projecting onto  general  subspaces           ,                                                                                                                               
includes the                                                                                                                                                                                           
                                                                                                                                                                                                       
1D case as a special case: If dim(U) = 1, then B                                                                                            1 0                                                         
                                                                                                                                                                                                       
⊤B ∈ R is a scalar and                                                                                                                      1 1                                                         
                                                                      0                                                                                                                                 
we can rewrite the projection matrix in (3.59)  P  π  =  B(B                                                                                1                                                          2

%%%

                                                                     3 5                                                                  Third, we solve the normal equation B                       
                                                                                                                                                                                                        
.                                                                    , B                                                                   ⊤Bλ = B                                                     
                                                                                                                                                                                                        
Second, we compute the matrix B                                       ⊤                                                                     ⊤                                                           
                                                                                                                                                                                                        
⊤B and the vector B                                                   x =                                                                   x to find λ:                                                
                                                                                                                                                                                                        
⊤                                                                                                                                                                                                     
                                                                                                                                                                                                        
x as                                                                  1 1 1                                                                 3 3                                                         
                                                                                                                                                                                                        
B                                                                     0 1 2                                                                3 5 λ1                                                    
                                                                                                                                                                                                        
⊤B =                                                                                                                                       λ2                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 1                                                                 6                                                                     =                                                           
                                                                                                                                                                                                        
0 1 2                                                                0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                     6                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1 0                                                                    =                                                                                                                              
                                                                                                                                                                                                        
1 1                                                                                                                                        ⇐⇒ λ =                                                      
                                                                                                                                                                                                        
1 2                                                                   6                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                     5                                                           
                                                                                                                                                                                                        
 =                                                                                                                                        −3                                                          
                                                                                                                                                                                                        
                                                                     .                                                                                                                                
                                                                                                                                                                                                        
3 3                                                                   (3.60)                                                                . (3.61)                                                    
                                                                                                                                                                                                        

%%%

Fourth, the projection πU (x) of x onto U,  i.e.,  into  the          √                                                                     P π = P                                                     
column space of                                                                                                                                                                                         
                                                                      6 . (3.63)                                                            2                                                           
B, can be directly computed via                                                                                                                                                                         
                                                                      Fifth, the projection matrix (for any x ∈ R3                          π                                                           
πU (x) = Bλ =                                                                                                                                                                                           
                                                                      ) is given by                                                         (see Definition 3.10).                                      
                                                                                                                                                                                                       
                                                                      P π = B(B                                                             Remark. The projections πU  (x)  are  still  vectors  in  Rn
                                                                                                                                           although they lie in                                        
                                                                      ⊤B)                                                                                                                               
5                                                                                                                                           an m-dimensional subspace U ⊆ Rn                            
                                                                      −1B                                                                                                                               
2                                                                                                                                           . However, to represent a projected                         
                                                                      ⊤ =                                                                                                                               
−1                                                                                                                                          vector we only need the m coordinates λ1, . . .  ,  λm  with
                                                                      1                                                                     respect to the                                              
                                                                                                                                                                                                       
                                                                      6                                                                     basis vectors b1, . . . , bm of U. ♢                        
 . (3.62)                                                                                                                                                                                              
                                                                                                                                           Remark. In vector spaces with  general  inner  products,  we
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                have to pay                                                 
by Cambridge University Press (2020).                                                                                                                                                                  
                                                                                                                                            attention when computing angles  and  distances,  which  are
88 Analytic Geometry                                                  5 2 −1                                                                defined by                                                  
                                                                                                                                                                                                        
projection error The corresponding projection error  is  the          2 2 2                                                                 means of the inner product. ♢ We can find                   
norm of the difference vector                                                                                                                                                                           
                                                                      −1 2 5                                                                approximate                                                 
The projection error between the  original  vector  and  its                                                                                                                                            
projection onto U, i.e.,                                                                                                                   solutions to                                                
                                                                                                                                                                                                        
is also called the                                                     . (3.64)                                                            unsolvable linear                                           
                                                                                                                                                                                                        
reconstruction error. ∥x − πU (x)∥ =                                  To  verify  the  results,  we  can  (a)  check  whether  the          equation systems                                            
                                                                      displacement vector                                                                                                               
1 −2 1⊤                                                                                                                                     using projections.                                          
                                                                      πU (x) − x is orthogonal to all basis vectors of U, and  (b)                                                                      
=                                                                     verify that                                                           Projections allow us to look at situations where we  have  a
                                                                                                                                            linear                                                system

%%%

Ax = b without a solution. Recall that  this  means  that  b          πU (x) = BB⊤                                                          orthogonalization constructs an orthogonal basis (u1, . .  .
does not lie in                                                                                                                             ,un) from any basis (b1, . . . , bn) of                     
                                                                      x (3.65)                                                                                                                          
the span of A, i.e., the  vector  b  does  not  lie  in  the                                                                                V as follows:                                               
subspace spanned by                                                   since B                                                                                                                           
                                                                                                                                            u1 := b1 (3.67)                                             
the columns of A. Given that the linear equation  cannot  be          ⊤B = I with coordinates                                                                                                           
solved exactly,                                                                                                                             uk := bk − πspan[u1,...,uk−1](bk), k = 2, . . . , n . (3.68)
                                                                      λ = B                                                                                                                             
we can find an approximate solution. The idea is to find the                                                                                In (3.68), the kth basis vector bk  is  projected  onto  the
vector in the                                                         ⊤                                                                     subspace spanned                                            
                                                                                                                                                                                                        
subspace spanned by the columns of A that is closest  to  b,          x . (3.66)                                                            by the first k − 1 constructed orthogonal vectors u1, . .  .
i.e., we compute                                                                                                                            ,uk−1; see Section 3.8.2. This projection is then subtracted
                                                                      This means that we no longer have  to  compute  the  inverse          from bk and yields a vector                                 
the orthogonal projection of b onto the subspace spanned  by          from (3.58),                                                                                                                      
the columns                                                                                                                                 uk that is orthogonal to the (k  −  1)-dimensional  subspace
                                                                      which saves computation time. ♢                                       spanned by                                                  
of A.  This  problem  arises  often  in  practice,  and  the                                                                                                                                            
solution is called the                                                Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          u1, . . . ,uk−1. Repeating this procedure for  all  n  basis
                                                                      Feedback: https://mml-book.com.                                       vectors b1, . . . , bn                                      
least-squares  least-squares  solution  (assuming  the   dot                                                                                                                                            
product as the inner product) of                                      3.8 Orthogonal Projections 89                                         yields an orthogonal basis (u1, . . . , un) of  V  .  If  we
                                                                                                                                            normalize the uk, we                                        
solution an overdetermined system. This is discussed further          3.8.3 Gram-Schmidt Orthogonalization                                                                                              
in Section 9.4. Using                                                                                                                       obtain an ONB where ∥uk∥ = 1 for k = 1, . . . , n.          
                                                                      Projections are at the core of the Gram-Schmidt method  that                                                                      
reconstruction errors (3.63) is  one  possible  approach  to          allows us to                                                          Example 3.12 (Gram-Schmidt Orthogonalization)               
derive principal                                                                                                                                                                                        
                                                                      constructively transform any basis (b1, . . . ,  bn)  of  an          Figure 3.2                                                  
component analysis (Section 10.3).                                    n-dimensional vector                                                                                                              
                                                                                                                                            Gram-Schmidt                                                
Remark. We just looked at projections of vectors  x  onto  a          space V into an orthogonal/orthonormal basis (u1, . . . ,un)                                                                      
subspace U with                                                       of V . This                                                           orthogonalization.                                          
                                                                                                                                                                                                        
basis vectors {b1, . . . , bk}. If this  basis  is  an  ONB,          basis always exists (Liesen and Mehrmann, 2015) and span[b1,          (a) non-orthogonal                                          
i.e., (3.33) and (3.34)                                               . . . , bn] =                                                                                                                     
                                                                                                                                            basis (b1, b2) of R2                                        
are satisfied, the  projection  equation  (3.58)  simplifies          span[u1, . .  .  ,un].  The  Gram-Schmidt  orthogonalization                                                                      
greatly                                                   to          method               iteratively                Gram-Schmidt          ;                                                           

%%%

(b) first constructed                                                 u1                                                                    see also Figure 3.2(a). Using the  Gram-Schmidt  method,  we
                                                                                                                                            construct an                                                
basis vector u1 and                                                   b2                                                                                                                                
                                                                                                                                            orthogonal basis (u1,u2) of R2 as follows (assuming the  dot
orthogonal                                                            0 πspan[u1](b2)                                                       product as                                                  
                                                                                                                                                                                                        
projection of b2                                                      u2                                                                    the inner product):                                         
                                                                                                                                                                                                        
onto span[u1];                                                        (c) Orthogonal basis vectors u1                                       u1 := b1 =                                                  
                                                                                                                                                                                                        
(c) orthogonal basis                                                  and u2 = b2 − πspan[u1]                                                                                                          
                                                                                                                                                                                                        
(u1, u2) of R2                                                        (b2).                                                                 2                                                           
                                                                                                                                                                                                        
.                                                                     Consider a basis (b1, b2) of R2                                       0                                                           
                                                                                                                                                                                                        
b1                                                                    , where                                                                                                                          
                                                                                                                                                                                                        
b2                                                                    b1 =                                                                  , (3.70)                                                    
                                                                                                                                                                                                        
0                                                                                                                                          u2 := b2 − πspan[u1](b2)                                    
                                                                                                                                                                                                        
(a) Original non-orthogonal                                           2                                                                     (3.45) = b2 −                                               
                                                                                                                                                                                                        
basis vectors b1, b2.                                                 0                                                                     u1u                                                         
                                                                                                                                                                                                        
u1                                                                                                                                         ⊤                                                           
                                                                                                                                                                                                        
b2                                                                    , b2 =                                                                1                                                           
                                                                                                                                                                                                        
0 πspan[u1](b2)                                                                                                                            ∥u1∥                                                        
                                                                                                                                                                                                        
(b) First new basis vector                                            1                                                                     2                                                           
                                                                                                                                                                                                        
u1 = b1 and projection of b2                                          1                                                                     b2 =                                                        
                                                                                                                                                                                                        
onto the subspace spanned by                                                                                                                                                                          
                                                                                                                                                                                                        
u1.                                                                   ; (3.69)                                                              1                                                           
                                                                                                                                                                                                        

%%%

1                                                                     (a) original setting;                                                 U = L − x0                                                  
                                                                                                                                                                                                        
                                                                     (b) setting shifted                                                   πU(x − x0)                                                  
                                                                                                                                                                                                        
−                                                                     by −x0 so that                                                        b2                                                          
                                                                                                                                                                                                        
                                                                     x − x0 can be                                                         (b) Reduce problem to projection πU onto vector subspace.   
                                                                                                                                                                                                        
1 0                                                                   projected onto the                                                    L                                                           
                                                                                                                                                                                                        
0 0 1                                                               direction space U;                                                    x0                                                          
                                                                                                                                                                                                        
1                                                                     (c) projection is                                                     x                                                           
                                                                                                                                                                                                        
                                                                     translated back to                                                    b2                                                          
                                                                                                                                                                                                        
=                                                                     x0 + πU (x − x0),                                                     0 b1                                                        
                                                                                                                                                                                                        
                                                                     which gives the final                                                 πL(x)                                                       
                                                                                                                                                                                                        
0                                                                     orthogonal                                                            (c) Add support point back in                               
                                                                                                                                                                                                        
1                                                                     projection πL(x).                                                     to get affine projection πL.                                
                                                                                                                                                                                                        
                                                                     L                                                                     These steps are illustrated in Figures 3.2(b)  and  (c).  We
                                                                                                                                            immediately see                                             
.                                                                     x0                                                                                                                                
                                                                                                                                            that u1 and u2 are orthogonal, i.e., u                      
(3.71)                                                                x                                                                                                                                 
                                                                                                                                            ⊤                                                           
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published          b2                                                                                                                                
by Cambridge University Press (2020).                                                                                                       1 u2 = 0.                                                   
                                                                      0 b1                                                                                                                              
90 Analytic Geometry                                                                                                                        3.8.4 Projection onto Affine Subspaces                      
                                                                      (a) Setting.                                                                                                                      
Figure 3.3                                                                                                                                  Thus far, we discussed  how  to  project  a  vector  onto  a
                                                                      0 b1                                                                  lower-dimensional                                           
Projection onto an                                                                                                                                                                                      
                                                                      x − x0                                                                subspace U. In the  following,  we  provide  a  solution  to
affine                                                space.                                                                                projecting                     a                      vector

%%%

onto an affine subspace.                                              from the affine                                                       rotate its joints in                                        
                                                                                                                                                                                                        
Consider the setting in  Figure  3.3(a).  We  are  given  an          space L is identical to the distance of x − x0 from U, i.e.,          order to pick up                                            
affine space L =                                                                                                                                                                                        
                                                                      d(x, L) = ∥x − πL(x)∥ = ∥x − (x0 + πU (x − x0))∥ (3.73a)              objects or to place                                         
x0 + U, where b1, b2 are basis vectors of  U.  To  determine                                                                                                                                            
the orthogonal                                                        = d(x − x0, πU (x − x0)) = d(x − x0, U). (3.73b)                      them correctly.                                             
                                                                                                                                                                                                        
projection πL(x) of x onto L, we transform the problem  into          We will use projections onto an affine  subspace  to  derive          Figure taken                                                
a problem                                                             the concept of                                                                                                                    
                                                                                                                                            from (Deisenroth                                            
that we know how to solve:  the  projection  onto  a  vector          a separating hyperplane in Section 12.1.                                                                                          
subspace. In                                                                                                                                et al., 2015).                                              
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
order to get there, we subtract the support point x0 from  x          Feedback: https://mml-book.com.                                       3.9 Rotations                                               
and from L,                                                                                                                                                                                             
                                                                      3.9 Rotations 91                                                      Length and angle preservation, as discussed in Section  3.4,
so that L − x0 = U is exactly the vector subspace U. We  can                                                                                are the two                                                 
now use the                                                           Figure 3.2 A                                                                                                                      
                                                                                                                                            characteristics   of   linear   mappings   with   orthogonal
orthogonal projections  onto  a  subspace  we  discussed  in          rotation rotates                                                      transformation matrices. In the following, we  will  have  a
Section 3.8.2 and                                                                                                                           closer look at specific orthogonal                          
                                                                      objects in a plane                                                                                                                
obtain the projection πU (x − x0), which is  illustrated  in                                                                                transformation matrices, which describe rotations.          
Figure 3.3(b).                                                        about the origin. If                                                                                                              
                                                                                                                                            A rotation  is  a  linear  mapping  (more  specifically,  an
This projection can now be translated back into L by  adding          the rotation angle is                                                 automorphism of rotation                                    
x0, such that                                                                                                                                                                                           
                                                                      positive, we rotate                                                   a Euclidean vector space) that rotates a plane by an angle θ
we obtain the orthogonal projection onto an affine  space  L                                                                                about the                                                   
as                                                                    counterclockwise.                                                                                                                 
                                                                                                                                            origin, i.e., the origin is a fixed point.  For  a  positive
πL(x) = x0 + πU (x − x0), (3.72)                                      Original                                                              angle  θ  >  0,  by  common  convention,  we  rotate  in   a
                                                                                                                                            counterclockwise direction. An example is                   
where πU (·) is the orthogonal projection onto the  subspace          Rotated by 112.5◦                                                                                                                 
U, i.e., the                                                                                                                                shown in Figure 3.2, where the transformation matrix is     
                                                                      Figure 3.1 The                                                                                                                    
direction space of L; see Figure 3.3(c).                                                                                                    R =                                                         
                                                                      robotic arm needs to                                                                                                              
From Figure 3.3, it is also evident that the distance  of  x                                                                                                                                           

%%%

−0.38 −0.92                                                           Φ(e1) = [cos θ,sin θ]                                                 . We aim to rotate this coordinate                          
                                                                                                                                                                                                        
0.92 −0.38                                                           ⊤                                                                     system by an angle θ as illustrated in Figure 3.2. Note that
                                                                                                                                            the rotated                                                 
. (3.74)                                                              cos θ                                                                                                                             
                                                                                                                                            vectors are still linearly independent and, therefore, are a
Important application areas of  rotations  include  computer          sin θ                                                                 basis of R2                                                 
graphics and                                                                                                                                                                                            
                                                                      − sin θ                                                               .                                                           
robotics. For example, in robotics, it is often important to                                                                                                                                            
know how to                                                           cos θ                                                                 This means that the rotation performs a basis change.       
                                                                                                                                                                                                        
rotate the joints of a robotic arm in order to  pick  up  or          3.9.1 Rotations in R2                                                 Rotations Φ are linear mappings so that we can express  them
place an object,                                                                                                                            by a                                                        
                                                                      Consider the standard basis                                                                                                      
see Figure 3.1.                                                                                                                             rotation matrix  rotation  matrix  R(θ).  Trigonometry  (see
                                                                      e1 =                                                                  Figure 3.2) allows us to determine the  coordinates  of  the
©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                                rotated axes (the image of Φ) with respect to               
by Cambridge University Press (2020).                                                                                                                                                                  
                                                                                                                                            the standard basis in R2                                    
92 Analytic Geometry                                                  1                                                                                                                                 
                                                                                                                                            . We obtain                                                 
Figure 3.2 Rotation                                                   0                                                                                                                                 
                                                                                                                                            Φ(e1) =                                                    
of the standard basis                                                                                                                                                                                  
                                                                                                                                            cos θ                                                       
in R2 by an angle θ.                                                  , e2 =                                                                                                                            
                                                                                                                                            sin θ                                                       
e1                                                                                                                                                                                                     
                                                                                                                                                                                                       
e2                                                                    0                                                                                                                                 
                                                                                                                                            , Φ(e2) =                                                  
θ                                                                     1                                                                                                                                 
                                                                                                                                            − sin θ                                                     
θ                                                                      of R2                                                                                                                          
                                                                                                                                            cos θ                                                       
Φ(e2) = [− sin θ, cos θ]                                              , which defines                                                                                                                   
                                                                                                                                                                                                       
⊤                                                                     the standard coordinate system in R2                                                                                              
                                                                                                                                            .                                                     (3.75)

%%%

Therefore, the  rotation  matrix  that  performs  the  basis          “counterclockwise” means when we operate in  more  than  two          Rotation about the e1-axis                                  
change into the                                                       dimensions. We                                                                                                                    
                                                                                                                                            R1(θ) =                                                     
rotated coordinates R(θ) is given as                                  use  the  convention  that  a  “counterclockwise”   (planar)                                                                      
                                                                      rotation about an                                                     Φ(e1) Φ(e2) Φ(e3)                                           
R(θ) =                                                                                                                                                                                                  
                                                                      axis refers to a rotation about an axis when we look at  the          =                                                           
Φ(e1) Φ(e2)                                                           axis “head on,                                                                                                                    
                                                                                                                                                                                                       
=                                                                     from the end toward the origin”. In R3                                                                                            
                                                                                                                                                                                                       
                                                                     , there are therefore three (planar)                                                                                              
                                                                                                                                            1 0 0                                                       
cos θ − sin θ                                                         rotations about the three standard basis vectors (see Figure                                                                      
                                                                      3.2):                                                                 0 cos θ − sin θ                                             
sin θ cos θ                                                                                                                                                                                             
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.          0 sin θ cos θ                                               
                                                                     Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                                                                                       
. (3.76)                                                              3.9 Rotations 93                                                                                                                  
                                                                                                                                             . (3.77)                                                  
3.9.2 Rotations in R3                                                 Figure 3.2 Rotation                                                                                                               
                                                                                                                                            Here, the e1 coordinate is fixed, and  the  counterclockwise
In contrast to the R2                                                 of a vector (gray) in                                                 rotation is                                                 
                                                                                                                                                                                                        
case, in R3 we can rotate any two-dimensional plane                   R3 by an angle θ                                                      performed in the e2e3 plane.                                
                                                                                                                                                                                                        
about a one-dimensional axis. The easiest way to specify the          about the e3-axis.                                                    Rotation about the e2-axis                                  
general rotation matrix is to specify how the images of  the                                                                                                                                            
standard basis e1, e2, e3 are                                         The rotated vector is                                                 R2(θ) =                                                     
                                                                                                                                                                                                        
supposed to be rotated, and making sure  these  images  Re1,          shown in blue.                                                                                                                   
Re2, Re3 are                                                                                                                                                                                            
                                                                      e1                                                                                                                               
orthonormal to each other. We  can  then  obtain  a  general                                                                                                                                            
rotation matrix                                                       e2                                                                    cos θ 0 sin θ                                               
                                                                                                                                                                                                        
R by combining the images of the standard basis.                      e3                                                                    0 1 0                                                       
                                                                                                                                                                                                        
To have a meaningful rotation angle, we have to define  what          θ                                                                     −          sin          θ          0          cos          θ

%%%

                                                                     ).                                                                                                                               
                                                                                                                                                                                                        
 . (3.78)                                                            Definition 3.11 (Givens Rotation). Let V be an n-dimensional                                                                     
                                                                      Euclidean                                                                                                                         
If we rotate the e1e3 plane about the e2 axis,  we  need  to                                                                                                                                           
look at the e2                                                        vector  space  and  Φ  :  V  →  V   an   automorphism   with                                                                      
                                                                      transformation ma-                                                                                                               
axis from its “tip” toward the origin.                                                                                                                                                                  
                                                                      ©2021 M. P. Deisenroth, A. A. Faisal, C. S.  Ong.  Published                                                                     
Rotation about the e3-axis                                            by Cambridge University Press (2020).                                                                                             
                                                                                                                                            ∈ R                                                         
R3(θ) =                                                               94 Analytic Geometry                                                                                                              
                                                                                                                                            n×n                                                         
                                                                     trix                                                                                                                              
                                                                                                                                            , (3.80)                                                    
                                                                     Rij (θ) :=                                                                                                                        
                                                                                                                                            Givens rotation for 1 ⩽ i < j ⩽ n and θ ∈ R. Then Rij (θ) is
cos θ − sin θ 0                                                                                                                            called a Givens rotation.                                   
                                                                                                                                                                                                        
sin θ cos θ 0                                                                                                                              Essentially, Rij (θ) is the identity matrix In with         
                                                                                                                                                                                                        
0 0 1                                                                                                                                      rii = cos θ , rij = − sin θ , rji = sin θ , rjj =  cos  θ  .
                                                                                                                                            (3.81)                                                      
                                                                                                                                                                                                      
                                                                                                                                            In two dimensions (i.e., n =  2),  we  obtain  (3.76)  as  a
 . (3.79)                                                                                                                                 special case.                                               
                                                                                                                                                                                                        
Figure 3.2 illustrates this.                                                                                                               3.9.4 Properties of Rotations                               
                                                                                                                                                                                                        
3.9.3 Rotations in n Dimensions                                       Ii−1 0 · · · · · · 0                                                  Rotations exhibit a number of useful properties,  which  can
                                                                                                                                            be derived by                                               
The  generalization  of  rotations  from  2D   and   3D   to          0 cos θ 0 − sin θ 0                                                                                                               
n-dimensional Euclidean vector  spaces  can  be  intuitively                                                                                considering them as orthogonal matrices (Definition 3.8):   
described as fixing  n  −  2  dimensions  and  restrict  the          0 0 Ij−i−1 0 0                                                                                                                    
rotation to a two-dimensional  plane  in  the  n-dimensional                                                                                Rotations preserve distances, i.e., ∥x−y∥  =  ∥Rθ(x)−Rθ(y)∥.
space. As in the three-dimensional case, we can  rotate  any          0 sin θ 0 cos θ 0                                                     In other                                                    
plane                                                                                                                                                                                                   
                                                                      0 · · · · · · 0 In−j                                                  words, rotations leave the distance between any  two  points
(two-dimensional subspace of Rn                                                                                                             unchanged                                                   
                                                                                                                                                                                                       

%%%

after the transformation.                                             where each vector is orthogonal to  all  others  (orthogonal          kernel-PCA (Scholkopf et al. ¨ ,  1997)  for  dimensionality
                                                                      bases) using the                                                      reduction. Gaussian processes (Rasmussen and Williams, 2006)
Rotations preserve angles, i.e., the angle between  Rθx  and                                                                                also fall into the category                                 
Rθy equals                                                            Gram-Schmidt  method.   These   bases   are   important   in                                                                      
                                                                      optimization and                                                      of kernel methods and are the current state of  the  art  in
the angle between x and y.                                                                                                                  probabilistic regression (fitting curves  to  data  points).
                                                                      numerical algorithms for solving  linear  equation  systems.          The idea of kernels is explored                             
Rotations in three (or more) dimensions  are  generally  not          For instance,                                                                                                                     
commutative. Therefore, the order  in  which  rotations  are                                                                                further in Chapter 12.                                      
applied is important,                                                 Krylov subspace methods, such as conjugate gradients or  the                                                                      
                                                                      generalized                                                           Projections are often used in computer  graphics,  e.g.,  to
even if they rotate  about  the  same  point.  Only  in  two                                                                                generate shadows. In  optimization,  orthogonal  projections
dimensions vector                                                     minimal residual method (GMRES),  minimize  residual  errors          are often used to (iteratively)                             
                                                                      that are orthogonal to  each  other  (Stoer  and  Burlirsch,                                                                      
rotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for          2002).                                                                minimize residual errors.  This  also  has  applications  in
all                                                                                                                                         machine learning,                                           
                                                                      In machine learning, inner products  are  important  in  the                                                                      
ϕ,  θ  ∈  [0,  2π).  They  form  an  Abelian   group   (with          context of                                                            e.g., in linear regression where we want to find a  (linear)
multiplication) only if                                                                                                                     function that                                               
                                                                      Draft (2023-02-15) of “Mathematics  for  Machine  Learning”.                                                                      
they rotate about the same point (e.g., the origin).                  Feedback: https://mml-book.com.                                       minimizes the residual errors,  i.e.,  the  lengths  of  the
                                                                                                                                            orthogonal projections of the data onto the linear  function
3.10 Further Reading                                                  3.10 Further Reading 95                                               (Bishop, 2006). We will investigate this further in  Chapter
                                                                                                                                            9. PCA (Pearson, 1901; Hotelling, 1933) also                
In this chapter, we gave a brief overview  of  some  of  the          kernel methods  (Scholkopf  and  Smola  ¨  ,  2002).  Kernel                                                                      
important concepts                                                    methods exploit the                                                   uses   projections   to   reduce   the   dimensionality   of
                                                                                                                                            high-dimensional data.                                      
of analytic geometry, which we will use in later chapters of          fact that many linear algorithms can be expressed purely  by                                                                      
the book.                                                             inner product computations. Then, the “kernel trick”  allows          We will discuss this in more detail in Chapter 10.          
                                                                      us to compute these                                         
For a broader and more in-depth  overview  of  some  of  the                                                                      
concepts we                                                           inner    products    implicitly    in     a     (potentially
                                                                      infinite-dimensional) feature                               
presented, we refer to the following excellent books:  Axler                                                                      
(2015) and                                                            space, without even knowing this feature  space  explicitly.
                                                                      This allowed the                                            
Boyd and Vandenberghe (2018).                                                                                                     
                                                                      “non-linearization”  of  many  algorithms  used  in  machine
Inner products allow  us  to  determine  specific  bases  of          learning, such as                                           
vector                                          (sub)spaces,                                                                      
