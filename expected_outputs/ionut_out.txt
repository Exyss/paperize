Contents                                                              5.7 Higher-Order Derivatives 164                                      Part II Central Machine Learning Problems 249               
                                                                                                                                                                                                        
Foreword 1                                                            5.8 Linearization and Multivariate Taylor Series 165                  8 When Models Meet Data 251                                 
                                                                                                                                                                                                        
Part I Mathematical Foundations 9                                     5.9 Further Reading 170                                               8.1 Data, Models, and Learning 251                          
                                                                                                                                                                                                        
1 Introdu                                                             Exercises 170                                                         8.2 Empirical Risk Minimization 258                         
                                                                                                                                                                                                        
ii Contents                                                           6 Probability and Distributions 172                                   8.3 Parameter Estimation 265                                
                                                                                                                                                                                                        
4.2 Eigenvalues and Eigenvectors 105                                  6.1 Construction of a Probability Space 172                           8.4 Probabilistic Modeling and Inference 272                
                                                                                                                                                                                                        
4.3 Cholesky Decomposition 114                                        6.2 Discrete and Continuous Probabilities 178                         8.5 Directed Graphical Models 278                           
                                                                                                                                                                                                        
4.4 Eigendecomposition and Diagonalization 115                        6.3 Sum Rule, Product Rule, and Bayes’ Theorem 183                    Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
4.5 Singular Value Decomposition 119                                  6.4 Summary Statistics and Independence 186                                                                                       
                                                                                                                                            Contents iii                                                
4.6 Matrix Approximation 129                                          6.5 Gaussian Distribution 197                                                                                                     
                                                                                                                                            8.6 Model Selection 283                                     
4.7 Matrix Phylogeny 134                                              6.6 Conjugacy and the Exponential Family 205                                                                                      
                                                                                                                                            9 Linear Regression 289                                     
4.8 Further Reading 135                                               6.7 Change of Variables/Inverse Transform 214                                                                                     
                                                                                                                                            9.1 Problem Formulation 291                                 
Exercises 137                                                         6.8 Further Reading 221                                                                                                           
                                                                                                                                            9.2 Parameter Estimation 292                                
5 Vector Calculus 139                                                 Exercises 221                                                                                                                     
                                                                                                                                            9.3 Bayesian Linear Regression 303                          
5.1 Differentiation of Univariate Functions 141                       7 Continuous Optimization 225                                                                                                     
                                                                                                                                            9.4 Maximum Likelihood as Orthogonal Projection 313         
5.2 Partial Differentiation and Gradients 146                         7.1 Optimization Using Gradient Descent 227                                                                                       
                                                                                                                                            9.5 Further Reading 315                                     
5.3 Gradients of Vector-Valued Functions 149                          7.2 Constrained Optimization and Lagrange Multipliers 233                                                                         
                                                                                                                                            10   Dimensionality   Reduction   with  Principal  Component
5.4 Gradients of Matrices 155                                         7.3 Convex Optimization 236                                           Analysis 317                                                
                                                                                                                                                                                                        
5.5 Useful Identities for Computing Gradients 158                     7.4 Further Reading 246                                               10.1 Problem Setting 318                                    
                                                                                                                                                                                                        
5.6 Backpropagation and Automatic Differentiation 159                 Exercises 247                                                         10.2 Maximum Variance Perspective 320                       
                                                                                                                                                                                                        

%%%

10.3 Projection Perspective 325                                       ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          tend to spend                                               
                                                                      by Cambridge University Press (2020).                                                                                             
10.4 Eigenvector Computation and Low-Rank Approximations 333                                                                                early   parts   of   the   course  covering  some  of  these
                                                                      Foreword                                                              pre-requisites.  For  historical reasons, courses in machine
10.5 PCA in High Dimensions 335                                                                                                             learning tend to be taught in the computer                  
                                                                      Machine learning is the latest in a long line of attempts to                                                                      
10.6 Key Steps of PCA in Practice 336                                 distill human                                                         science  department, where students are often trained in the
                                                                                                                                            first two areas                                             
10.7 Latent Variable Perspective 339                                  knowledge  and  reasoning  into  a form that is suitable for                                                                      
                                                                      constructing  machines and engineering automated systems. As          of knowledge, but not so much in mathematics and statistics.
10.8 Further Reading 343                                              machine learning becomes                                                                                                          
                                                                                                                                            Current   machine  learning  textbooks  primarily  focus  on
11 Density Estimation with Gaussian Mixture Models 348                more  ubiquitous  and its software packages become easier to          machine  learning  algorithms  and  methodologies and assume
                                                                      use,   it  is  natural  and  desirable  that  the  low-level          that  the reader is competent in mathematics and statistics.
11.1 Gaussian Mixture Model 349                                       technical details are abstracted away                                 Therefore, these books only spend                           
                                                                                                                                                                                                        
11.2 Parameter Learning via Maximum Likelihood 350                    and  hidden from the practitioner. However, this brings with          one or two chapters on background mathematics, either at the
                                                                      it the danger                                                         beginning                                                   
11.3 EM Algorithm 360                                                                                                                                                                                   
                                                                      that  a practitioner becomes unaware of the design decisions          of  the book or as appendices. We have found many people who
11.4 Latent-Variable Perspective 363                                  and, hence,                                                           want to                                                     
                                                                                                                                                                                                        
11.5 Further Reading 368                                              the limits of machine learning algorithms.                            delve into the foundations of basic machine learning methods
                                                                                                                                            who  struggle  with  the  mathematical knowledge required to
12 Classification with Support Vector Machines 370                    The  enthusiastic  practitioner  who  is interested to learn          read a machine learning                                     
                                                                      more about the                                                                                                                    
12.1 Separating Hyperplanes 372                                                                                                             textbook.  Having  taught undergraduate and graduate courses
                                                                      magic   behind   successful   machine   learning  algorithms          at  universities,  we  find that the gap between high school
12.2 Primal Support Vector Machine 374                                currently faces a                                                     mathematics  and  the  mathematics  level required to read a
                                                                                                                                            standard machine learning textbook is too                   
12.3 Dual Support Vector Machine 383                                  daunting set of pre-requisite knowledge:                                                                                          
                                                                                                                                            big for many people.                                        
12.4 Kernels 388                                                      Programming languages and data analysis tools                                                                                     
                                                                                                                                            This  book  brings  the  mathematical  foundations  of basic
12.5 Numerical Solution 390                                           Large-scale computation and the associated frameworks                 machine  learning  concepts  to  the  fore  and collects the
                                                                                                                                            information in a single place so                            
12.6 Further Reading 392                                              Mathematics  and  statistics and how machine learning builds                                                                      
                                                                      on it                                                                 that this skills gap is narrowed or even closed.            
References 395                                                                                                                                                                                          
                                                                      At  universities,  introductory  courses on machine learning          1                                                           

%%%

                                                                      “Math  is  linked  in  erature that forms the foundations of          Ben-David, 2014; Rogers                                     
This  material is published by Cambridge University Press as          modern machine learning. We mothe popular mind                                                                                    
Mathematics for Machine Learning by                                                                                                         and  Girolami,  2016)  or  programmatic  aspects  of machine
                                                                      with phobia and                                                       learning (Muller ¨                                          
Marc  Peter  Deisenroth,  A. Aldo Faisal, and Cheng Soon Ong                                                                                                                                            
(2020). This version is free to view                                  anxiety. You’d think                                                  and  Guido,  2016;  Raschka and Mirjalili, 2017; Chollet and
                                                                                                                                            Allaire, 2018),                                             
and download for personal use only. Not for re-distribution,          we’re discussing                                                                                                                  
re-sale, or use in derivative works.                                                                                                        we  provide  only  four  representative  examples of machine
                                                                      spiders.” (Strogatz,                                                  learning  algorithms.  Instead, we focus on the mathematical
©by  M.  P.  Deisenroth,  A. A. Faisal, and C. S. Ong, 2021.                                                                                concepts behind the models                                  
https://mml-book.com.                                                 2014, page 281)                                                                                                                   
                                                                                                                                            themselves.  We  hope  that  readers  will be able to gain a
2 Foreword                                                            tivate  the  need  for  mathematical  concepts  by  directly          deeper  understanding  of  the  basic  questions  in machine
                                                                      pointing out their                                                    learning  and  connect  practical questions arising from the
Why Another Book on Machine Learning?                                                                                                       use of machine learning with fundamental choices            
                                                                      usefulness  in  the  context of fundamental machine learning                                                                      
Machine  learning builds upon the language of mathematics to          problems. In                                                          in the mathematical model.                                  
express                                                                                                                                                                                                 
                                                                      the  interest  of  keeping  the book short, many details and          We  do  not  aim to write a classical machine learning book.
concepts   that   seem  intuitively  obvious  but  that  are          more advanced                                                         Instead, our                                                
surprisingly difficult                                                                                                                                                                                  
                                                                      concepts  have  been  left  out.  Equipped  with  the  basic          intention is to provide the mathematical background, applied
to formalize. Once formalized properly, we can gain insights          concepts presented                                                    to four central machine learning problems, to make it easier
into the task                                                                                                                               to read other machine                                       
                                                                      here,  and  how  they fit into the larger context of machine                                                                      
we  want  to  solve.  One  common  complaint  of students of          learning, the                                                         learning textbooks.                                         
mathematics                                                                                                                                                                                             
                                                                      reader  can find numerous resources for further study, which          Who Is the Target Audience?                                 
around  the  globe  is  that the topics covered seem to have          we provide at                                                                                                                     
little relevance                                                                                                                            As  applications  of  machine  learning become widespread in
                                                                      the  end  of  the  respective  chapters.  For readers with a          society, we                                                 
to  practical  problems. We believe that machine learning is          mathematical  background,  this  book  provides  a brief but                                                                      
an obvious and                                                        precisely stated glimpse of machine                                   believe that everybody should have some understanding of its
                                                                                                                                            underlying                                                  
direct motivation for people to learn mathematics.                    learning.  In  contrast to other books that focus on methods                                                                      
                                                                      and models                                                            principles. This book is written in an academic mathematical
This  book  is  intended  to  be  a  guidebook  to  the vast                                                                                style, which                                                
mathematical lit-                                                     of  machine  learning (MacKay, 2003; Bishop, 2006; Alpaydin,                                                                      
                                                                      2010;   Barber,   2012;  Murphy,  2012;  Shalev-Shwartz  and          enables  us  to be precise about the concepts behind machine

%%%

learning. We                                                                                                                                to their audience. Using the mathematics presented here as a
                                                                      Astute  Listener  The democratization of machine learning by          primer,  practitioners  would  be  able  to  understand  the
encourage readers unfamiliar with this seemingly terse style          the  provision of open-source software, online tutorials and          benefits  and limits of their favorite method, and to extend
to persevere                                                          cloud-based  tools  allows  users  to  not  worry  about the          and                                                         
                                                                      specifics of pipelines. Users can focus on                                                                                        
and  to  keep  the  goals of each topic in mind. We sprinkle                                                                                generalize  existing  machine  learning  algorithms. We hope
comments and                                                          extracting  insights  from  data  using off-the-shelf tools.          that this book                                              
                                                                      This  enables  nontech-savvy  domain experts to benefit from                                                                      
remarks  throughout  the  text, in the hope that it provides          machine learning. This is similar to listening to music; the          provides  the  impetus  for  more  rigorous  and  principled
useful guidance                                                       user is able to choose and discern between                            development of                                              
                                                                                                                                                                                                        
with respect to the big picture.                                      different  types  of machine learning, and benefits from it.          machine learning methods.                                   
                                                                      More  experienced  users  are  like  music  critics,  asking                                                                      
The  book  assumes the reader to have mathematical knowledge          important questions about the                                         Fledgling  Composer  As  machine  learning is applied to new
commonly                                                                                                                                    domains,                                                    
                                                                      application  of  machine learning in society such as ethics,                                                                      
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          fairness,  and  privacy of the individual. We hope that this          developers  of  machine learning need to develop new methods
Feedback: https://mml-book.com.                                       book provides a foundation for                                        and extend                                                  
                                                                                                                                                                                                        
Foreword 3                                                            thinking  about  the  certification  and  risk management of          existing  algorithms. They are often researchers who need to
                                                                      machine learning                                                      understand                                                  
covered in high school mathematics and physics. For example,                                                                                                                                            
the reader                                                            systems,  and  allows  them to use their domain expertise to          the  mathematical  basis  of  machine  learning  and uncover
                                                                      build better                                                          relationships  between  different  tasks. This is similar to
should  have  seen  derivatives  and  integrals  before, and                                                                                composers of music who, within                              
geometric vectors                                                     machine learning systems.                                                                                                         
                                                                                                                                            the  rules  and  structure of musical theory, create new and
in   two  or  three  dimensions.  Starting  from  there,  we          Experienced Artist Skilled practitioners of machine learning          amazing pieces.                                             
generalize these concepts. Therefore, the target audience of          can plug                                                                                                                          
the book includes undergraduate                                                                                                             We  hope  this  book provides a high-level overview of other
                                                                      and  play  different  tools  and  libraries into an analysis          technical books                                             
university   students,   evening   learners   and   learners          pipeline.  The  stereotypical  practitioner  would be a data                                                                      
participating in online                                               scientist or engineer who understands                                 for people who want to become composers of machine learning.
                                                                                                                                            There is                                                    
machine learning courses.                                             machine learning interfaces and their use cases, and is able                                                                      
                                                                      to perform                                                            a  great need in society for new researchers who are able to
In  analogy  to  music, there are three types of interaction                                                                                propose and                                                 
that people                                                           wonderful  feats of prediction from data. This is similar to                                                                      
                                                                      a virtuoso playing music, where highly skilled practitioners          explore  novel  approaches for attacking the many challenges
have with machine learning:                                           can  bring  existing instruments to life and bring enjoyment          of learning                                                 

%%%

                                                                                                                                                                                                        
from data.                                                            Abdul-Ganiy Usman                                                     Chao Qu                                                     
                                                                                                                                                                                                        
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          Adam Gaier                                                            Cheng Li                                                    
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      Adele Jackson                                                         Chris Sherlock                                              
4 Foreword                                                                                                                                                                                              
                                                                      Aditya Menon                                                          Christopher Gray                                            
Acknowledgments                                                                                                                                                                                         
                                                                      Alasdair Tran                                                         Daniel McNamara                                             
We are grateful to many people who looked at early drafts of                                                                                                                                            
the book                                                              Aleksandar Krnjaic                                                    Daniel Wood                                                 
                                                                                                                                                                                                        
and  suffered  through  painful  expositions of concepts. We          Alexander Makrigiorgos                                                Darren Siegel                                               
tried  to  implement  their ideas that we did not vehemently                                                                                                                                            
disagree with. We would                                               Alfredo Canziani                                                      David Johnston                                              
                                                                                                                                                                                                        
like  to  especially  acknowledge Christfried Webers for his          Ali Shafti                                                            Dawei Chen                                                  
careful reading                                                                                                                                                                                         
                                                                      Amr Khalifa                                                           Ellen Broad                                                 
of  many  parts of the book, and his detailed suggestions on                                                                                                                                            
structure and                                                         Andrew Tanggara                                                       Fengkuangtian Zhu                                           
                                                                                                                                                                                                        
presentation.  Many  friends  and  colleagues have also been          Angus Gruen                                                           Fiona Condon                                                
kind enough                                                                                                                                                                                             
                                                                      Antal A. Buss                                                         Georgios Theodorou                                          
to  provide  their  time and energy on different versions of                                                                                                                                            
each chapter.                                                         Antoine Toisoul Le Cann                                               He Xin                                                      
                                                                                                                                                                                                        
We  have  been  lucky  to benefit from the generosity of the          Areg Sarvazyan                                                        Irene Raissa Kameni                                         
online   community,  who  have  suggested  improvements  via                                                                                                                                            
https://github.com, which                                             Artem Artemev                                                         Jakub Nabaglo                                               
                                                                                                                                                                                                        
greatly improved the book.                                            Artyom Stepanov                                                       James Hensman                                               
                                                                                                                                                                                                        
The    following    people   have   found   bugs,   proposed          Bill Kromydas                                                         Jamie Liu                                                   
clarifications and suggested relevant literature, either via                                                                                                                                            
https://github.com or personal                                        Bob Williamson                                                        Jean Kaddour                                                
                                                                                                                                                                                                        
communication. Their names are sorted alphabetically.                 Boon Ping Lim                                                         Jean-Paul Ebejer                                            

%%%

                                                                      Foreword 5                                                            Ryan-Rhys Griffiths                                         
Jerry Qiang                                                                                                                                                                                             
                                                                      Maximus McCann                                                        Salomon Kabongo                                             
Jitesh Sindhare                                                                                                                                                                                         
                                                                      Mengyan Zhang                                                         Samuel Ogunmola                                             
John Lloyd                                                                                                                                                                                              
                                                                      Michael Bennett                                                       Sandeep Mavadia                                             
Jonas Ngnawe                                                                                                                                                                                            
                                                                      Michael Pedersen                                                      Sarvesh Nikumbh                                             
Jon Martin                                                                                                                                                                                              
                                                                      Minjeong Shin                                                         Sebastian Raschka                                           
Justin Hsi                                                                                                                                                                                              
                                                                      Mohammad Malekzadeh                                                   Senanayak Sesh Kumar Karri                                  
Kai Arulkumaran                                                                                                                                                                                         
                                                                      Naveen Kumar                                                          Seung-Heon Baek                                             
Kamil Dreczkowski                                                                                                                                                                                       
                                                                      Nico Montali                                                          Shahbaz Chaudhary                                           
Lily Wang                                                                                                                                                                                               
                                                                      Oscar Armas                                                           Shakir Mohamed                                              
Lionel Tondji Ngoupeyou                                                                                                                                                                                 
                                                                      Patrick Henriksen                                                     Shawn Berry                                                 
Lydia Knufing ¨                                                                                                                                                                                         
                                                                      Patrick Wieschollek                                                   Sheikh Abdul Raheem Ali                                     
Mahmoud Aslan                                                                                                                                                                                           
                                                                      Pattarawat Chormai                                                    Sheng Xue                                                   
Mark Hartenstein                                                                                                                                                                                        
                                                                      Paul Kelly                                                            Sridhar Thiagarajan                                         
Mark van der Wilk                                                                                                                                                                                       
                                                                      Petros Christodoulou                                                  Syed Nouman Hasany                                          
Markus Hegland                                                                                                                                                                                          
                                                                      Piotr Januszewski                                                     Szymon Brych                                                
Martin Hewing                                                                                                                                                                                           
                                                                      Pranav Subramani                                                      Thomas Buhler ¨                                             
Matthew Alger                                                                                                                                                                                           
                                                                      Quyu Kong                                                             Timur Sharapov                                              
Matthew Lee                                                                                                                                                                                             
                                                                      Ragib Zaman                                                           Tom Melamed                                                 
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                       Rui Zhang                                                             Vincent Adam                                                
                                                                                                                                                                                                        

%%%

Vincent Dutordoir                                                                                                                                                                                       
                                                                      idoamihai                                                             6 Foreword                                                  
Vu Minh                                                                                                                                                                                                 
                                                                      deepakiim                                                             Table of Symbols                                            
Wasim Aftab                                                                                                                                                                                             
                                                                      insad                                                                 Symbol Typical meaning                                      
Wen Zhi                                                                                                                                                                                                 
                                                                      HorizonP                                                              a, b, c, α, β, γ Scalars are lowercase                      
Wojciech Stokowiec                                                                                                                                                                                      
                                                                      cs-maillist                                                           x, y, z Vectors are bold lowercase                          
Xiaonan Chong                                                                                                                                                                                           
                                                                      kudo23                                                                A, B, C Matrices are bold uppercase                         
Xiaowei Zhang                                                                                                                                                                                           
                                                                      empet                                                                 x                                                           
Yazhou Hao                                                                                                                                                                                              
                                                                      victorBigand                                                          ⊤, A                                                        
Yicheng Luo                                                                                                                                                                                             
                                                                      17SKYE                                                                ⊤                                                           
Young Lee                                                                                                                                                                                               
                                                                      jessjing1995                                                          Transpose of a vector or matrix                             
Yu Lu                                                                                                                                                                                                   
                                                                      We are also very grateful to Parameswaran Raman and the many          A                                                           
Yun Cheng                                                             anonymous   reviewers,  organized  by  Cambridge  University                                                                      
                                                                      Press, who read one                                                   −1                                                          
Yuxiao Huang                                                                                                                                                                                            
                                                                      or  more chapters of earlier versions of the manuscript, and          Inverse of a matrix                                         
Zac Cranko                                                            provided  constructive  criticism  that  led to considerable                                                                      
                                                                      improvements.  A  special mention goes to Dinesh Singh Negi,          ⟨x, y⟩ Inner product of x and y                             
Zijian Cao                                                            our LATEX support, for detailed and prompt                                                                                        
                                                                                                                                            x                                                           
Zoe Nolan                                                             advice  about  LATEX-related  issues. Last but not least, we                                                                      
                                                                      are very grateful                                                     ⊤y Dot product of x and y                                   
Contributors  through  GitHub,  whose  real  names  were not                                                                                                                                            
listed on their                                                       to  our editor Lauren Cowles, who has been patiently guiding          B = (b1, b2, b3) (Ordered) tuple                            
                                                                      us through                                                                                                                        
GitHub profile, are:                                                                                                                        B   =   [b1,  b2,  b3]  Matrix  of  column  vectors  stacked
                                                                      the gestation process of this book.                                   horizontally                                                
SamDataMad                                                                                                                                                                                              
                                                                      ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          B = {b1, b2, b3} Set of vectors (unordered)                 
bumptiousmonkey                                                       by Cambridge University Press (2020).                                                                                             

%%%

Z, N Integers and natural numbers, respectively                       ei  Standard/canonical vector (where i is the component that          PN                                                          
                                                                      is 1)                                                                                                                             
R, C Real and complex numbers, respectively                                                                                                 Qn=1 xn Sum of the xn: x1 + . . . + xN                      
                                                                      dim Dimensionality of vector space                                                                                                
Rn n-dimensional vector space of real numbers                                                                                               N                                                           
                                                                      rk(A) Rank of matrix A                                                                                                            
∀x Universal quantifier: for all x                                                                                                          n=1 xn Product of the xn: x1 · . . . · xN                   
                                                                      Im(Φ) Image of linear mapping Φ                                                                                                   
∃x Existential quantifier: there exists x                                                                                                   θ Parameter vector                                          
                                                                      ker(Φ) Kernel (null space) of a linear mapping Φ                                                                                  
a := b a is defined as b                                                                                                                    ∂f                                                          
                                                                      span[b1] Span (generating set) of b1                                                                                              
a =: b b is defined as a                                                                                                                    ∂x Partial derivative of f with respect to x                
                                                                      tr(A) Trace of A                                                                                                                  
a ∝ b a is proportional to b, i.e., a = constant · b                                                                                        df                                                          
                                                                      det(A) Determinant of A                                                                                                           
g ◦ f Function composition: “g after f”                                                                                                     dx                                                          
                                                                      | · | Absolute value or determinant (depending on context)                                                                        
⇐⇒ If and only if                                                                                                                           Total derivative of f with respect to x                     
                                                                      ∥·∥ Norm; Euclidean, unless specified                                                                                             
=⇒ Implies                                                                                                                                  ∇ Gradient                                                  
                                                                      λ Eigenvalue or Lagrange multiplier                                                                                               
A, C Sets                                                                                                                                   f∗ = minx f(x) The smallest function value of f             
                                                                      Eλ Eigenspace corresponding to eigenvalue λ                                                                                       
a ∈ A a is an element of set A                                                                                                              x∗  ∈ arg minx f(x) The value x∗ that minimizes f (note: arg
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          min returns a set of values)                                
∅ Empty set                                                           Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                            L Lagrangian                                                
A\B A without B: the set of elements in A but not in B                Foreword 7                                                                                                                        
                                                                                                                                            L Negative log-likelihood                                   
D Number of dimensions; indexed by d = 1, . . . , D                   Symbol Typical meaning                                                                                                            
                                                                                                                                            n                                                           
N Number of data points; indexed by n = 1, . . . , N                  x ⊥ y Vectors x and y are orthogonal                                                                                              
                                                                                                                                            k                                                           
Im Identity matrix of size m × m                                      V Vector space                                                                                                                    
                                                                                                                                                                                                       
0m,n Matrix of zeros of size m × n                                    V                                                                                                                                 
                                                                                                                                            Binomial coefficient, n choose k                            
1m,n Matrix of ones of size m × n                                     ⊥ Orthogonal complement of vector space V                                                                                         
                                                                                                                                            VX[x] Variance of x with respect to the random variable X   

%%%

                                                                                                                                            “automatic”, i.e.,                                          
EX[x] Expectation of x with respect to the random variable X          PCA Principal component analysis                                                                                                  
                                                                                                                                            machine   learning   is   concerned   about  general-purpose
CovX,Y [x, y] Covariance between x and y.                             PPCA Probabilistic principal component analysis                       methodologies that                                          
                                                                                                                                                                                                        
X ⊥⊥ Y |Z X is conditionally independent of Y given Z                 REF Row-echelon form                                                  can  be  applied to many datasets, while producing something
                                                                                                                                            that is meaningful. There are three concepts that are at the
X ∼ p Random variable X is distributed according to p                 SPD Symmetric, positive definite                                      core of machine learning:                                   
                                                                                                                                                                                                        
N                                                                     SVM Support vector machine                                            data, a model, and learning.                                
                                                                                                                                                                                                        
µ, Σ                                                                  ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          Since machine learning is inherently data driven, data is at
                                                                      by Cambridge University Press (2020                                   the core data                                               
                                                                                                                                                                                                       
                                                                      Part I                                                                of  machine  learning.  The  goal  of machine learning is to
Gaussian distribution with mean µ and covariance Σ                                                                                          design  generalpurpose  methodologies  to  extract  valuable
                                                                      Mathematical Foundations                                              patterns from data, ideally                                 
Ber(µ) Bernoulli distribution with parameter µ                                                                                                                                                          
                                                                      9                                                                     without much domain-specific expertise. For example, given a
Bin(N, µ) Binomial distribution with parameters N, µ                                                                                        large corpus                                                
                                                                      This  material is published by Cambridge University Press as                                                                      
Beta(α, β) Beta distribution with parameters α, β                     Mathematics for Machine Learning by                                   of  documents  (e.g.,  books  in  many  libraries),  machine
                                                                                                                                            learning methods                                            
Table of Abbreviations and Acronyms                                   Marc  Peter  Deisenroth,  A. Aldo Faisal, and Cheng Soon Ong                                                                      
                                                                      (2020). This version is free to view                                  can  be  used to automatically find relevant topics that are
Acronym Meaning                                                                                                                             shared across                                               
                                                                      and download for personal use only. Not for re-distribution,                                                                      
e.g. Exempli gratia (Latin: for example)                              re-sale, or use in derivative works.                                  documents  (Hoffman  et al., 2010). To achieve this goal, we
                                                                                                                                            design models that are typically related to the process that
GMM Gaussian mixture model                                            ©by  M.  P.  Deisenroth,  A. A. Faisal, and C. S. Ong, 2021.          generates data, similar to model                            
                                                                      https://mml-book.com.                                                                                                             
i.e. Id est (Latin: this means)                                                                                                             the  dataset  we  are  given.  For  example, in a regression
                                                                      1                                                                     setting, the model                                          
i.i.d. Independent, identically distributed                                                                                                                                                             
                                                                      Introduction and Motivation                                           would  describe  a  function that maps inputs to real-valued
MAP Maximum a posteriori                                                                                                                    outputs. To                                                 
                                                                      Machine   learning   is   about  designing  algorithms  that                                                                      
MLE Maximum likelihood estimation/estimator                           automatically extract                                                 paraphrase  Mitchell  (1997):  A model is said to learn from
                                                                                                                                            data  if  its performance on a given task improves after the
ONB Orthonormal basis                                                 valuable  information  from  data.  The  emphasis here is on          data is taken into account.                                 

%%%

                                                                      and download for personal use only. Not for re-distribution,          mean  different  things.  However,  we  attempt  to make the
The  goal is to find good models that generalize well to yet          re-sale, or use in derivative works.                                  context sufficiently clear to reduce the level of ambiguity.
unseen data,                                                                                                                                                                                            
                                                                      ©by  M.  P.  Deisenroth,  A. A. Faisal, and C. S. Ong, 2021.          The  first  part  of  this  book introduces the mathematical
which  we  may  care  about  in  the future. Learning can be          https://mml-book.com.                                                 concepts and                                                
understood as a learning                                                                                                                                                                                
                                                                      12 Introduction and Motivation                                        foundations  needed  to talk about the three main components
way  to automatically find patterns and structure in data by                                                                                of a machine                                                
optimizing the                                                        1.1 Finding Words for Intuitions                                                                                                  
                                                                                                                                            learning system: data, models, and learning. We will briefly
parameters of the model.                                              A  challenge  we  face regularly in machine learning is that          outline these                                               
                                                                      concepts and                                                                                                                      
While  machine  learning  has seen many success stories, and                                                                                components here, and we will revisit them again in Chapter 8
software is                                                           words  are  slippery,  and  a  particular  component  of the          once we                                                     
                                                                      machine learning                                                                                                                  
readily  available  to  design  and  train rich and flexible                                                                                have discussed the necessary mathematical concepts.         
machine learning                                                      system can be abstracted to different mathematical concepts.                                                                      
                                                                      For example,                                                          While  not  all  data  is  numerical,  it is often useful to
systems,  we  believe  that  the mathematical foundations of                                                                                consider data in                                            
machine  learning  are  important  in  order  to  understand          the  word  “algorithm”  is  used  in  at least two different                                                                      
fundamental principles upon                                           senses  in  the  context  of  machine learning. In the first          a  number  format.  In  this  book,  we assume that data has
                                                                      sense, we use the phrase “machine                                     already been                                                
which  more  complicated machine learning systems are built.                                                                                                                                            
Understanding  these  principles can facilitate creating new          learning  algorithm” to mean a system that makes predictions          appropriately  converted  into  a  numerical  representation
machine learning solutions,                                           based  on inpredictor put data. We refer to these algorithms          suitable  for  readdata  as  vectors  ing  into  a  computer
                                                                      as predictors. In the second sense,                                   program. Therefore, we think of data as vectors. As         
understanding   and   debugging   existing  approaches,  and                                                                                                                                            
learning about the                                                    we use the exact same phrase “machine learning algorithm” to          another  illustration of how subtle words are, there are (at
                                                                      mean a                                                                least) three                                                
inherent assumptions and limitations of the methodologies we                                                                                                                                            
are working with.                                                     system that adapts some internal parameters of the predictor          different  ways to think about vectors: a vector as an array
                                                                      so that it                                                            of numbers (a                                               
11                                                                                                                                                                                                      
                                                                      performs  well on future unseen input data. Here we refer to          computer  science  view),  a  vector  as  an  arrow  with  a
This  material is published by Cambridge University Press as          this adaptatraining tion as training a system.                        direction and magnitude (a physics view), and a vector as an
Mathematics for Machine Learning by                                                                                                         object that obeys addition and                              
                                                                      This  book  will  not resolve the issue of ambiguity, but we                                                                      
Marc  Peter  Deisenroth,  A. Aldo Faisal, and Cheng Soon Ong          want  to  highlight  upfront that, depending on the context,          scaling (a mathematical view).                              
(2020). This version is free to view                                  the same expressions can                                                                                                          
                                                                                                                                            model  A  model  is typically used to describe a process for

%%%

generating  data, similar to the dataset at hand. Therefore,          interested in                                                         Bottom-up:  Building  up  the  concepts from foundational to
good models can also be thought                                                                                                             more  advanced. This is often the preferred approach in more
                                                                      the model to perform well on unseen data. Performing well on          technical fields,                                           
of   as   simplified   versions   of   the   real  (unknown)          data that                                                                                                                         
data-generating process,                                                                                                                    such  as  mathematics.  This strategy has the advantage that
                                                                      we  have  already seen (training data) may only mean that we          the reader                                                  
capturing  aspects  that  are relevant for modeling the data          found a                                                                                                                           
and extracting                                                                                                                              at  all  times  is  able to rely on their previously learned
                                                                      good  way  to  memorize  the  data.  However,  this  may not          concepts.  Unfortunately,  for  a  practitioner  many of the
hidden  patterns  from  it. A good model can then be used to          generalize well to                                                    foundational concepts are not                               
predict what                                                                                                                                                                                            
                                                                      unseen  data,  and, in practical applications, we often need          particularly  interesting  by  themselves,  and  the lack of
would happen in the real world without performing real-world          to expose our                                                         motivation means                                            
experiments.                                                                                                                                                                                            
                                                                      machine  learning  system  to  situations  that  it  has not          that most foundational definitions are quickly forgotten.   
learning We now come to the crux of the matter, the learning          encountered before.                                                                                                               
component of                                                                                                                                Top-down:  Drilling  down from practical needs to more basic
                                                                      Let  us summarize the main concepts of machine learning that          requirements.  This  goal-driven  approach has the advantage
machine  learning.  Assume  we  are  given  a  dataset and a          we cover                                                              that the readers                                            
suitable model.                                                                                                                                                                                         
                                                                      in this book:                                                         know  at  all  times  why  they need to work on a particular
Training  the  model  means  to  use  the  data available to                                                                                concept, and                                                
optimize  some  parameters  of  the  model with respect to a          We represent data as vectors.                                                                                                     
utility function that evaluates how                                                                                                         there is a clear path of required knowledge. The downside of
                                                                      We   choose   an   appropriate   model,   either  using  the          this  strategy is that the knowledge is built on potentially
well  the  model  predicts  the training data. Most training          probabilistic or optimization view.                                   shaky foundations, and                                      
methods can be                                                                                                                                                                                          
                                                                      We learn from available data by using numerical optimization          the readers have to remember a set of words that they do not
thought  of  as  an approach analogous to climbing a hill to          methods                                                               have any                                                    
reach its peak.                                                                                                                                                                                         
                                                                      with  the  aim that the model performs well on data not used          way of understanding.                                       
In  this  analogy,  the  peak  of  the hill corresponds to a          for training.                                                                                                                     
maximum of some                                                                                                                             We  decided  to write this book in a modular way to separate
                                                                      1.2 Two Ways to Read This Book                                        foundational                                                
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                       We   can  consider  two  strategies  for  understanding  the          (mathematical)  concepts from applications so that this book
                                                                      mathematics for                                                       can be read                                                 
1.2 Two Ways to Read This Book 13                                                                                                                                                                       
                                                                      machine learning:                                                     in both ways. The book is split into two parts, where Part I
desired  performance  measure.  However, in practice, we are                                                                                lays  the  mathematical  foundations and Part II applies the

%%%

concepts from Part I to a set                                                                                                               linear   algebra  which  we  introduce  in  Chapter  2.  The
                                                                      Estimation                                                            collection of vectors as a matrix is                        
of  fundamental  machine  learning problems, which form four                                                                                                                                            
pillars of                                                            Reduction                                                             also described there.                                       
                                                                                                                                                                                                        
machine  learning  as illustrated in Figure 1.2: regression,          Machine Learning                                                      Given  two  vectors  representing  two  objects  in the real
dimensionality                                                                                                                              world, we want                                              
                                                                      Vector Calculus Probability & Distributions Optimization                                                                          
reduction,  density estimation, and classification. Chapters                                                                                to  make statements about their similarity. The idea is that
in Part I mostly                                                      Linear Algebra Analytic Geometry Matrix Decomposition                 vectors that                                                
                                                                                                                                                                                                        
build  upon  the previous ones, but it is possible to skip a          between  the  two  parts  of  the  book to link mathematical          are  similar  should be predicted to have similar outputs by
chapter and work                                                      concepts with                                                         our machine                                                 
                                                                                                                                                                                                        
backward  if necessary. Chapters in Part II are only loosely          machine learning algorithms.                                          learning algorithm (our predictor). To formalize the idea of
coupled and                                                                                                                                 similarity  between vectors, we need to introduce operations
                                                                      Of  course  there  are more than two ways to read this book.          that take two vectors as                                    
can  be  read  in any order. There are many pointers forward          Most readers                                                                                                                      
and backward                                                                                                                                input  and  return  a  numerical  value  representing  their
                                                                      learn   using   a  combination  of  top-down  and  bottom-up          similarity. The conanalytic geometry struction of similarity
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          approaches,  sometimes building up basic mathematical skills          and distances is central to analytic geometry and is        
by Cambridge University Press (2020).                                 before  attempting  more complex concepts, but also choosing                                                                      
                                                                      topics based on applications of machine                               discussed in Chapter 3.                                     
14 Introduction and Motivation                                                                                                                                                                          
                                                                      learning.                                                             In  Chapter  4, we introduce some fundamental concepts about
Figure 1.2 The                                                                                                                              matrimatrix ces and matrix decomposition. Some operations on
                                                                      Part I Is about Mathematics                                           matrices are extremely                                      
foundations and                                                                                                                                                                                         
                                                                      The  four  pillars of machine learning we cover in this book          decomposition useful in machine learning, and they allow for
four pillars of                                                       (see Figure 1.2)                                                      an intuitive interpretation                                 
                                                                                                                                                                                                        
machine learning.                                                     require  a  solid mathematical foundation, which is laid out          of the data and more efficient learning.                    
                                                                      in Part I.                                                                                                                        
Classification                                                                                                                              We often consider data to be noisy observations of some true
                                                                      We represent numerical data as vectors and represent a table          underlying signal. We hope that by applying machine learning
Density                                                               of such                                                               we can identify the                                         
                                                                                                                                                                                                        
Regression                                                            data  as  a  matrix.  The  study  of vectors and matrices is          signal  from  the noise. This requires us to have a language
                                                                      called linear algebra,                                                for quantifying what “noise” means. We often would also like
Dimensionality                                                                                                                              to have predictors that                                     

%%%

                                                                      Broadly  speaking,  chapters  are  ordered by difficulty (in                                                                      
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          ascending order).                                                     of  high-dimensional  data  x ∈ RD, which is often easier to
Feedback: https://mml-book.com.                                                                                                             analyze than                                                
                                                                      In  Chapter  8,  we  restate the three components of machine                                                                      
1.2 Two Ways to Read This Book 15                                     learning                                                              the   original   data.   Unlike  regression,  dimensionality
                                                                                                                                            reduction  is only concerned about modeling the data – there
allow  us  to  express  some  sort  of uncertainty, e.g., to          (data,  models,  and parameter estimation) in a mathematical          are no labels associated with a                             
quantify  the  confidence  we  have  about  the value of the          fashion. In                                                                                                                       
prediction at a particular test data                                                                                                        data point x.                                               
                                                                      addition,   we   provide   some   guidelines   for  building                                                                      
point.   Quantification  of  uncertainty  is  the  realm  of          experimental set-ups                                                  In  Chapter  11,  we  will move to our third pillar: density
probability theory and probability theory                                                                                                   estimation. The density estimation                          
                                                                      that  guard against overly optimistic evaluations of machine                                                                      
is covered in Chapter 6.                                              learning  systems.  Recall  that  the  goal  is  to  build a          objective  of  density  estimation  is to find a probability
                                                                      predictor that performs well on                                       distribution  that  describes a given dataset. We will focus
To   train   machine  learning  models,  we  typically  find                                                                                on Gaussian mixture models for this                         
parameters that                                                       unseen data.                                                                                                                      
                                                                                                                                            purpose, and we will discuss an iterative scheme to find the
maximize   some   performance   measure.  Many  optimization          In   Chapter  9,  we  will  have  a  close  look  at  linear          parameters of                                               
techniques require the concept of a gradient, which tells us          regression, where our linear regression                                                                                           
the direction in which to                                                                                                                   this  model.  As  in  dimensionality reduction, there are no
                                                                      objective  is  to  find  functions that map inputs x ∈ RD to          labels associated                                           
search  for  a  solution. Chapter 5 is about vector calculus          corresponding  observed  function values y ∈ R, which we can                                                                      
and details the vector calculus                                       interpret as the labels of their                                      with  the  data  points  x  ∈  RD. However, we do not seek a
                                                                                                                                            low-dimensional                                             
concept  of  gradients, which we subsequently use in Chapter          respective  inputs.  We will discuss classical model fitting                                                                      
7, where we                                                           (parameter  estimation) via maximum likelihood and maximum a          representation  of the data. Instead, we are interested in a
                                                                      posteriori estimation,                                                density model                                               
talk  about optimization to find maxima/minima of functions.                                                                                                                                            
optimization                                                          as  well  as  Bayesian linear regression, where we integrate          that describes the data.                                    
                                                                      the parameters                                                                                                                    
Part II Is about Machine Learning                                                                                                           Chapter 12 concludes the book with an in-depth discussion of
                                                                      out instead of optimizing them.                                       the fourth                                                  
The  second  part  of  the  book  introduces four pillars of                                                                                                                                            
machine learning                                                      Chapter  10  focuses on dimensionality reduction, the second          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                      pillar in Fig- dimensionality                                         by Cambridge University Press (2020).                       
as  shown  in Figure 1.2. We illustrate how the mathematical                                                                                                                                            
concepts  introduced  in  the first part of the book are the          ure  1.2 reduction , using principal component analysis. The          16 Introduction and Motivation                              
foundation for each pillar.                                           key  objective  of  dimensionality  reduction  is  to find a                                                                      
                                                                      compact, lower-dimensional representation                             classification   pillar:  classification.  We  will  discuss

%%%

classification in the context of support                              URL.                                                                  examples of such                                            
                                                                                                                                                                                                        
vector  machines. Similar to regression (Chapter 9), we have          Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          vector objects:                                             
inputs x and                                                          Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                            1.  Geometric  vectors.  This  example  of  a  vector may be
corresponding  labels  y.  However, unlike regression, where          2                                                                     familiar from high                                          
the labels were                                                                                                                                                                                         
                                                                      Linear Algebra                                                        school  mathematics  and  physics.  Geometric  vectors – see
real-valued,  the  labels  in  classification  are integers,                                                                                Figure 2.1(a)                                               
which requires special                                                When formalizing intuitive concepts, a common approach is to                                                                      
                                                                      construct a                                                           – are directed segments, which can be drawn (at least in two
care.                                                                                                                                       dimensions). Two geometric vectors →                        
                                                                      set  of  objects  (symbols) and a set of rules to manipulate                                                                      
1.3 Exercises and Feedback                                            these objects. This                                                   x,                                                          
                                                                                                                                                                                                        
We  provide  some  exercises  in  Part  I, which can be done          is  known  as  an  algebra.  Linear  algebra is the study of          →                                                           
mostly by pen and                                                     vectors and certain algebra                                                                                                       
                                                                                                                                            y can be added, such that →                                 
paper.   For  Part  II,  we  provide  programming  tutorials          rules  to  manipulate  vectors.  The vectors many of us know                                                                      
(jupyter notebooks)                                                   from school are                                                       x+                                                          
                                                                                                                                                                                                        
to   explore   some   properties  of  the  machine  learning          called  “geometric  vectors”, which are usually denoted by a          →                                                           
algorithms we discuss                                                 small arrow                                                                                                                       
                                                                                                                                            y =                                                         
in this book.                                                         above  the  letter,  e.g.,  −→x  and  −→y . In this book, we                                                                      
                                                                      discuss more general                                                  →                                                           
We  appreciate  that  Cambridge  University  Press  strongly                                                                                                                                            
supports our                                                          concepts of vectors and use a bold letter to represent them,          z                                                           
                                                                      e.g., x and y.                                                                                                                    
aim  to  democratize  education  and learning by making this                                                                                is  another geometric vector. Furthermore, multiplication by
book freely                                                           In  general,  vectors  are special objects that can be added          a scalar                                                    
                                                                      together and                                                                                                                      
available for download at                                                                                                                   λ                                                           
                                                                      multiplied  by scalars to produce another object of the same                                                                      
https://mml-book.com                                                  kind. From                                                            →                                                           
                                                                                                                                                                                                        
where  tutorials,  errata,  and  additional materials can be          an   abstract   mathematical   viewpoint,  any  object  that          x,  λ  ∈  R,  is also a geometric vector. In fact, it is the
found. Mistakes                                                       satisfies these two                                                   original vector                                             
                                                                                                                                                                                                        
can  be  reported  and feedback provided using the preceding          properties  can  be  considered  a  vector.  Here  are  some          scaled  by  λ. Therefore, geometric vectors are instances of

%%%

the vector                                                                                                                                  they can                                                    
                                                                      (a) Geometric vectors.                                                                                                            
concepts  introduced  previously.  Interpreting  vectors  as                                                                                be  multiplied  by  a  scalar  λ  ∈  R,  and the result is a
geometric  vectors  enables  us  to use our intuitions about          −2 0 2                                                                polynomial as                                               
direction and magnitude to                                                                                                                                                                              
                                                                      x                                                                     well.  Therefore, polynomials are (rather unusual) instances
reason about mathematical operations.                                                                                                       of vectors.                                                 
                                                                      −6                                                                                                                                
2.  Polynomials  are  also  vectors;  see Figure 2.1(b): Two                                                                                Note  that  polynomials  are  very  different from geometric
polynomials can                                                       −4                                                                    vectors. While                                              
                                                                                                                                                                                                        
Figure 2.1                                                            −2                                                                    geometric  vectors  are concrete “drawings”, polynomials are
                                                                                                                                            abstract                                                    
Different types of                                                    0                                                                                                                                 
                                                                                                                                            concepts.  However,  they  are  both  vectors  in  the sense
vectors. Vectors can                                                  2                                                                     previously described.                                       
                                                                                                                                                                                                        
be surprising                                                         4                                                                     3.  Audio signals are vectors. Audio signals are represented
                                                                                                                                            as a series of                                              
objects, including                                                    y                                                                                                                                 
                                                                                                                                            numbers. We can add audio signals together, and their sum is
(a) geometric                                                         (b) Polynomials.                                                      a new                                                       
                                                                                                                                                                                                        
vectors                                                               17                                                                    audio signal. If we scale an audio signal, we also obtain an
                                                                                                                                            audio signal.                                               
and (b) polynomials.                                                  This  material is published by Cambridge University Press as                                                                      
                                                                      Mathematics for Machine Learning by                                   Therefore, audio signals are a type of vector, too.         
→                                                                                                                                                                                                       
                                                                      Marc  Peter  Deisenroth,  A. Aldo Faisal, and Cheng Soon Ong          4. Elements of Rn                                           
x →                                                                   (2020). This version is free to view                                                                                              
                                                                                                                                            (tuples of n real numbers) are vectors. Rn                  
y                                                                     and download for personal use only. Not for re-distribution,                                                                      
                                                                      re-sale, or use in derivative works.                                  is more                                                     
→                                                                                                                                                                                                       
                                                                      ©by  M.  P.  Deisenroth,  A. A. Faisal, and C. S. Ong, 2021.          abstract than polynomials, and it is the concept we focus on
x +                                                                   https://mml-book.com.                                                 in this                                                     
                                                                                                                                                                                                        
→                                                                     18 Linear Algebra                                                     book. For instance,                                         
                                                                                                                                                                                                        
y                                                                     be  added together, which results in another polynomial; and          a =                                                         

%%%

                                                                                                                                            on linear algebra:                                          
                                                                     operations when                                                                                                                   
                                                                                                                                            https://tinyurl.                                            
                                                                     implementing on a                                                                                                                 
                                                                                                                                            com/h5g4kps                                                 
1                                                                     computer.                                                                                                                         
                                                                                                                                            focus on vectors in Rn                                      
2                                                                     it  loosely  corresponds  to  arrays  of  real  numbers on a                                                                      
                                                                      computer. Many                                                        since most algorithms in linear algebra are formulated in Rn
3                                                                                                                                                                                                       
                                                                      programming  languages support array operations, which allow          . We will see in Chapter 8 that we often consider data to   
                                                                     for  convenient  implementation  of  algorithms that involve                                                                      
                                                                      vector operations.                                                    be represented as vectors in Rn                             
 ∈ R                                                                                                                                                                                                   
                                                                      Linear  algebra  focuses  on  the similarities between these          .  In  this  book, we will focus on finitedimensional vector
3                                                                     vector concepts.                                                      spaces, in which case there is a 1:1 correspondence         
                                                                                                                                                                                                        
(2.1)                                                                 We  can  add  them together and multiply them by scalars. We          between any kind of vector and Rn                           
                                                                      will largely Pavel Grinfeld’s                                                                                                     
is an example of a triplet of numbers. Adding two vectors a,                                                                                . When it is convenient, we will use                        
b ∈ Rn                                                                series on linear                                                                                                                  
                                                                                                                                            intuitions  about geometric vectors and consider array-based
component-wise results in another vector: a + b = c ∈ Rn              algebra:                                                              algorithms.                                                 
                                                                                                                                                                                                        
. Moreover,                                                           http://tinyurl.                                                       One major idea in mathematics is the idea of “closure”. This
                                                                                                                                            is  the  question:  What  is  the set of all things that can
multiplying  a ∈ Rn by λ ∈ R results in a scaled vector λa ∈          com/nahclwm                                                           result  from my proposed operations? In the case of vectors:
Rn                                                                                                                                          What is the set of vectors that can result by               
                                                                      Gilbert Strang’s                                                                                                                  
.                                                                                                                                           starting  with  a  small  set of vectors, and adding them to
                                                                      course on linear                                                      each other and                                              
Considering vectors as elements of Rn                                                                                                                                                                   
                                                                      algebra:                                                              scaling  them? This results in a vector space (Section 2.4).
Be careful to check has an additional benefit that                                                                                          The concept of                                              
                                                                      http://tinyurl.                                                                                                                   
whether array                                                                                                                               a  vector  space and its properties underlie much of machine
                                                                      com/29p5q8j                                                           learning. The                                               
operations actually                                                                                                                                                                                     
                                                                      3Blue1Brown series                                                    concepts introduced in this chapter are summarized in Figure
perform vector                                                                                                                              2.2.                                                        

%%%

                                                                                                                                                                                                        
This  chapter is mostly based on the lecture notes and books          linear equations                                                      with +                                                      
by Drumm                                                                                                                                                                                                
                                                                      Matrix                                                                represents                                                  
and  Weil  (2001),  Strang (2003), Hogben (2013), Liesen and                                                                                                                                            
Mehrmann                                                              inverse                                                               represents                                                  
                                                                                                                                                                                                        
(2015),  as  well as Pavel Grinfeld’s Linear Algebra series.          Gaussian                                                              solved by                                                   
Other excellent                                                                                                                                                                                         
                                                                      elimination                                                           solves                                                      
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                       Linear/affine                                                         property of                                                 
                                                                                                                                                                                                        
2.1 Systems of Linear Equations 19                                    mapping                                                               maximal set                                                 
                                                                                                                                                                                                        
Figure 2.2 A mind                                                     Linear                                                                resources  are Gilbert Strang’s Linear Algebra course at MIT
                                                                                                                                            and the Linear                                              
map of the concepts                                                   independence                                                                                                                      
                                                                                                                                            Algebra Series by 3Blue1Brown.                              
introduced in this                                                    Basis                                                                                                                             
                                                                                                                                            Linear  algebra  plays an important role in machine learning
chapter, along with                                                   Chapter 10                                                            and  general  mathematics.  The  concepts introduced in this
                                                                                                                                            chapter are further expanded to include the idea of geometry
where they are used                                                   Dimensionality                                                        in Chapter 3. In Chapter 5, we                              
                                                                                                                                                                                                        
in other parts of the                                                 reduction                                                             will  discuss  vector calculus, where a principled knowledge
                                                                                                                                            of  matrix  operations  is essential. In Chapter 10, we will
book.                                                                 Chapter 12                                                            use  projections  (to  be  introduced  in  Section  3.8) for
                                                                                                                                            dimensionality  reduction  with principal component analysis
Vector                                                                Classification                                                        (PCA).  In  Chapter  9,  we  will discuss linear regression,
                                                                                                                                            where                                                       
Vector space                                                          Chapter 3                                                                                                                         
                                                                                                                                            linear   algebra   plays   a   central   role   for  solving
Matrix Chapter 5                                                      Analytic geometry                                                     least-squares problems.                                     
                                                                                                                                                                                                        
Vector calculus                                                       composes                                                              2.1 Systems of Linear Equations                             
                                                                                                                                                                                                        
Group                                                                 closure                                                               Systems  of  linear  equations play a central part of linear
                                                                                                                                            algebra. Many                                               
System of                                                             Abelian                                                                                                                           

%%%

problems  can  be formulated as systems of linear equations,                                                                                (2.4)                                                       
and linear                                                            ,                                                                                                                                 
                                                                                                                                            has  no  solution:  Adding  the  first  two equations yields
algebra gives us the tools for solving them.                          therefore, has to satisfy the following system of equations:          2x1+3x3 = 5, which                                          
                                                                                                                                                                                                        
Example 2.1                                                           a11x1 + · · · + a1nxn = b1                                            contradicts the third equation (3).                         
                                                                                                                                                                                                        
A  company  produces  products  N1,  .  .  .  , Nn for which          .                                                                     Let us have a look at the system of linear equations        
resources                                                                                                                                                                                               
                                                                      .                                                                     x1 + x2 + x3 = 3 (1)                                        
R1, . . . , Rm are required. To produce a unit of product Nj                                                                                                                                            
, aij units of                                                        .                                                                     x1 − x2 + 2x3 = 2 (2)                                       
                                                                                                                                                                                                        
resource  Ri are needed, where i = 1, . . . , m and j = 1, .          am1x1 + · · · + amnxn = bm                                            x2 + x3 = 2 (3)                                             
. . , n.                                                                                                                                                                                                
                                                                      , (2.3)                                                               . (2.5)                                                     
The objective is to find an optimal production plan, i.e., a                                                                                                                                            
plan of how                                                           where aij ∈ R and bi ∈ R.                                             From  the  first and third equation, it follows that x1 = 1.
                                                                                                                                            From (1)+(2),                                               
many units xj of product Nj should be produced if a total of          system  of  linear  Equation  (2.3) is the general form of a                                                                      
bi units of                                                           system of linear equations, and                                       we  get  2x1  + 3x3 = 5, i.e., x3 = 1. From (3), we then get
                                                                                                                                            that x2 = 1.                                                
resource  Ri  are  available  and (ideally) no resources are          equations  x1,  .  . . , xn are the unknowns of this system.                                                                      
left over.                                                            Every n-tuple (x1, . . . , xn) ∈                                      Therefore,  (1,  1,  1)  is  the  only  possible  and unique
                                                                                                                                            solution (verify that                                       
If  we  produce  x1,  .  . . , xn units of the corresponding          Rn                                                                                                                                
products, we need                                                                                                                           (1, 1, 1) is a solution by plugging in).                    
                                                                      solution  that  satisfies  (2.3) is a solution of the linear                                                                      
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          equation system.                                                      As a third example, we consider                             
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      Example 2.2                                                           x1 + x2 + x3 = 3 (1)                                        
20 Linear Algebra                                                                                                                                                                                       
                                                                      The system of linear equations                                        x1 − x2 + 2x3 = 2 (2)                                       
a total of                                                                                                                                                                                              
                                                                      x1 + x2 + x3 = 3 (1)                                                  2x1 + 3x3 = 5 (3)                                           
ai1x1 + · · · + ainxn (2.2)                                                                                                                                                                             
                                                                      x1 − x2 + 2x3 = 2 (2)                                                 . (2.6)                                                     
many units of resource Ri                                                                                                                                                                               
                                                                      2x1 + 3x3 = 1 (3)                                                     Since   (1)+(2)=(3),   we   can   omit  the  third  equation
. An optimal production plan (x1, . . . , xn) ∈ Rn                                                                                          (redundancy). From                                          

%%%

                                                                                                                                                                                                        
(1) and (2), we get 2x1 = 5−3x3 and 2x2 = 1+x3. We define x3          solution space of a                                                   Remark   (Geometric  Interpretation  of  Systems  of  Linear
= a ∈ R                                                                                                                                     Equations). In a                                            
                                                                      system of two linear                                                                                                              
as a free variable, such that any triplet                                                                                                   system  of  linear equations with two variables x1, x2, each
                                                                      equations with two                                                    linear equation                                             
                                                                                                                                                                                                       
                                                                      variables can be                                                      defines  a  line  on  the  x1x2-plane. Since a solution to a
5                                                                                                                                           system of linear                                            
                                                                      geometrically                                                                                                                     
2                                                                                                                                           equations  must  satisfy  all  equations simultaneously, the
                                                                      interpreted as the                                                    solution set is the                                         
−                                                                                                                                                                                                       
                                                                      intersection of two                                                   intersection  of these lines. This intersection set can be a
3                                                                                                                                           line (if the linear                                         
                                                                      lines. Every linear                                                                                                               
2                                                                                                                                           equations  describe  the same line), a point, or empty (when
                                                                      equation represents                                                   the lines are                                               
a,                                                                                                                                                                                                      
                                                                      a line.                                                               parallel).  An  illustration  is given in Figure 2.1 for the
1                                                                                                                                           system                                                      
                                                                      2x1 − 4x2 = 1 4x1 + 4x2 = 5 x1                                                                                                    
2                                                                                                                                           4x1 + 4x2 = 5                                               
                                                                      x2                                                                                                                                
+                                                                                                                                           2x1 − 4x2 = 1                                               
                                                                      is  a  solution  of the system of linear equations, i.e., we                                                                      
1                                                                     obtain a solution                                                     (2.8)                                                       
                                                                                                                                                                                                        
2                                                                     set that contains infinitely many solutions.                          where the solution space is the point (x1, x2) = (1,        
                                                                                                                                                                                                        
a, a                                                                 In  general, for a real-valued system of linear equations we          1                                                           
                                                                      obtain either                                                                                                                     
, a ∈ R (2.7)                                                                                                                               4                                                           
                                                                      no,  exactly  one,  or  infinitely  many  solutions.  Linear                                                                      
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          regression (Chapter 9)                                                ). Similarly, for three                                     
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                      solves  a  version  of  Example 2.1 when we cannot solve the          variables,  each  linear  equation  determines  a  plane  in
2.1 Systems of Linear Equations 21                                    system of linear                                                      three-dimensional                                           
                                                                                                                                                                                                        
Figure 2.1 The                                                        equations.                                                            space.  When  we  intersect  these planes, i.e., satisfy all

%%%

linear equations at                                                   x1 +                                                                  amn                                                         
                                                                                                                                                                                                        
the same time, we can obtain a solution set that is a plane,                                                                                                                                          
a line, a point                                                                                                                                                                                         
                                                                                                                                                                                                      
or empty (when the planes have no common intersection). ♢                                                                                                                                               
                                                                                                                                                                                                      
For  a  systematic  approach  to  solving  systems of linear                                                                                                                                            
equations, we                                                         a12                                                                   xn =                                                        
                                                                                                                                                                                                        
will  introduce  a  useful  compact notation. We collect the          .                                                                                                                                
coefficients aij                                                                                                                                                                                        
                                                                      .                                                                                                                                
into vectors and collect the vectors into matrices. In other                                                                                                                                            
words, we write                                                       .                                                                                                                                
                                                                                                                                                                                                        
the system from (2.3) in the following form:                          am2                                                                   b1                                                          
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
                                                                                                                                          .                                                           
                                                                                                                                                                                                        
a11                                                                   x2 + · · · +                                                          bm                                                          
                                                                                                                                                                                                        
.                                                                                                                                                                                                     
                                                                                                                                                                                                        
.                                                                                                                                                                                                     
                                                                                                                                                                                                        
.                                                                                                                                           (2.9)                                                     
                                                                                                                                                                                                        
am1                                                                   a1n                                                                   ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
                                                                     .                                                                                                                                 
                                                                                                                                            22 Linear Algebra                                           
                                                                     .                                                                                                                                 
                                                                                                                                            ⇐⇒                                                          
                                                                     .                                                                                                                                 
                                                                                                                                                                                                       

%%%

                                                                                                                                                                                                        
                                                                     xn                                                                    functions  (linear mappings) as we will see later in Section
                                                                                                                                            2.7. Before we                                              
                                                                                                                                                                                                      
                                                                                                                                            discuss  some  of  these  interesting  topics,  let us first
a11 · · · a1n                                                                                                                              define what a matrix                                        
                                                                                                                                                                                                        
.                                                                      =                                                                   is  and  what kind of operations we can do with matrices. We
                                                                                                                                            will see more                                               
.                                                                                                                                                                                                      
                                                                                                                                            properties of matrices in Chapter 4.                        
.                                                                                                                                                                                                      
                                                                                                                                            matrix  Definition 2.1 (Matrix). With m, n ∈ N a real-valued
.                                                                                                                                          (m, n) matrix A is                                          
                                                                                                                                                                                                        
.                                                                     b1                                                                    an  m·n-tuple of elements aij , i = 1, . . . , m, j = 1, . .
                                                                                                                                            . , n, which is ordered                                     
.                                                                     .                                                                                                                                 
                                                                                                                                            according to a rectangular scheme consisting of m rows and n
am1 · · · amn                                                         .                                                                     columns:                                                    
                                                                                                                                                                                                        
                                                                     .                                                                     A =                                                         
                                                                                                                                                                                                        
                                                                     bm                                                                                                                               
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                      . (2.10)                                                                                                                       
                                                                                                                                                                                                        
                                                                     In  the  following,  we  will  have  a  close  look at these                                                                     
                                                                      matrices  and  define  computation  rules. We will return to                                                                      
x1                                                                    solving linear equations in Section 2.3.                              a11 a12 · · · a1n                                           
                                                                                                                                                                                                        
.                                                                     2.2 Matrices                                                          a21 a22 · · · a2n                                           
                                                                                                                                                                                                        
.                                                                     Matrices  play a central role in linear algebra. They can be          .                                                           
                                                                      used to compactly represent systems of linear equations, but                                                                      
.                                                                     they also represent linear                                            .                                                           

%%%

                                                                                                                                                                                                       
.                                                                     stacking its                                                                                                                      
                                                                                                                                                                                                       
.                                                                     columns, a matrix A                                                                                                               
                                                                                                                                            a11 + b11 · · · a1n + b1n                                   
.                                                                     can be represented                                                                                                                
                                                                                                                                            .                                                           
.                                                                     as a long vector a.                                                                                                               
                                                                                                                                            .                                                           
.                                                                     re-shape                                                                                                                          
                                                                                                                                            .                                                           
.                                                                     A ∈ R                                                                                                                             
                                                                                                                                            .                                                           
.                                                                     4×2 a ∈ R                                                                                                                         
                                                                                                                                            .                                                           
am1 am2 · · · amn                                                     8                                                                                                                                 
                                                                                                                                            .                                                           
                                                                     Rm×n                                                                                                                              
                                                                                                                                            am1 + bm1 · · · amn + bmn                                   
                                                                     is the set of all real-valued (m, n)-matrices. A ∈ Rm×n                                                                           
                                                                                                                                                                                                       
                                                                     can be                                                                                                                            
                                                                                                                                                                                                       
                                                                     equivalently  represented  as  a  ∈  Rmn  by  stacking all n                                                                      
                                                                      columns of the                                                         ∈ R                                                       
                                                                                                                                                                                                       
                                                                      matrix into a long vector; see Figure 2.2.                            m×n                                                         
, aij ∈ R . (2.11)                                                                                                                                                                                      
                                                                      2.2.1 Matrix Addition and Multiplication                              . (2.12)                                                    
row  By  convention  (1, n)-matrices are called rows and (m,                                                                                                                                            
1)-matrices are called                                                The sum of two matrices A ∈ Rm×n                                      For matrices A ∈ Rm×n                                       
                                                                                                                                                                                                        
column  columns.  These  special  matrices  are  also called          , B ∈ Rm×n                                                            ,  B  ∈  Rn×k Note the size of the , the elements cij of the
row/column vectors.                                                                                                                         product                                                     
                                                                      is defined as the elementwise sum, i.e.,                                                                                          
row vector                                                                                                                                  matrices. C = AB ∈ Rm×k are computed as                     
                                                                      A + B :=                                                                                                                          
column vector                                                                                                                               C =                                                         
                                                                                                                                                                                                       
Figure 2.2 By                                                                                                                               np.einsum(’il,                                              

%%%

                                                                                                                                            Remark.   Matrix   multiplication   is  not  defined  as  an
lj’, A, B) cij =                                                      cases,  where  we need to be explicit that we are performing          element-wise operation                                      
                                                                      multiplication,                                                                                                                   
Xn                                                                                                                                          on  matrix  elements, i.e., cij ̸= aij bij (even if the size
                                                                      we   use  the  notation  A  ·  B  to  denote  multiplication          of A, B was chosen appropriately). This kind of element-wise
l=1                                                                   (explicitly showing                                                   multiplication often appears                                
                                                                                                                                                                                                        
ailblj , i = 1, . . . , m, j = 1, . . . , k. (2.13)                   “·”).                                                                 in     programming     languages     when     we    multiply
                                                                                                                                            (multi-dimensional) arrays                                  
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          Remark.   Matrices   can   only   be   multiplied  if  their                                                                      
Feedback: https://mml-book.com.                                       “neighboring” dimensions                                              with  each  other,  and  is  called  a  Hadamard  product. ♢
                                                                                                                                            Hadamard product                                            
2.2 Matrices 23                                                       match.  For  instance,  an  n × k-matrix A can be multiplied                                                                      
                                                                      with a k × mmatrix B, but only from the left side:                    Example 2.3                                                 
This  means, to compute element cij we multiply the elements                                                                                                                                            
of the ith There are n columns                                        |{z}                                                                  For A =                                                     
                                                                                                                                                                                                        
in A and n rows in                                                    A                                                                                                                                
                                                                                                                                                                                                        
B so that we can                                                      n×k                                                                   1 2 3                                                       
                                                                                                                                                                                                        
compute ailblj for                                                    |{z}                                                                  3 2 1                                                      
                                                                                                                                                                                                        
l = 1, . . . , n.                                                     B                                                                     ∈ R2×3                                                      
                                                                                                                                                                                                        
Commonly, the dot                                                     k×m                                                                   , B =                                                       
                                                                                                                                                                                                        
product between                                                       = |{z}                                                                                                                           
                                                                                                                                                                                                        
two vectors a, b is                                                   C                                                                                                                                
                                                                                                                                                                                                        
denoted by a⊤b or                                                     n×m                                                                   0 2                                                         
                                                                                                                                                                                                        
⟨a, b⟩.                                                               (2.14)                                                                1 −1                                                        
                                                                                                                                                                                                        
row  of A with the jth column of B and sum them up. Later in          The  product  BA  is  not  defined  if  m  ̸=  n  since  the          0 1                                                         
Section 3.2,                                                          neighboring dimensions                                                                                                            
                                                                                                                                                                                                       
we  will  call this the dot product of the corresponding row          do not match. ♢                                                                                                                   
and column. In                                                                                                                               ∈ R3×2                                                    

%%%

                                                                                                                                                                                                        
, we obtain                                                                                                                                both matrix                                                 
                                                                                                                                                                                                        
AB =                                                                  0 2                                                                   multiplications AB                                          
                                                                                                                                                                                                        
                                                                     1 −1                                                                  and BA are                                                  
                                                                                                                                                                                                        
1 2 3                                                                 0 1                                                                   defined, the                                                
                                                                                                                                                                                                        
3 2 1                                                                                                                                     dimensions of the                                           
                                                                                                                                                                                                        
                                                                                                                                          results can be                                              
                                                                                                                                                                                                        
                                                                                                                                          different.                                                  
                                                                                                                                                                                                        
0 2                                                                   1 2 3                                                                 From   this   example,   we  can  already  see  that  matrix
                                                                                                                                            multiplication is not                                       
1 −1                                                                  3 2 1                                                                                                                            
                                                                                                                                            commutative,  i.e.,  AB  ̸=  BA;  see also Figure 2.3 for an
0 1                                                                   =                                                                     illustration.                                               
                                                                                                                                                                                                        
                                                                                                                                          Definition 2.2 (Identity Matrix). In Rn×n                   
                                                                                                                                                                                                        
 =                                                                                                                                        , we define the identity matrix                             
                                                                                                                                                                                                        
                                                                     6 4 2                                                                 identity matrix                                             
                                                                                                                                                                                                        
2 3                                                                   −2 0 2                                                                In :=                                                       
                                                                                                                                                                                                        
2 5                                                                  3 2 1                                                                                                                            
                                                                                                                                                                                                        
∈ R                                                                                                                                                                                                   
                                                                                                                                                                                                        
2×2                                                                    ∈ R                                                                                                                            
                                                                                                                                                                                                        
, (2.15)                                                              3×3                                                                                                                              
                                                                                                                                                                                                        
BA =                                                                  . (2.16)                                                                                                                         
                                                                                                                                                                                                        
                                                                     Figure 2.3 Even if                                                                                                               

%%%

                                                                                                                                                                                                        
                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
                                                                     .                                                                     0 0 · · · 0 · · · 1                                         
                                                                                                                                                                                                        
1 0 · · · 0 · · · 0                                                   0 0 · · · 1 · · · 0                                                                                                              
                                                                                                                                                                                                        
0 1 · · · 0 · · · 0                                                   .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                                                                                
                                                                                                                                                                                                        
.                                                                     .                                                                     ∈ R                                                         
                                                                                                                                                                                                        
.                                                                     .                                                                     n×n                                                         
                                                                                                                                                                                                        
.                                                                     .                                                                     (2.17)                                                      
                                                                                                                                                                                                        
.                                                                     .                                                                     ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
.                                                                     .                                                                                                                                 
                                                                                                                                            24 Linear Algebra                                           
.                                                                     .                                                                                                                                 

%%%

as  the  n  ×  n-matrix  containing  1 on the diagonal and 0                                                                                                                                            
everywhere else.                                                      A(C + D) = AC + AD (2.19b)                                            regular     inverse     does     exist,    A    is    called
                                                                                                                                            regular/invertible/nonsingular, otherwise                   
Now  that  we defined matrix multiplication, matrix addition          Multiplication with the identity matrix:                                                                                          
and the                                                                                                                                     invertible                                                  
                                                                      ∀A ∈ R                                                                                                                            
identity  matrix,  let  us have a look at some properties of                                                                                nonsingular                                                 
matrices:                                                             m×n                                                                                                                               
                                                                                                                                            singular/noninvertible.  When  the matrix inverse exists, it
associativity                                                         : ImA = AIn = A (2.20)                                                is unique. In Secsingular                                   
                                                                                                                                                                                                        
Associativity:                                                        Note that Im ̸= In for m ̸= n.                                        noninvertible                                               
                                                                                                                                                                                                        
∀A ∈ R                                                                2.2.2 Inverse and Transpose                                           tion  2.3,  we  will  discuss  a  general way to compute the
                                                                                                                                            inverse of a matrix                                         
m×n                                                                   Definition 2.3 (Inverse). Consider a square matrix A ∈ Rn×n                                                                       
                                                                                                                                            by solving a system of linear equations.                    
, B ∈ R                                                               A square matrix . Let matrix                                                                                                      
                                                                                                                                            Remark  (Existence  of  the  Inverse  of  a  2  × 2-matrix).
n×p                                                                   possesses the same                                                    Consider a matrix                                           
                                                                                                                                                                                                        
, C ∈ R                                                               number of columns                                                     A :=                                                       
                                                                                                                                                                                                        
p×q                                                                   and rows.                                                             a11 a12                                                     
                                                                                                                                                                                                        
: (AB)C = A(BC) (2.18)                                                B  ∈  Rn×n  have the property that AB = In = BA. B is called          a21 a22                                                    
                                                                      the                                                                                                                               
distributivity                                                                                                                              ∈ R                                                         
                                                                      inverse of A and denoted by A                                                                                                     
Distributivity:                                                                                                                             2×2                                                         
                                                                      −1                                                                                                                                
∀A, B ∈ R                                                                                                                                   . (2.21)                                                    
                                                                      .                                                                                                                                 
m×n                                                                                                                                         If we multiply A with                                       
                                                                      inverse  Unfortunately,  not  every  matrix  A  possesses an                                                                      
, C, D ∈ R                                                            inverse A                                                             A                                                           
                                                                                                                                                                                                        
n×p                                                                   −1                                                                    ′                                                           
                                                                                                                                                                                                        
: (A + B)C = AC + BC (2.19a)                                          . If this                                                             :=                                                         

%%%

                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
a22 −a12                                                              Feedback: https://mml-book.com.                                                                                                  
                                                                                                                                                                                                        
−a21 a11                                                             2.2 Matrices 25                                                        (2.25)                                                    
                                                                                                                                                                                                        
(2.22)                                                                a12a21  is  the determinant of a 2×2-matrix. Furthermore, we          are inverse to each other since AB = I = BA.                
                                                                      can generally                                                                                                                     
we obtain                                                                                                                                   Definition 2.4 (Transpose). For A ∈ Rm×n                    
                                                                      use the determinant to check whether a matrix is invertible.                                                                      
AA′ =                                                                 ♢                                                                     the matrix B ∈ Rn×m with                                    
                                                                                                                                                                                                        
                                                                     Example 2.4 (Inverse Matrix)                                          bij = aji is called the transpose of A. We write B = A      
                                                                                                                                                                                                        
a11a22 − a12a21 0                                                     The matrices                                                          ⊤                                                           
                                                                                                                                                                                                        
0 a11a22 − a12a21                                                    A =                                                                   . transpose                                                 
                                                                                                                                                                                                        
= (a11a22 − a12a21)I .                                                                                                                     The main diagonal                                           
                                                                                                                                                                                                        
(2.23)                                                                                                                                     (sometimes called                                           
                                                                                                                                                                                                        
Therefore,                                                            1 2 1                                                                 “principal diagonal”,                                       
                                                                                                                                                                                                        
A                                                                     4 4 5                                                                 “primary diagonal”,                                         
                                                                                                                                                                                                        
−1 =                                                                  6 7 7                                                                 “leading diagonal”,                                         
                                                                                                                                                                                                        
1                                                                                                                                          or “major diagonal”)                                        
                                                                                                                                                                                                        
a11a22 − a12a21                                                       , B =                                                               of a matrix A is the                                        
                                                                                                                                                                                                        
a22 −a12                                                                                                                                   collection of entries                                       
                                                                                                                                                                                                        
−a21 a11                                                                                                                                  Aij where i = j.                                            
                                                                                                                                                                                                        
(2.24)                                                                −7 −7 6                                                               In general, A                                               
                                                                                                                                                                                                        
if  and only if a11a22 −a12a21 ̸= 0. In Section 4.1, we will          2 1 −1                                                                ⊤                                                           
see that a11a22 −                                                                                                                                                                                       
                                                                      4 5 −4                                                                can be obtained by writing the columns of A as the rows     

%%%

                                                                      (A + B)                                                               Definition 2.5 (Symmetric Matrix). A matrix A ∈ Rn×n        
of A                                                                                                                                                                                                    
                                                                      −1                                                                    is symmetric if symmetric matrix                            
⊤                                                                                                                                                                                                       
                                                                      ̸= A                                                                  A = A                                                       
.  The  following  are  important properties of inverses and                                                                                                                                            
transposes:                                                           −1 + B                                                                ⊤                                                           
                                                                                                                                                                                                        
The scalar case of                                                    −1                                                                    .                                                           
                                                                                                                                                                                                        
(2.28) is                                                             (2.28)                                                                Note  that only (n, n)-matrices can be symmetric. Generally,
                                                                                                                                            we call                                                     
1                                                                     (A                                                                                                                                
                                                                                                                                            (n,  n)-matrices  also  square matrices because they possess
2+4 = 1                                                               ⊤                                                                     the same num- square matrix                                 
                                                                                                                                                                                                        
6                                                                     )                                                                     ber  of rows and columns. Moreover, if A is invertible, then
                                                                                                                                            so is A                                                     
̸= 1                                                                  ⊤ = A (2.29)                                                                                                                      
                                                                                                                                            ⊤                                                           
2 + 1                                                                 (A + B)                                                                                                                           
                                                                                                                                            , and                                                       
4                                                                     ⊤ = A                                                                                                                             
                                                                                                                                            (A                                                          
.                                                                     ⊤ + B                                                                                                                             
                                                                                                                                            −1                                                          
AA−1 = I = A                                                          ⊤                                                                                                                                 
                                                                                                                                            )                                                           
−1A (2.26)                                                            (2.30)                                                                                                                            
                                                                                                                                            ⊤ = (A                                                      
(AB)                                                                  (AB)                                                                                                                              
                                                                                                                                            ⊤                                                           
−1 = B                                                                ⊤ = B                                                                                                                             
                                                                                                                                            )                                                           
−1A                                                                   ⊤A                                                                                                                                
                                                                                                                                            −1 =: A                                                     
−1                                                                    ⊤                                                                                                                                 
                                                                                                                                            −⊤                                                          
(2.27)                                                                (2.31)                                                                                                                            
                                                                                                                                            .                                                           

%%%

                                                                      by Cambridge University Press (2020).                                 λ(B + C) = λB + λC, B, C ∈ Rm×n                             
Remark  (Sum  and Product of Symmetric Matrices). The sum of                                                                                                                                            
symmetric matrices A, B ∈ Rn×n                                        26 Linear Algebra                                                     Example 2.5 (Distributivity)                                
                                                                                                                                                                                                        
is always symmetric. However, although their                          associativity Associativity:                                          If we define                                                
                                                                                                                                                                                                        
product is always defined, it is generally not symmetric:             (λψ)C = λ(ψC), C ∈ Rm×n                                               C :=                                                       
                                                                                                                                                                                                        
                                                                     λ(BC) = (λB)C = B(λC) = (BC)λ, B ∈ Rm×n                               1 2                                                         
                                                                                                                                                                                                        
1 0                                                                   , C ∈ Rn×k                                                            3 4                                                        
                                                                                                                                                                                                        
0 0 1 1                                                             .                                                                     , (2.33)                                                    
                                                                                                                                                                                                        
1 1                                                                  Note that this allows us to move scalar values around.                then for any λ, ψ ∈ R we obtain                             
                                                                                                                                                                                                        
=                                                                     (λC)                                                                  (λ + ψ)C =                                                  
                                                                                                                                                                                                        
                                                                     ⊤ = C                                                                                                                            
                                                                                                                                                                                                        
1 1                                                                   ⊤                                                                     (λ + ψ)1 (λ + ψ)2                                           
                                                                                                                                                                                                        
0 0                                                                  λ                                                                     (λ + ψ)3 (λ + ψ)4                                          
                                                                                                                                                                                                        
. (2.32)                                                              ⊤ = C                                                                 =                                                           
                                                                                                                                                                                                        
♢                                                                     ⊤                                                                                                                                
                                                                                                                                                                                                        
2.2.3 Multiplication by a Scalar                                      λ = λC                                                                λ + ψ 2λ + 2ψ                                               
                                                                                                                                                                                                        
Let  us  look  at  what  happens  to  matrices when they are          ⊤                                                                     3λ + 3ψ 4λ + 4ψ                                             
multiplied by a                                                                                                                                                                                         
                                                                      since λ = λ                                                                                                                      
scalar  λ  ∈ R. Let A ∈ Rm×n and λ ∈ R. Then λA = K, Kij = λ                                                                                                                                            
aij .                                                                 ⊤ for all λ ∈ R. distributivity                                       (2.34a)                                                     
                                                                                                                                                                                                        
Practically,  λ  scales each element of A. For λ, ψ ∈ R, the          Distributivity:                                                       =                                                           
following holds:                                                                                                                                                                                        
                                                                      (λ + ψ)C = λC + ψC, C ∈ Rm×n                                                                                                     
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                                                                            

%%%

λ 2λ                                                                                                                                        one.                                                        
                                                                      4 −2 −7                                                                                                                           
3λ 4λ                                                                                                                                       Generally,  a  system  of  linear equations can be compactly
                                                                      9 5 −3                                                                represented in                                              
                                                                                                                                                                                                       
                                                                                                                                           their  matrix  form as Ax = b; see (2.3), and the product Ax
+                                                                                                                                           is a (linear)                                               
                                                                                                                                                                                                       
                                                                                                                                           combination  of  the  columns  of  A. We will discuss linear
                                                                                                                                           combinations in                                             
ψ 2ψ                                                                                                                                                                                                    
                                                                                                                                           more detail in Section 2.5.                                 
3ψ 4ψ                                                                                                                                                                                                   
                                                                      x1                                                                    Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
                                                                                                                                           Feedback: https://mml-book.com.                             
                                                                      x2                                                                                                                                
= λC + ψC . (2.34b)                                                                                                                         2.3 Solving Systems of Linear Equations 27                  
                                                                      x3                                                                                                                                
2.2.4 Compact Representations of Systems of Linear Equations                                                                                2.3 Solving Systems of Linear Equations                     
                                                                                                                                                                                                       
If we consider the system of linear equations                                                                                               In  (2.3),  we  introduced  the  general form of an equation
                                                                       =                                                                   system, i.e.,                                               
2x1 + 3x2 + 5x3 = 1                                                                                                                                                                                     
                                                                                                                                           a11x1 + · · · + a1nxn = b1                                  
4x1 − 2x2 − 7x3 = 8                                                                                                                                                                                     
                                                                                                                                           .                                                           
9x1 + 5x2 − 3x3 = 2                                                                                                                                                                                     
                                                                      1                                                                     .                                                           
(2.35)                                                                                                                                                                                                  
                                                                      8                                                                     .                                                           
and  use  the  rules for matrix multiplication, we can write                                                                                                                                            
this equation                                                         2                                                                     am1x1 + · · · + amnxn = bm ,                                
                                                                                                                                                                                                        
system in a more compact form as                                                                                                           (2.37)                                                      
                                                                                                                                                                                                        
                                                                      . (2.36)                                                            where  aij  ∈  R  and  bi ∈ R are known constants and xj are
                                                                                                                                            unknowns,                                                   
                                                                     Note that x1 scales the first column, x2 the second one, and                                                                      
                                                                      x3 the third                                                          i  =  1,  . . . , m, j = 1, . . . , n. Thus far, we saw that
2 3 5                                                                                                                                       matrices can be used as                                     

%%%

                                                                                                                                            matrix and                                                  
a  compact way of formulating systems of linear equations so          x3                                                                                                                                
that we can                                                                                                                                 b  the  right-hand-side of (2.38). A solution to the problem
                                                                      x4                                                                    in (2.38) can                                               
write  Ax = b, see (2.10). Moreover, we defined basic matrix                                                                                                                                            
operations,                                                                                                                                be found immediately by taking 42 times the first column and
                                                                                                                                            8 times the                                                 
such  as  addition  and  multiplication  of matrices. In the                                                                                                                                           
following, we will                                                                                                                          second column so that                                       
                                                                                                                                                                                                       
focus  on solving systems of linear equations and provide an                                                                                b =                                                         
algorithm for                                                          =                                                                                                                               
                                                                                                                                                                                                       
finding the inverse of a matrix.                                                                                                                                                                       
                                                                                                                                            42                                                          
2.3.1 Particular and General Solution                                 42                                                                                                                                
                                                                                                                                            8                                                           
Before  discussing  how to generally solve systems of linear          8                                                                                                                                 
equations, let                                                                                                                                                                                         
                                                                                                                                                                                                       
us  have  a  look  at  an  example.  Consider  the system of                                                                                = 42                                                       
equations                                                             . (2.38)                                                                                                                          
                                                                                                                                            1                                                           
                                                                     The  system  has two equations and four unknowns. Therefore,                                                                      
                                                                      in general                                                            0                                                           
1 0 8 −4                                                                                                                                                                                                
                                                                      we  would  expect  infinitely many solutions. This system of                                                                     
0 1 2 12                                                             equations is                                                                                                                      
                                                                                                                                            + 8                                                        
                                                                     in  a  particularly  easy  form, where the first two columns                                                                      
                                                                      consist of a 1                                                        0                                                           
                                                                                                                                                                                                       
                                                                      P                                                                     1                                                           
                                                                                                                                                                                                       
                                                                      and  a  0. Remember that we want to find scalars x1, . . . ,                                                                     
                                                                     x4, such that                                                                                                                     
                                                                                                                                            . (2.39)                                                    
x1                                                                    4                                                                                                                                 
                                                                                                                                            Therefore,  a  solution  is [42, 8, 0, 0]⊤. This solution is
x2                                                                    i=1 xici = b, where we define ci to be the ith column of the          called a particular particular solution                     

%%%

                                                                                                                                                                                                       
solution  or special solution. However, this is not the only                                                                                0                                                           
solution of this special solution                                     (2.40)                                                                                                                            
                                                                                                                                                                                                       
system  of  linear  equations.  To  capture  all  the  other          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
solutions, we need                                                    by Cambridge University Press (2020).                                                                                            
                                                                                                                                                                                                        
to  be  creative  in generating 0 in a non-trivial way using          28 Linear Algebra                                                                                                                
the columns of                                                                                                                                                                                          
                                                                      so that 0 = 8c1 + 2c2 − 1c3 + 0c4 and (x1, x2, x3, x4) = (8,                                                                     
the matrix: Adding 0 to our special solution does not change          2, −1, 0). In                                                                                                                     
the special                                                                                                                                                                                            
                                                                      fact,  any scaling of this solution by λ1 ∈ R produces the 0                                                                      
solution.  To  do  so, we express the third column using the          vector, i.e.,                                                          = λ1(8c1 + 2c2 − c3) = 0 . (2.41)                       
first two columns                                                                                                                                                                                       
                                                                                                                                           Following  the same line of reasoning, we express the fourth
(which are of this very simple form)                                                                                                        column of the                                               
                                                                      1 0 8 −4                                                                                                                          
                                                                                                                                           matrix  in  (2.38)  using the first two columns and generate
                                                                      0 1 2 12                                                             another set of                                              
8                                                                                                                                                                                                       
                                                                                                                                           non-trivial versions of 0 as                                
2                                                                                                                                                                                                       
                                                                                                                                                                                                    
                                                                                                                                                                                                       
                                                                      λ1                                                                    1 0 8 −4                                                    
= 8                                                                                                                                                                                                    
                                                                                                                                           0 1 2 12                                                   
1                                                                                                                                                                                                       
                                                                                                                                                                                                      
0                                                                                                                                                                                                       
                                                                                                                                                                                                    
                                                                                                                                                                                                       
                                                                                                                                           λ2                                                          
+ 2                                                                                                                                                                                                    
                                                                      8                                                                                                                                
0                                                                                                                                                                                                       
                                                                      2                                                                                                                                
1                                                                                                                                                                                                       
                                                                      −1                                                                                                                               

%%%

                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
−4                                                                                                                                                                                                    
                                                                                                                                                                                                        
12                                                                                                                                                                                                    
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
−1                                                                    42                                                                                                                               
                                                                                                                                                                                                        
                                                                     8                                                                     + λ2                                                        
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
 = λ2(−4c1 + 12c2 − c4) = 0 (2.42)                                                                                                      −4                                                          
                                                                                                                                                                                                        
for  any  λ2 ∈ R. Putting everything together, we obtain all                                                                               12                                                          
solutions of the                                                                                                                                                                                        
                                                                      + λ1                                                                  0                                                           
general  solution equation system in (2.38), which is called                                                                                                                                            
the general solution, as the set                                                                                                           −1                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                   
                                                                                                                                                                                                        
x ∈ R                                                                 8                                                                                                                                
                                                                                                                                                                                                        
4                                                                     2                                                                     , λ1, λ2 ∈ R                                                
                                                                                                                                                                                                        
: x =                                                                 −1                                                                                                                               

%%%

                                                                                                                                                                                                        
                                                                   of systems of linear equations, which transform the equation          4x1 − 8x2 + 3x3 − 3x4 + x5 = 2                              
                                                                      system into                                                                                                                       
                                                                                                                                         x1 − 2x2 + x3 − x4 + x5 = 0                                 
                                                                      a  simple  form.  Then,  we can apply the three steps to the                                                                      
. (2.43)                                                              simple form that                                                      x1 − 2x2 − 3x4 + 4x5 = a                                    
                                                                                                                                                                                                        
Remark.  The  general  approach we followed consisted of the          we just discussed in the context of the example in (2.38).            . (2.44)                                                    
following                                                                                                                                                                                               
                                                                      2.3.2 Elementary Transformations                                      We  start  by  converting  this system of equations into the
three steps:                                                                                                                                compact matrix                                              
                                                                      elementary  Key  to solving a system of linear equations are                                                                      
1. Find a particular solution to Ax = b.                              elementary transformations                                            notation  Ax  =  b.  We  no  longer  mention the variables x
                                                                                                                                            explicitly and                                              
2. Find all solutions to Ax = 0.                                      transformations  that  keep  the  solution set the same, but                                                                      
                                                                      that transform the equation system                                    build the augmented matrix (in the form                     
3. Combine the solutions from steps 1. and 2. to the general                                                                                                                                            
solution.                                                             into a simpler form:                                                  A | b                                                       
                                                                                                                                                                                                        
Neither  the  general nor the particular solution is unique.          Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          ) augmented matrix                                          
♢                                                                     Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                                                                                       
The  system of linear equations in the preceding example was          2.3 Solving Systems of Linear Equations 29                                                                                        
easy to                                                                                                                                                                                                
                                                                      Exchange  of  two equations (rows in the matrix representing                                                                      
solve  because  the  matrix  in (2.38) has this particularly          the system                                                                                                                       
convenient form,                                                                                                                                                                                        
                                                                      of equations)                                                                                                                    
which  allowed  us  to  find  the particular and the general                                                                                                                                            
solution  by  inspection.  However, general equation systems          Multiplication  of  an  equation  (row)  with a constant λ ∈          −2 4 −2 −1 4 −3                                             
are not of this simple form.                                          R\{0}                                                                                                                             
                                                                                                                                            4 −8 3 −3 1 2                                               
Fortunately,  there exists a constructive algorithmic way of          Addition of two equations (rows)                                                                                                  
transforming                                                                                                                                1 −2 1 −1 1 0                                               
                                                                      Example 2.6                                                                                                                       
any system of linear equations into this particularly simple                                                                                1 −2 0 −3 4 a                                               
form: Gaussian                                                        For  a ∈ R, we seek all solutions of the following system of                                                                      
                                                                      equations:                                                                                                                       
elimination.  Key  to  Gaussian  elimination  are elementary                                                                                                                                            
transformations                                                       −2x1 + 4x2 − 2x3 − x4 + 4x5 = −3                                                                                                 

%%%

                                                                      1 −2 1 −1 1 0                                                                                                                     
                                                                                                                                           0 0 −1 −2 3 a                                               
                                                                      4 −8 3 −3 1 2                                                                                                                     
                                                                                                                                                                                                      
                                                                      −2 4 −2 −1 4 −3                                                                                                                   
Swap with R3                                                                                                                                                                                           
                                                                      1 −2 0 −3 4 a                                                                                                                     
Swap with R1                                                                                                                                                                                           
                                                                                                                                                                                                       
where  we  used  the vertical line to separate the left-hand                                                                                                                                           
side from the                                                                                                                                                                                          
                                                                                                                                            −R2 − R3                                                    
right-hand   side   in  (2.44).  We  use  ⇝  to  indicate  a                                                                                                                                           
transformation of the                                                                                                                       ⇝                                                           
                                                                                                                                                                                                       
augmented   matrix  using  elementary  transformations.  The                                                                                                                                           
augmented                                                             −4R1                                                                                                                              
                                                                                                                                                                                                       
matrix                                                                +2R1                                                                                                                              
                                                                                                                                                                                                       
A | b                                                                 −R1                                                                                                                               
                                                                                                                                                                                                       
compactly                                                             When  we  now  apply  the  indicated  transformations (e.g.,                                                                      
                                                                      subtract Row 1                                                        1 −2 1 −1 1 0                                               
represents the                                                                                                                                                                                          
                                                                      four times from Row 2), we obtain                                     0 0 −1 1 −3 2                                               
system of linear                                                                                                                                                                                        
                                                                                                                                           0 0 0 −3 6 −3                                               
equations Ax = b.                                                                                                                                                                                       
                                                                                                                                           0 0 0 0 0 a+ 1                                              
Swapping Rows 1 and 3 leads to                                                                                                                                                                          
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                      1 −2 1 −1 1 0                                                                                                                    
                                                                                                                                                                                                       
                                                                      0 0 −1 1 −3 2                                                                                                                    
                                                                                                                                                                                                       
                                                                      0 0 0 −3 6 −3                                                         ·(−1)                                                       

%%%

                                                                      row-echelon  form This (augmented) matrix is in a convenient                                                                      
·(−                                                                   form, the row-echelon form                                            x5                                                          
                                                                                                                                                                                                        
1                                                                     (REF).   Reverting  this  compact  notation  back  into  the                                                                     
                                                                      explicit notation with                                                                                                            
3                                                                                                                                                                                                      
                                                                      the variables we seek, we obtain                                                                                                  
)                                                                                                                                                                                                      
                                                                      x1 − 2x2 + x3 − x4 + x5 = 0                                                                                                       
⇝                                                                                                                                                                                                      
                                                                      x3 − x4 + 3x5 = −2                                                                                                                
                                                                                                                                                                                                      
                                                                      x4 − 2x5 = 1                                                                                                                      
                                                                                                                                                                                                      
                                                                      0 = a + 1                                                                                                                         
                                                                                                                                           =                                                           
                                                                      . (2.45)                                                                                                                          
                                                                                                                                                                                                      
                                                                      particular  solution  Only  for  a  =  −1 this system can be                                                                      
1 −2 1 −1 1 0                                                         solved. A particular solution is                                                                                                 
                                                                                                                                                                                                        
0 0 1 −1 3 −2                                                                                                                                                                                         
                                                                                                                                                                                                        
0 0 0 1 −2 1                                                                                                                                                                                          
                                                                                                                                                                                                        
0 0 0 0 0 a+ 1                                                                                                                                                                                        
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          2                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                     x1                                                                    −1                                                          
                                                                                                                                                                                                        
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          x2                                                                    1                                                           
by Cambridge University Press (2020).                                                                                                                                                                   
                                                                      x3                                                                    0                                                           
30 Linear Algebra                                                                                                                                                                                       
                                                                      x4                                                                                                                               

%%%

                                                                      2                                                                     0                                                           
                                                                                                                                                                                                       
                                                                      0                                                                     0                                                           
                                                                                                                                                                                                       
                                                                      −1                                                                    0                                                           
                                                                                                                                                                                                       
                                                                      1                                                                                                                                
                                                                                                                                                                                                       
                                                                      0                                                                                                                                
                                                                                                                                                                                                       
                                                                                                                                                                                                      
. (2.46)                                                                                                                                                                                                
                                                                                                                                                                                                      
general  solution  The  general solution, which captures the                                                                                                                                            
set of all possible solutions, is                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                      + λ2                                                        
                                                                                                                                                                                                        
                                                                                                                                                                                                 
                                                                                                                                                                                                        
x ∈ R                                                                 + λ1                                                                                                                             
                                                                                                                                                                                                        
5                                                                                                                                                                                                     
                                                                                                                                                                                                        
: x =                                                                                                                                                                                                 
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          2                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                     2                                                                     −1                                                          
                                                                                                                                                                                                        
                                                                     1                                                                     2                                                           
                                                                                                                                                                                                        

%%%

1                                                                     is in row-echelon form if                                             2.3 Solving Systems of Linear Equations 31                  
                                                                                                                                                                                                        
                                                                     All  rows  that  contain only zeros are at the bottom of the          our  lives  easier  when  we  need to determine a particular
                                                                      matrix;  correspondingly, all rows that contain at least one          solution. To do                                             
                                                                     nonzero element are on                                                                                                            
                                                                                                                                            this,  we express the right-hand side of the equation system
                                                                     top of rows that contain only zeros.                                  using the pivot                                             
                                                                                                                                                                                                        
                                                                     Looking  at nonzero rows only, the first nonzero number from          columns, such that b =                                      
                                                                      the left                                                                                                                          
                                                                                                                                           PP                                                          
                                                                      pivot  (also called the pivot or the leading coefficient) is                                                                      
                                                                     always strictly to the                                                i=1 λipi                                                    
                                                                                                                                                                                                        
, λ1, λ2 ∈ R                                                          leading coefficient right of the pivot of the row above it.           , where pi                                                  
                                                                                                                                                                                                        
                                                                     In other texts, it is                                                 , i = 1, . . . , P, are the pivot                           
                                                                                                                                                                                                        
                                                                 sometimes required                                                    columns.  The λi are determined easiest if we start with the
                                                                                                                                            rightmost pivot                                             
                                                                 that the pivot is 1.                                                                                                              
                                                                                                                                            column and work our way to the left.                        
. (2.47)                                                              Remark   (Basic   and   Free   Variables).   The   variables                                                                      
                                                                      corresponding to the                                                  In  the previous example, we would try to find λ1, λ2, λ3 so
In  the  following,  we  will  detail  a constructive way to                                                                                that                                                        
obtain a particular                                                   pivots  in  the  row-echelon form are called basic variables                                                                      
                                                                      and the other                                                         λ1                                                          
and general solution of a system of linear equations.                                                                                                                                                   
                                                                      basic variable variables are free variables. For example, in                                                                     
Remark   (Pivots   and  Staircase  Structure).  The  leading          (2.45), x1, x3, x4 are basic                                                                                                      
coefficient of a row                                                                                                                                                                                   
                                                                      free  variable variables, whereas x2, x5 are free variables.                                                                      
pivot  (first  nonzero  number  from the left) is called the          ♢                                                                                                                                
pivot and is always                                                                                                                                                                                     
                                                                      Remark  (Obtaining  a  Particular Solution). The row-echelon                                                                     
strictly  to  the  right  of  the pivot of the row above it.          form makes                                                                                                                        
Therefore,  any  equation  system in row-echelon form always                                                                                1                                                           
has a “staircase” structure. ♢                                        Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       0                                                           
row-echelon form Definition 2.6 (Row-Echelon Form). A matrix                                                                                                                                            

%%%

0                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     −1                                                                    . (2.48)                                                    
                                                                                                                                                                                                        
                                                                     −1                                                                    From here, we find relatively directly that λ3 = 1, λ2 = −1,
                                                                                                                                            λ1 = 2. When                                                
+ λ2                                                                  1                                                                                                                                 
                                                                                                                                            we put everything together, we must not forget the non-pivot
                                                                     0                                                                     columns                                                     
                                                                                                                                                                                                        
                                                                                                                                          for   which   we  set  the  coefficients  implicitly  to  0.
                                                                                                                                            Therefore, we get the                                       
                                                                                                                                                                                                      
                                                                                                                                            particular solution x = [2, 0, −1, 1, 0]⊤. ♢                
                                                                                                                                                                                                      
                                                                                                                                            Remark  (Reduced Row Echelon Form). An equation system is in
1                                                                      =                                                                   reduced reduced                                             
                                                                                                                                                                                                        
1                                                                                                                                          row-echelon  form  (also:  row-reduced  echelon  form or row
                                                                                                                                            canonical form) if row-echelon form                         
0                                                                                                                                                                                                      
                                                                                                                                            It is in row-echelon form.                                  
0                                                                                                                                                                                                      
                                                                                                                                            Every pivot is 1.                                           
                                                                                                                                                                                                      
                                                                                                                                            The pivot is the only nonzero entry in its column.          
                                                                     0                                                                                                                                 
                                                                                                                                            ♢                                                           
                                                                     −2                                                                                                                                
                                                                                                                                            The  reduced  row-echelon  form  will play an important role
                                                                     1                                                                     later in Section 2.3.3 because it allows us to determine the
                                                                                                                                            general  solution  of  a  system  of  linear  equations in a
+ λ3                                                                  0                                                                     straightforward way.                                        
                                                                                                                                                                                                        

%%%

Gaussian                                                              straightforward,  and  we  express  the non-pivot columns in          equation system.                                            
                                                                      terms of sums                                                                                                                     
Remark  (Gaussian  Elimination).  Gaussian elimination is an                                                                                To summarize, all solutions of Ax = 0, x ∈ R5 are given by  
algorithm that elimination                                            and  multiples  of the pivot columns that are on their left:                                                                      
                                                                      The second column is 3 times the first column (we can ignore                                                                     
performs  elementary  transformations  to  bring a system of          the pivot columns on the                                                                                                          
linear equations                                                                                                                                                                                   
                                                                      right of the second column). Therefore, to obtain 0, we need                                                                      
into reduced row-echelon form. ♢                                      to subtract                                                                                                                  
                                                                                                                                                                                                        
Example 2.7 (Reduced Row Echelon Form)                                ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          x ∈ R                                                       
                                                                      by Cambridge University Press (2020).                                                                                             
Verify  that  the following matrix is in reduced row-echelon                                                                                5                                                           
form (the pivots                                                      32 Linear Algebra                                                                                                                 
                                                                                                                                            : x = λ1                                                    
are in bold):                                                         the second column from three times the first column. Now, we                                                                      
                                                                      look at the                                                                                                                      
A =                                                                                                                                                                                                     
                                                                      fifth  column,  which  is  our  second non-pivot column. The                                                                     
                                                                     fifth column can                                                                                                                  
                                                                                                                                                                                                       
                                                                     be  expressed as 3 times the first pivot column, 9 times the                                                                      
                                                                      second pivot                                                                                                                     
1 3 0 0 3                                                                                                                                                                                               
                                                                      column, and −4 times the third pivot column. We need to keep                                                                     
0 0 1 0 9                                                             track of                                                                                                                          
                                                                                                                                                                                                       
0 0 0 1 −4                                                            the  indices  of the pivot columns and translate this into 3                                                                      
                                                                      times  the first column, 0 times the second column (which is          3                                                           
                                                                     a non-pivot column), 9 times                                                                                                      
                                                                                                                                            −1                                                          
 . (2.49)                                                            the  third column (which is our second pivot column), and −4                                                                      
                                                                      times the                                                             0                                                           
The  key idea for finding the solutions of Ax = 0 is to look                                                                                                                                            
at  the nonpivot columns, which we will need to express as a          fourth  column  (which  is  the third pivot column). Then we          0                                                           
(linear) combination of                                               need to subtract                                                                                                                  
                                                                                                                                            0                                                           
the  pivot  columns. The reduced row echelon form makes this          the  fifth  column  to  obtain  0.  In the end, we are still                                                                      
relatively                                                            solving a homogeneous                                                                                                            
                                                                                                                                                                                                        

%%%

                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                     , λ1, λ2 ∈ R                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
+ λ2                                                                                                                                                                                               
                                                                                                                                                                                                       
                                                                                                                                                                                                  
                                                                                                                                            0 · · · 0 1 ∗ · · · ∗ 0 ∗ · · · ∗ 0 ∗ · · · ∗               
                                                                     . (2.50)                                                                                                                          
                                                                                                                                            .                                                           
                                                                     2.3.3 The Minus-1 Trick                                                                                                           
                                                                                                                                            .                                                           
                                                                     In the following, we introduce a practical trick for reading                                                                      
                                                                      out  the  solutions  x  of  a  homogeneous  system of linear          .                                                           
                                                                     equations Ax = 0, where                                                                                                           
                                                                                                                                            .                                                           
                                                                     A ∈ Rk×n                                                                                                                          
                                                                                                                                            .                                                           
3                                                                     , x ∈ Rn                                                                                                                          
                                                                                                                                            . 0 0 · · · 0 1 ∗ · · · ∗                                   
0                                                                     .                                                                                                                                 
                                                                                                                                            .                                                           
9                                                                     To  start,  we  assume that A is in reduced row-echelon form                                                                      
                                                                      without any                                                           .                                                           
−4                                                                                                                                                                                                      
                                                                      rows that just contain zeros, i.e.,                                   .                                                           
−1                                                                                                                                                                                                      
                                                                      A =                                                                   .                                                           
                                                                                                                                                                                                       
                                                                                                                                           .                                                           
                                                                                                                                                                                                       
                                                                                                                                           .                                                           
                                                                                                                                                                                                       
                                                                                                                                           .                                                           

%%%

                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     . 0                                                         
                                                                                                                                                                                                        
. 0                                                                   .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        
.                                                                     .                                                                     .                                                           

%%%

                                                                      . We extend this matrix                                                                                                           
.                                                                                                                                           1 3 0 0 3                                                   
                                                                      to an n × n-matrix A˜ by adding n − k rows of the form                                                                            
.                                                                                                                                           0 0 1 0 9                                                   
                                                                      0 · · · 0 −1 0 · · · 0                                                                                                            
0 · · · 0 0 0 · · · 0 0 0 · · · 0 1 ∗ · · · ∗                                                                                               0 0 0 1 −4                                                  
                                                                      (2.52)                                                                                                                            
                                                                                                                                                                                                      
                                                                      so  that  the  diagonal  of the augmented matrix A˜ contains                                                                      
                                                                     either 1 or −1.                                                        . (2.53)                                                  
                                                                                                                                                                                                        
                                                                     Then,  the  columns  of A˜ that contain the −1 as pivots are          We  now augment this matrix to a 5 × 5 matrix by adding rows
                                                                      solutions of                                                          of the                                                      
                                                                                                                                                                                                       
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          form  (2.52)  at the places where the pivots on the diagonal
                                                                     Feedback: https://mml-book.com.                                       are missing                                                 
                                                                                                                                                                                                        
                                                                     2.3 Solving Systems of Linear Equations 33                            and obtain                                                  
                                                                                                                                                                                                        
                                                                     the  homogeneous equation system Ax = 0. To be more precise,          A˜ =                                                        
                                                                      these                                                                                                                             
                                                                                                                                                                                                      
                                                                      columns  form  a basis (Section 2.6.1) of the solution space                                                                      
                                                                     of Ax = 0,                                                                                                                       
                                                                                                                                                                                                        
,                                                                     which  we  will  later  call  the  kernel or null space (see                                                                     
                                                                      Section 2.7.3). kernel                                                                                                            
(2.51)                                                                                                                                                                                                 
                                                                      null space                                                                                                                        
where   ∗   can  be  an  arbitrary  real  number,  with  the                                                                                                                                           
constraints that the first                                            Example 2.8 (Minus-1 Trick)                                                                                                       
                                                                                                                                                                                                       
nonzero entry per row must be 1 and all other entries in the          Let  us  revisit  the  matrix in (2.49), which is already in                                                                      
corresponding                                                         reduced REF:                                                          1 3 0 0 3                                                   
                                                                                                                                                                                                        
column must be 0. The columns j1, . . . , jk with the pivots          A =                                                                   0 −1 0 0 0                                                  
(marked in                                                                                                                                                                                              
                                                                                                                                           0 0 1 0 9                                                   
bold) are the standard unit vectors e1, . . . , ek ∈ Rk                                                                                                                                                 
                                                                                                                                           0 0 0 1 −4                                                  

%%%

                                                                                                                                                                                                      
0 0 0 0 −1                                                                                                                                                                                              
                                                                                                                                           3                                                           
                                                                                                                                                                                                       
                                                                                                                                           0                                                           
                                                                                                                                                                                                       
                                                                      3                                                                     9                                                           
                                                                                                                                                                                                       
                                                                      −1                                                                    −4                                                          
                                                                                                                                                                                                       
                                                                      0                                                                     −1                                                          
                                                                                                                                                                                                       
                                                                      0                                                                                                                                
                                                                                                                                                                                                       
                                                                      0                                                                                                                                
. (2.54)                                                                                                                                                                                                
                                                                                                                                                                                                      
From this form, we can immediately read out the solutions of                                                                                                                                            
Ax = 0 by                                                                                                                                                                                             
                                                                                                                                                                                                        
taking the columns of A˜ , which contain −1 on the diagonal:                                                                                                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                      , λ1, λ2 ∈ R                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                 
                                                                                                                                                                                                        
x ∈ R                                                                 + λ2                                                                                                                         
                                                                                                                                                                                                        
5                                                                                                                                                                                                 
                                                                                                                                                                                                        
: x = λ1                                                                                                                                   , (2.55)                                                    
                                                                                                                                                                                                        
                                                                                                                                          which  is  identical  to  the  solution  in  (2.50)  that we
                                                                                                                                            obtained by “insight”.                                      
                                                                                                                                                                                                      
                                                                                                                                            Calculating the Inverse                                     
                                                                                                                                                                                                      
                                                                                                                                            To compute the inverse A                                    

%%%

                                                                                                                                                                                                        
−1                                                                    ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                     
                                                                      by Cambridge University Press (2020).                                                                                             
of A ∈ Rn×n                                                                                                                                                                                            
                                                                      34 Linear Algebra                                                                                                                 
, we need to find a matrix X                                                                                                                                                                           
                                                                      Example  2.9  (Calculating  an  Inverse  Matrix  by Gaussian                                                                      
that satisfies AX = In. Then, X = A                                   Elimination)                                                                                                                     
                                                                                                                                                                                                        
−1                                                                    To determine the inverse of                                           1 0 2 0 1 0 0 0                                             
                                                                                                                                                                                                        
. We can write this down as                                           A =                                                                   1 1 0 0 0 1 0 0                                             
                                                                                                                                                                                                        
a  set  of  simultaneous  linear equations AX = In, where we                                                                               1 2 0 1 0 0 1 0                                             
solve for                                                                                                                                                                                               
                                                                                                                                           1 1 1 1 0 0 0 1                                             
X  =  [x1|  · · · |xn]. We use the augmented matrix notation                                                                                                                                            
for a compact                                                                                                                                                                                         
                                                                                                                                                                                                        
representation  of  this  set of systems of linear equations                                                                                                                                          
and obtain                                                                                                                                                                                              
                                                                      1 0 2 0                                                                                                                          
A|In                                                                                                                                                                                                    
                                                                      1 1 0 0                                                                                                                          
⇝ · · · ⇝                                                                                                                                                                                               
                                                                      1 2 0 1                                                               and  use  Gaussian  elimination  to  bring  it  into reduced
In|A                                                                                                                                        row-echelon form                                            
                                                                      1 1 1 1                                                                                                                           
−1                                                                                                                                                                                                     
                                                                                                                                                                                                       
. (2.56)                                                                                                                                                                                               
                                                                                                                                                                                                       
This  means  that  if we bring the augmented equation system                                                                                                                                           
into reduced                                                                                                                                                                                           
                                                                                                                                                                                                       
row-echelon  form,  we  can  read  out  the  inverse  on the                                                                                                                                           
right-hand side of                                                                                                                          1 0 0 0 −1 2 −2 2                                           
                                                                      (2.57)                                                                                                                            
the  equation  system.  Hence,  determining the inverse of a                                                                                0 1 0 0 1 −1 2 −2                                           
matrix is equivalent to solving systems of linear equations.          we write down the augmented matrix                                                                                                

%%%

0 0 1 0 1 −1 1 −1                                                                                                                           case.  Otherwise,  under  mild assumptions (i.e., A needs to
                                                                                                                                           have linearly                                               
0 0 0 1 −1 0 −1 2                                                                                                                                                                                       
                                                                                                                                           independent columns) we can use the transformation          
                                                                                                                                                                                                       
                                                                      . (2.58)                                                              Ax = b ⇐⇒ A                                                 
                                                                                                                                                                                                       
                                                                      We   can  verify  that  (2.58)  is  indeed  the  inverse  by          ⊤Ax = A                                                     
                                                                     performing the multiplication AA−1                                                                                                
                                                                                                                                            ⊤                                                           
                                                                     and observing that we recover I4.                                                                                                 
                                                                                                                                            b ⇐⇒ x = (A                                                 
,                                                                     2.3.4 Algorithms for Solving a System of Linear Equations                                                                         
                                                                                                                                            ⊤A)                                                         
such  that  the  desired  inverse is given as its right-hand          In the following, we briefly discuss approaches to solving a                                                                      
side:                                                                 system  of  linear equations of the form Ax = b. We make the          −1A                                                         
                                                                      assumption  that  a  solution  exists.  Should  there  be no                                                                      
A                                                                     solution, we need to resort to approximate                            ⊤                                                           
                                                                                                                                                                                                        
−1 =                                                                  solutions, which we do not cover in this chapter. One way to          b (2.59)                                                    
                                                                      solve  the  approximate  problem  is  using  the approach of                                                                      
                                                                     linear regression, which we                                           Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
                                                                                                                                            Feedback: https://mml-book.com.                             
                                                                     discuss in detail in Chapter 9.                                                                                                   
                                                                                                                                            2.4 Vector Spaces 35                                        
                                                                     In special cases, we may be able to determine the inverse A                                                                       
                                                                                                                                            and use the Moore-Penrose pseudo-inverse (A                 
                                                                     −1                                                                                                                                
                                                                                                                                            ⊤A)                                                         
−1 2 −2 2                                                             , such                                                                                                                            
                                                                                                                                            −1A                                                         
1 −1 2 −2                                                             that the solution of Ax = b is given as x = A                                                                                     
                                                                                                                                            ⊤                                                           
1 −1 1 −1                                                             −1                                                                                                                                
                                                                                                                                            to determine the Moore-Penrose                              
−1 0 −1 2                                                             b. However, this is                                                                                                               
                                                                                                                                            pseudo-inverse  solution  (2.59)  that  solves Ax = b, which
                                                                     only  possible if A is a square matrix and invertible, which          also corresponds to the minimum norm least-squares solution.
                                                                      is often not the                                                      A disadvantage of this approach is that                     
                                                                                                                                                                                                       

%%%

it  requires many computations for the matrix-matrix product          gradients. We refer to the books                                      characterized vectors as                                    
and computing the inverse of A                                                                                                                                                                          
                                                                      by Stoer and Burlirsch (2002), Strang (2003), and Liesen and          objects  that  can  be  added  together  and multiplied by a
⊤A. Moreover, for reasons of numerical precision it                   Mehrmann                                                              scalar, and they                                            
                                                                                                                                                                                                        
is  generally  not  recommended  to  compute  the inverse or          (2015) for further details.                                           remain  objects  of  the  same  type.  Now,  we are ready to
pseudo-inverse.                                                                                                                             formalize this,                                             
                                                                      Let  x∗  be  a  solution  of  Ax  = b. The key idea of these                                                                      
In  the  following, we therefore briefly discuss alternative          iterative methods                                                     and  we  will  start  by introducing the concept of a group,
approaches to                                                                                                                               which is a set                                              
                                                                      is to set up an iteration of the form                                                                                             
solving systems of linear equations.                                                                                                        of  elements and an operation defined on these elements that
                                                                      x                                                                     keeps some                                                  
Gaussian  elimination plays an important role when computing                                                                                                                                            
determinants  (Section  4.1),  checking  whether  a  set  of          (k+1) = Cx(k) + d (2.60)                                              structure of the set intact.                                
vectors is linearly independent (Section 2.5), computing the                                                                                                                                            
inverse of a matrix (Section 2.2.2),                                  for suitable C and d that reduces the residual error ∥x               ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
computing   the  rank  of  a  matrix  (Section  2.6.2),  and          (k+1)−x∗∥ in every                                                                                                                
determining a basis                                                                                                                         36 Linear Algebra                                           
                                                                      iteration  and  converges to x∗. We will introduce norms ∥ ·                                                                      
of  a  vector space (Section 2.6.1). Gaussian elimination is          ∥, which allow                                                        2.4.1 Groups                                                
an intuitive and                                                                                                                                                                                        
                                                                      us to compute similarities between vectors, in Section 3.1.           Groups  play  an important role in computer science. Besides
constructive  way to solve a system of linear equations with                                                                                providing a                                                 
thousands of                                                          2.4 Vector Spaces                                                                                                                 
                                                                                                                                            fundamental  framework  for  operations  on  sets,  they are
variables.  However, for systems with millions of variables,          Thus  far, we have looked at systems of linear equations and          heavily used in                                             
it  is  impractical  as  the  required  number of arithmetic          how to solve                                                                                                                      
operations scales cubically in the                                                                                                          cryptography, coding theory, and graphics.                  
                                                                      them  (Section 2.3). We saw that systems of linear equations                                                                      
number of simultaneous equations.                                     can  be  compactly  represented using matrix-vector notation          Definition  2.7 (Group). Consider a set G and an operation ⊗
                                                                      (2.10). In the following,                                             : G ×G → G                                                  
In  practice,  systems  of  many linear equations are solved                                                                                                                                            
indirectly,  by either stationary iterative methods, such as          we  will  have  a  closer  look  at  vector  spaces, i.e., a          group  defined  on  G. Then G := (G, ⊗) is called a group if
the  Richardson  method,  the Jacobi method, the Gauß-Seidel          structured space in which                                             the following hold:                                         
method, and the successive over-relaxation                                                                                                                                                              
                                                                      vectors live.                                                         closure                                                     
method,  or  Krylov  subspace  methods,  such  as  conjugate                                                                                                                                            
gradients,  generalized  minimal  residual,  or  biconjugate          In   the   beginning   of   this   chapter,   we  informally          1. Closure of G under ⊗: ∀x, y ∈ G : x ⊗ y ∈ G associativity

%%%

                                                                      (Z, +) is an Abelian group.                                                                                                       
2. Associativity: ∀x, y, z ∈ G : (x ⊗ y) ⊗ z = x ⊗ (y ⊗ z)                                                                                  addition as defined in (2.61)).                             
                                                                      N  (N0,  +)  is  not  a  group: Although (N0, +) possesses a                                                                      
neutral element                                                       neutral element 0 := N ∪ {0}                                          Let us have a closer look at (Rn×n                          
                                                                                                                                                                                                        
inverse  element 3. Neutral element: ∃e ∈ G ∀x ∈ G : x ⊗ e =          (0), the inverse elements are missing.                                , ·), i.e., the set of n×n-matrices with                    
x and e ⊗ x = x                                                                                                                                                                                         
                                                                      (Z,  ·)  is  not a group: Although (Z, ·) contains a neutral          matrix multiplication as defined in (2.13).                 
4. Inverse element: ∀x ∈ G ∃y ∈ G : x ⊗ y = e and y ⊗ x = e,          element (1), the                                                                                                                  
where e is                                                                                                                                  –   Closure  and  associativity  follow  directly  from  the
                                                                      inverse elements for any z ∈ Z, z ̸= ±1, are missing.                 definition of matrix                                        
the neutral element. We often write x                                                                                                                                                                   
                                                                      (R,  ·)  is  not a group since 0 does not possess an inverse          multiplication.                                             
−1                                                                    element.                                                                                                                          
                                                                                                                                            –  Neutral  element:  The  identity matrix In is the neutral
to denote the inverse element                                         (R\{0}, ·) is Abelian.                                                element with                                                
                                                                                                                                                                                                        
of x.                                                                 (Rn                                                                   respect to matrix multiplication “·” in (Rn×n               
                                                                                                                                                                                                        
Remark.  The  inverse element is defined with respect to the          , +),(Z                                                               , ·).                                                       
operation ⊗                                                                                                                                                                                             
                                                                      n                                                                     Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
and does not necessarily mean 1                                                                                                             Feedback: https://mml-book.com.                             
                                                                      , +), n ∈ N are Abelian if + is defined componentwise, i.e.,                                                                      
x                                                                                                                                           2.4 Vector Spaces 37                                        
                                                                      (x1, · · · , xn) + (y1, · · · , yn) = (x1 + y1, · · · , xn +                                                                      
. ♢                                                                   yn). (2.61)                                                           –  Inverse  element:  If  the inverse exists (A is regular),
                                                                                                                                            then A                                                      
Abelian  group  If  additionally  ∀x, y ∈ G : x ⊗ y = y ⊗ x,          Then, (x1, · · · , xn)                                                                                                            
then G = (G, ⊗) is an Abelian                                                                                                               −1                                                          
                                                                      −1                                                                                                                                
group (commutative).                                                                                                                        is the                                                      
                                                                      := (−x1, · · · , −xn) is the inverse element and                                                                                  
Example 2.10 (Groups)                                                                                                                       inverse element of A ∈ Rn×n                                 
                                                                      e = (0, · · · , 0) is the neutral element.                                                                                        
Let  us have a look at some examples of sets with associated                                                                                , and in exactly this case (Rn×n                            
operations                                                            (Rm×n                                                                                                                             
                                                                                                                                            , ·) is a                                                   
and see whether they are groups:                                      ,   +),   the  set  of  m  ×  n-matrices  is  Abelian  (with                                                                      
                                                                      componentwise                                                         group, called the general linear group.                     

%%%

                                                                                                                                            element-wise multiplication, such that c = ab               
Definition  2.8  (General  Linear Group). The set of regular          + : V × V → V (2.62)                                                                                                              
(invertible)                                                                                                                                with  cj  = aj bj . This “array multiplication” is common to
                                                                      · : R × V → V (2.63)                                                  many  programming languages but makes mathematically limited
matrices A ∈ Rn×n                                                                                                                           sense using the standard rules for matrix multiplication: By
                                                                      where                                                                 treating vectors as n × 1 matrices                          
is a group with respect to matrix multiplication as                                                                                                                                                     
                                                                      1. (V, +) is an Abelian group                                         ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
defined  in  (2.13) and is called general linear group GL(n,                                                                                by Cambridge University Press (2020).                       
R). However, general linear group                                     2. Distributivity:                                                                                                                
                                                                                                                                            38 Linear Algebra                                           
since matrix multiplication is not commutative, the group is          1. ∀λ ∈ R, x, y ∈ V : λ · (x + y) = λ · x + λ · y                                                                                 
not Abelian.                                                                                                                                (which  we usually do), we can use the matrix multiplication
                                                                      2. ∀λ, ψ ∈ R, x ∈ V : (λ + ψ) · x = λ · x + ψ · x                     as defined                                                  
2.4.2 Vector Spaces                                                                                                                                                                                     
                                                                      3.  Associativity  (outer  operation):  ∀λ,  ψ  ∈ R, x ∈ V :          in  (2.13).  However,  then the dimensions of the vectors do
When  we  discussed  groups,  we  looked at sets G and inner          λ·(ψ·x) = (λψ)·x                                                      not match. Only                                             
operations on                                                                                                                                                                                           
                                                                      4.  Neutral  element with respect to the outer operation: ∀x          the  following  multiplications for vectors are defined: ab⊤
G, i.e., mappings G × G → G that only operate on elements in          ∈ V : 1·x = x                                                         ∈ Rn×n                                                      
G. In the                                                                                                                                                                                               
                                                                      The  elements  x ∈ V are called vectors. The neutral element          outer product (outer                                        
following,  we  will  consider  sets  that in addition to an          of (V, +) is vector                                                                                                               
inner operation +                                                                                                                           product), a                                                 
                                                                      the zero vector 0 = [0, . . . , 0]⊤, and the inner operation                                                                      
also  contain  an outer operation ·, the multiplication of a          + is called vector vector addition                                    ⊤b ∈ R (inner/scalar/dot product). ♢                        
vector x ∈ G by                                                                                                                                                                                         
                                                                      addition.  The  elements  λ  ∈  R are called scalars and the          Example 2.11 (Vector Spaces)                                
a  scalar  λ  ∈  R. We can think of the inner operation as a          outer operation scalar                                                                                                            
form of addition,                                                                                                                           Let us have a look at some important examples:              
                                                                      · is a multiplication by scalars. Note that a scalar product                                                                      
and  the outer operation as a form of scaling. Note that the          is something multiplication by                                        V = Rn                                                      
inner/outer                                                                                                                                                                                             
                                                                      different, and we will get to this in Section 3.2 scalars             ,  n  ∈  N  is  a  vector  space  with operations defined as
operations have nothing to do with inner/outer products.                                                                                    follows:                                                    
                                                                      .                                                                                                                                 
Definition  2.9 (Vector Space). A real-valued vector space V                                                                                –  Addition:  x+y  =  (x1,  .  .  . , xn)+(y1, . . . , yn) =
= (V, +, ·) is vector space                                           Remark. A “vector multiplication” ab, a, b ∈ Rn                       (x1+y1, . . . , xn+yn)                                      
                                                                                                                                                                                                        
a set V with two operations                                           ,   is  not  defined.  Theoretically,  we  could  define  an          for all x, y ∈ Rn                                           

%%%

                                                                      is defined elementwise for all A, B ∈ V                               V  =  C, with the standard definition of addition of complex
–  Multiplication by scalars: λx = λ(x1, . . . , xn) = (λx1,                                                                                numbers.                                                    
. . . , λxn) for                                                      – Multiplication by scalars: λA =                                                                                                 
                                                                                                                                            Remark.  In the following, we will denote a vector space (V,
all λ ∈ R, x ∈ Rn                                                                                                                          +, ·) by V                                                  
                                                                                                                                                                                                        
V = Rm×n                                                                                                                                   when  +  and  ·  are the standard vector addition and scalar
                                                                                                                                            multiplication.                                             
, m, n ∈ N is a vector space with                                                                                                                                                                      
                                                                                                                                            Moreover, we will use the notation x ∈ V for vectors in V to
– Addition: A + B =                                                   λa11 · · · λa1n                                                       simplify                                                    
                                                                                                                                                                                                        
                                                                     .                                                                     notation. ♢                                                 
                                                                                                                                                                                                        
                                                                     .                                                                     Remark. The vector spaces Rn                                
                                                                                                                                                                                                        
                                                                     .                                                                     , Rn×1                                                      
                                                                                                                                                                                                        
a11 + b11 · · · a1n + b1n                                             .                                                                     , R1×n are only different in the way                        
                                                                                                                                                                                                        
.                                                                     .                                                                     we  write  vectors.  In  the  following,  we will not make a
                                                                                                                                            distinction between                                         
.                                                                     .                                                                                                                                 
                                                                                                                                            Rn and Rn×1                                                 
.                                                                     λam1 · · · λamn                                                                                                                   
                                                                                                                                            column  vector , which allows us to write n-tuples as column
.                                                                                                                                          vectors                                                     
                                                                                                                                                                                                        
.                                                                                                                                          x =                                                         
                                                                                                                                                                                                        
.                                                                                                                                                                                                     
                                                                                                                                                                                                        
am1 + bm1 · · · amn + bmn                                             as defined in                                                                                                                    
                                                                                                                                                                                                        
                                                                     Section 2.2. Remember that Rm×n                                                                                                  
                                                                                                                                                                                                        
                                                                     is equivalent to Rmn                                                  x1                                                          
                                                                                                                                                                                                        
                                                                     .                                                                     .                                                           
                                                                                                                                                                                                        

%%%

.                                                                     Vector subspaces are a                                                                                                            
                                                                                                                                            Example 2.12 (Vector Subspaces)                             
.                                                                     key  idea  in  machine  learning.  For  example,  Chapter 10                                                                      
                                                                      demonstrates how                                                      Let us have a look at some examples:                        
xn                                                                                                                                                                                                      
                                                                      to use vector subspaces for dimensionality reduction.                 For  every  vector  space  V  ,  the trivial subspaces are V
                                                                                                                                           itself and {0}.                                             
                                                                      Definition  2.10  (Vector  Subspace). Let V = (V, +, ·) be a                                                                      
                                                                     vector space                                                          Only example D in Figure 2.1 is a subspace of R2            
                                                                                                                                                                                                        
 . (2.64)                                                            and  U  ⊆  V,  U  ̸=  ∅. Then U = (U, +, ·) is called vector          (with the usual inner/                                      
                                                                      subspace of V (or vector subspace                                                                                                 
This   simplifies   the   notation  regarding  vector  space                                                                                outer  operations).  In  A  and  C,  the closure property is
operations. However,                                                  linear  subspace)  if  U  is  a vector space with the vector          violated; B does                                            
                                                                      space operations + linear subspace                                                                                                
we do distinguish between Rn×1 and R1×n                                                                                                     not contain 0.                                              
                                                                      and · restricted to U ×U and R×U. We write U ⊆ V to denote a                                                                      
row  vector (the row vectors) to avoid confusion with matrix          subspace                                                              The solution set of a homogeneous system of linear equations
multiplication.  By  default,  we write x to denote a column                                                                                Ax = 0                                                      
vector, and a row vector is denoted by x                              U of V .                                                                                                                          
                                                                                                                                            with n unknowns x = [x1, . . . , xn]                        
⊤ transpose , the transpose of x. ♢                                   If  U ⊆ V and V is a vector space, then U naturally inherits                                                                      
                                                                      many  properties directly from V because they hold for all x          ⊤ is a subspace of Rn                                       
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          ∈ V, and in particular for                                                                                                        
Feedback: https://mml-book.com.                                                                                                             .                                                           
                                                                      all  x  ∈ U ⊆ V. This includes the Abelian group properties,                                                                      
2.4 Vector Spaces 39                                                  the   distributivity,  the  associativity  and  the  neutral          The  solution of an inhomogeneous system of linear equations
                                                                      element. To determine whether                                         Ax =                                                        
2.4.3 Vector Subspaces                                                                                                                                                                                  
                                                                      (U, +, ·) is a subspace of V we still do need to show                 b, b ̸= 0 is not a subspace of Rn                           
In  the  following,  we  will  introduce  vector  subspaces.                                                                                                                                            
Intuitively, they are                                                 1. U ̸= ∅, in particular: 0 ∈ U                                       .                                                           
                                                                                                                                                                                                        
sets  contained  in  the  original  vector  space  with  the          2. Closure of U:                                                      The intersection of arbitrarily many subspaces is a subspace
property that when                                                                                                                          itself.                                                     
                                                                      a.  With  respect to the outer operation: ∀λ ∈ R ∀x ∈ U : λx                                                                      
we  perform  vector space operations on elements within this          ∈ U.                                                                  Figure 2.1 Not all                                          
subspace, we                                                                                                                                                                                            
                                                                      b.  With respect to the inner operation: ∀x, y ∈ U : x + y ∈          subsets of R2 are                                           
will  never  leave  it.  In  this  sense, they are “closed”.          U.                                                                                                                                

%%%

subspaces. In A and                                                   vectors together                                                                                                                  
                                                                                                                                            The 0-vector can always be written as the linear combination
C, the closure                                                        and   multiply  them  with  scalars.  The  closure  property          of k vectors x1, . . . , xk because 0 =                     
                                                                      guarantees that we                                                                                                                
property is violated;                                                                                                                       Pk                                                          
                                                                      end  up  with another vector in the same vector space. It is                                                                      
B does not contain                                                    possible to find                                                      i=1 0xi                                                     
                                                                                                                                                                                                        
0. Only D is a                                                        a set of vectors with which we can represent every vector in          is always true. In the following,                           
                                                                      the vector                                                                                                                        
subspace.                                                                                                                                   we  are  interested  in non-trivial linear combinations of a
                                                                      space  by adding them together and scaling them. This set of          set of vectors to                                           
0 0 0 0                                                               vectors is                                                                                                                        
                                                                                                                                            represent  0, i.e., linear combinations of vectors x1, . . .
A                                                                     a  basis,  and we will discuss them in Section 2.6.1. Before          , xk, where not all                                         
                                                                      we get there,                                                                                                                     
B                                                                                                                                           coefficients λi                                             
                                                                      we   will   need   to   introduce  the  concepts  of  linear                                                                      
C                                                                     combinations and linear                                               in (2.65) are 0.                                            
                                                                                                                                                                                                        
D                                                                     independence.                                                         Definition  2.12  (Linear (In)dependence). Let us consider a
                                                                                                                                            vector space                                                
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          Definition  2.11  (Linear  Combination).  Consider  a vector                                                                      
by Cambridge University Press (2020).                                 space V and a                                                         V  with  k  ∈  N  and  x1,  .  .  . , xk ∈ V . If there is a
                                                                                                                                            non-trivial linear combination, such that 0 =               
40 Linear Algebra                                                     finite  number of vectors x1, . . . , xk ∈ V . Then, every v                                                                      
                                                                      ∈ V of the form                                                       Pk                                                          
Remark. Every subspace U ⊆ (Rn                                                                                                                                                                          
                                                                      v = λ1x1 + · · · + λkxk =                                             i=1 λixi with at least one λi ̸= 0, the vectors             
,  +,  ·)  is  the solution space of a homogeneous system of                                                                                                                                            
linear equations Ax = 0 for x ∈ Rn                                    X                                                                     linearly dependent x1, . . . , xk are linearly dependent. If
                                                                                                                                            only the trivial solution exists, i.e.,                     
. ♢                                                                   k                                                                                                                                 
                                                                                                                                            linearly  λ1 = . . . = λk = 0 the vectors x1, . . . , xk are
2.5 Linear Independence                                               i=1                                                                   linearly independent.                                       
                                                                                                                                                                                                        
In  the  following, we will have a close look at what we can          λixi ∈ V (2.65)                                                       independent                                                 
do with vectors                                                                                                                                                                                         
                                                                      linear  combination  with  λ1,  .  .  . , λk ∈ R is a linear          Linear independence is one of the most important concepts in
(elements  of  the  vector space). In particular, we can add          combination of the vectors x1, . . . , xk.                            linear                                                      

%%%

                                                                                                                                                                                                        
algebra.  Intuitively, a set of linearly independent vectors          “374 km Southwest” vector (purple) are linearly independent.          are linearly independent:                                   
consists of vectors                                                   This means                                                                                                                        
                                                                                                                                            k   vectors   are  either  linearly  dependent  or  linearly
that  have  no  redundancy,  i.e., if we remove any of those          the  Southwest  vector  cannot  be described in terms of the          independent. There                                          
vectors from                                                          Northwest vector, and vice versa. However, the third “751 km                                                                      
                                                                      West” vector (black) is a                                             is no third option.                                         
the  set,  we  will  lose  something.  Throughout  the  next                                                                                                                                            
sections, we will                                                     linear  combination  of  the other two vectors, and it makes          If at least one of the vectors x1, . . . , xk is 0 then they
                                                                      the  set  of vectors linearly dependent. Equivalently, given          are  linearly  dependent.  The same holds if two vectors are
formalize this intuition more.                                        “751 km West” and “374 km                                             identical.                                                  
                                                                                                                                                                                                        
Example 2.13 (Linearly Dependent Vectors)                             Southwest”  can  be  linearly  combined  to  obtain  “506 km          The  vectors {x1, . . . , xk : xi ̸= 0, i = 1, . . . , k}, k
                                                                      Northwest”.                                                           ⩾ 2, are linearly                                           
A  geographic  example  may  help  to clarify the concept of                                                                                                                                            
linear  independence. A person in Nairobi (Kenya) describing          Figure 2.2                                                            dependent  if and only if (at least) one of them is a linear
where Kigali (Rwanda) is                                                                                                                    combination                                                 
                                                                      Geographic example                                                                                                                
might  say  ,“You  can  get  to Kigali by first going 506 km                                                                                of the others. In particular, if one vector is a multiple of
Northwest  to  Kampala (Uganda) and then 374 km Southwest.”.          (with crude                                                           another vector,                                             
This is sufficient information                                                                                                                                                                          
                                                                      approximations to                                                     i.e.,  xi = λxj , λ ∈ R then the set {x1, . . . , xk : xi ̸=
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                0, i = 1, . . . , k}                                        
Feedback: https://mml-book.com.                                       cardinal directions)                                                                                                              
                                                                                                                                            is linearly dependent.                                      
2.5 Linear Independence 41                                            of linearly                                                                                                                       
                                                                                                                                            A  practical  way of checking whether vectors x1, . . . , xk
to  describe  the  location of Kigali because the geographic          dependent vectors                                                     ∈ V are linearly                                            
coordinate system may be considered a two-dimensional vector                                                                                                                                            
space (ignoring altitude                                              in a                                                                  independent  is  to  use  Gaussian  elimination:  Write  all
                                                                                                                                            vectors as columns                                          
and  the Earth’s curved surface). The person may add, “It is          two-dimensional                                                                                                                   
about 751 km                                                                                                                                of  a  matrix  A  and perform Gaussian elimination until the
                                                                      space (plane).                                                        matrix is in                                                
West  of  here.” Although this last statement is true, it is                                                                                                                                            
not necessary to                                                      506  km  Northwest  751  km  West  374  km  Southwest 374 km          row   echelon   form   (the   reduced  row-echelon  form  is
                                                                      SouthwestKampala Nairobi Kigali                                       unnecessary here):                                          
find  Kigali  given the previous information (see Figure 2.2                                                                                                                                            
for   an   illustration).  In  this  example,  the  “506  km          Remark.  The  following  properties  are  useful to find out          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
Northwest” vector (blue) and the                                      whether vectors                                                       by Cambridge University Press (2020).                       

%%%

                                                                      Consider R4 with                                                      1                                                           
42 Linear Algebra                                                                                                                                                                                       
                                                                      x1 =                                                                  0                                                           
– The pivot columns indicate the vectors, which are linearly                                                                                                                                            
independent  of  the vectors on the left. Note that there is                                                                               2                                                           
an ordering of vectors when the matrix is built.                                                                                                                                                        
                                                                                                                                                                                                      
–   The   non-pivot  columns  can  be  expressed  as  linear                                                                                                                                            
combinations of                                                                                                                                                                                       
                                                                                                                                                                                                        
the   pivot   columns  on  their  left.  For  instance,  the                                                                                                                                          
row-echelon form                                                                                                                                                                                        
                                                                      1                                                                                                                                
                                                                                                                                                                                                       
                                                                      2                                                                     , x3 =                                                      
1 3 0                                                                                                                                                                                                   
                                                                      −3                                                                                                                               
0 0 2                                                                                                                                                                                                  
                                                                      4                                                                                                                                
(2.66)                                                                                                                                                                                                  
                                                                                                                                                                                                      
tells us that the first and third columns are pivot columns.                                                                                                                                            
The  second column is a non-pivot column because it is three                                                                                                                                          
times the first                                                                                                                                                                                         
                                                                                                                                           −1                                                          
column.                                                                                                                                                                                                 
                                                                                                                                           −2                                                          
All  column  vectors are linearly independent if and only if                                                                                                                                            
all columns                                                           , x2 =                                                                1                                                           
                                                                                                                                                                                                        
are  pivot  columns.  If  there  is  at  least one non-pivot                                                                               1                                                           
column, the columns                                                                                                                                                                                     
                                                                                                                                                                                                      
(and,  therefore,  the  corresponding  vectors) are linearly                                                                                                                                            
dependent.                                                                                                                                                                                            
                                                                                                                                                                                                        
♢                                                                                                                                                                                                     
                                                                                                                                                                                                        
Example 2.14                                                          1                                                                                                                                
                                                                                                                                                                                                        

%%%

. (2.67)                                                                                                                                                                                                
                                                                      1                                                                      = 0 (2.68)                                                
To  check whether they are linearly dependent, we follow the                                                                                                                                            
general approach and solve                                            1                                                                     for λ1, . . . , λ3. We write the vectors xi                 
                                                                                                                                                                                                        
λ1x1 + λ2x2 + λ3x3 = λ1                                               0                                                                     , i = 1, 2, 3, as the columns of a                          
                                                                                                                                                                                                        
                                                                     2                                                                     matrix and apply elementary row operations until we identify
                                                                                                                                            the pivot                                                   
                                                                                                                                                                                                      
                                                                                                                                            columns:                                                    
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
1                                                                                                                                                                                                      
                                                                                                                                                                                                       
2                                                                     + λ3                                                                                                                              
                                                                                                                                                                                                       
−3                                                                                                                                                                                                     
                                                                                                                                            1 1 −1                                                      
4                                                                                                                                                                                                      
                                                                                                                                            2 1 −2                                                      
                                                                                                                                                                                                      
                                                                                                                                            −3 0 1                                                      
                                                                                                                                                                                                      
                                                                                                                                            4 2 1                                                       
                                                                     −1                                                                                                                                
                                                                                                                                                                                                       
                                                                     −2                                                                                                                                
                                                                                                                                                                                                       
+ λ2                                                                  1                                                                                                                                 
                                                                                                                                                                                                       
                                                                     1                                                                                                                                 
                                                                                                                                             ⇝ · · · ⇝                                                 
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      

%%%

                                                                                                                                                                                                      
                                                                      x1 =                                                                                                                              
                                                                                                                                                                                                      
                                                                      X                                                                                                                                 
1 1 −1                                                                                                                                                                                                 
                                                                      k                                                                                                                                 
0 1 0                                                                                                                                       λ1j                                                         
                                                                      i=1                                                                                                                               
0 0 1                                                                                                                                       .                                                           
                                                                      λi1bi                                                                                                                             
0 0 0                                                                                                                                       .                                                           
                                                                      ,                                                                                                                                 
                                                                                                                                           .                                                           
                                                                      .                                                                                                                                 
                                                                                                                                           λkj                                                         
                                                                      .                                                                                                                                 
                                                                                                                                                                                                      
                                                                      .                                                                                                                                 
                                                                                                                                                                                                      
                                                                      xm =                                                                                                                              
. (2.69)                                                                                                                                     , j = 1, . . . , m , (2.71)                               
                                                                      X                                                                                                                                 
Here,  every  column  of  the  matrix  is  a  pivot  column.                                                                                in a more compact form.                                     
Therefore, there is no                                                k                                                                                                                                 
                                                                                                                                            We  want  to  test  whether  x1,  .  .  .  , xm are linearly
non-trivial  solution, and we require λ1 = 0, λ2 = 0, λ3 = 0          i=1                                                                   independent. For this                                       
to solve the                                                                                                                                                                                            
                                                                      λimbi                                                                 purpose, we follow the general approach of testing when Pm  
equation  system. Hence, the vectors x1, x2, x3 are linearly                                                                                                                                            
independent.                                                          .                                                                     j=1 ψjxj = 0.                                               
                                                                                                                                                                                                        
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          (2.70)                                                                With (2.71), we obtain                                      
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                      Defining  B  =  [b1, . . . , bk] as the matrix whose columns          Xm                                                          
2.5 Linear Independence 43                                            are the linearly                                                                                                                  
                                                                                                                                            j=1                                                         
Remark.   Consider   a   vector  space  V  with  k  linearly          independent vectors b1, . . . , bk, we can write                                                                                  
independent vectors                                                                                                                         ψjxj =                                                      
                                                                      xj = Bλj , λj =                                                                                                                   
b1, . . . , bk and m linear combinations                                                                                                    Xm                                                          

%%%

                                                                      linearly independent? To answer this                                                                                             
j=1                                                                                                                                                                                                     
                                                                      question, we investigate whether the column vectors                                                                              
ψjBλj = B                                                                                                                                                                                               
                                                                                                                                           −4                                                          
Xm                                                                                                                                                                                                      
                                                                                                                                         −2                                                          
j=1                                                                                                                                                                                                     
                                                                                                                                         0                                                           
ψjλj . (2.72)                                                                                                                                                                                           
                                                                                                                                           4                                                           
This means that {x1, . . . , xm} are linearly independent if                                                                                                                                            
and only if the                                                                                                                                                                                       
                                                                                                                                                                                                        
column vectors {λ1, . . . , λm} are linearly independent.                                                                                                                                             
                                                                                                                                                                                                        
♢                                                                                                                                                                                                     
                                                                                                                                                                                                        
Remark.  In  a  vector  space V , m linear combinations of k          1                                                                                                                                
vectors x1, . . . , xk                                                                                                                                                                                  
                                                                      −2                                                                    ,                                                           
are linearly dependent if m > k. ♢                                                                                                                                                                      
                                                                      1                                                                                                                                
Example 2.15                                                                                                                                                                                            
                                                                      −1                                                                                                                               
Consider  a  set of linearly independent vectors b1, b2, b3,                                                                                                                                            
b4 ∈ Rn and                                                                                                                                                                                           
                                                                                                                                                                                                        
x1 = b1 − 2b2 + b3 − b4                                                                                                                                                                               
                                                                                                                                                                                                        
x2 = −4b1 − 2b2 + 4b4                                                                                                                      2                                                           
                                                                                                                                                                                                        
x3 = 2b1 + 3b2 − b3 − 3b4                                                                                                                  3                                                           
                                                                                                                                                                                                        
x4 = 17b1 − 10b2 + 11b3 + b4                                          ,                                                                     −1                                                          
                                                                                                                                                                                                        
. (2.73)                                                                                                                                   −3                                                          
                                                                                                                                                                                                        
Are the vectors x1, . . . , x4 ∈ Rn                                                                                                                                                                   
                                                                                                                                                                                                        

%%%

                                                                     ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
                                                                      by Cambridge University Press (2020).                                                                                            
                                                                                                                                                                                                       
                                                                      44 Linear Algebra                                                                                                                
                                                                                                                                                                                                       
                                                                      are  linearly  independent.  The reduced row-echelon form of                                                                     
,                                                                     the  corresponding  linear  equation system with coefficient                                                                      
                                                                      matrix                                                                1 0 0 −7                                                    
                                                                                                                                                                                                       
                                                                      A =                                                                   0 1 0 −15                                                   
                                                                                                                                                                                                       
                                                                                                                                           0 0 1 −18                                                   
                                                                                                                                                                                                       
                                                                                                                                           0 0 0 0                                                     
                                                                                                                                                                                                       
                                                                                                                                                                                                      
17                                                                                                                                                                                                      
                                                                                                                                                                                                      
−10                                                                                                                                                                                                     
                                                                      1 −4 2 17                                                                                                                        
11                                                                                                                                                                                                      
                                                                      −2 −2 3 −10                                                                                                                      
1                                                                                                                                                                                                       
                                                                      1 0 −1 11                                                             . (2.76)                                                    
                                                                                                                                                                                                       
                                                                      −1 4 −3 1                                                             We  see  that  the  corresponding  linear equation system is
                                                                                                                                           non-trivially  solvable:  The  last  column  is  not a pivot
                                                                                                                                           column, and x4 = −7x1−15x2−18x3.                            
                                                                                                                                                                                                       
                                                                                                                                           Therefore,  x1,  . . . , x4 are linearly dependent as x4 can
                                                                                                                                           be expressed as a                                           
                                                                                                                                                                                                       
                                                                                                                                           linear combination of x1, . . . , x3.                       
                                                                                                                                                                                                       
                                                                                                                                         2.6 Basis and Rank                                          
                                                                      (2.75)                                                                                                                            
                                                                                                                                         In a vector space V , we are particularly interested in sets
                                                                      is given as                                                           of vectors A that                                           
(2.74)                                                                                                                                                                                                  
                                                                                                                                           possess  the  property that any vector v ∈ V can be obtained

%%%

by a linear                                                           minimal  V.  A  generating  set  A of V is called minimal if          x =                                                         
                                                                      there exists no smaller set                                                                                                       
combination  of  vectors  in  A.  These  vectors are special                                                                                X                                                           
vectors, and in the                                                   A˜  ⊊  A  ⊆  V  that  spans  V  . Every linearly independent                                                                      
                                                                      generating set of V                                                   k                                                           
following, we will characterize them.                                                                                                                                                                   
                                                                      basis is minimal and is called a basis of V .                         i=1                                                         
2.6.1 Generating Set and Basis                                                                                                                                                                          
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          λibi =                                                      
Definition 2.13 (Generating Set and Span). Consider a vector          Feedback: https://mml-book.com.                                                                                                   
space V =                                                                                                                                   X                                                           
                                                                      2.6 Basis and Rank 45                                                                                                             
(V,  +,  ·)  and set of vectors A = {x1, . . . , xk} ⊆ V. If                                                                                k                                                           
every vector v ∈                                                      Let V = (V, +, ·) be a vector space and B ⊆ V, B ̸= ∅. Then,                                                                      
                                                                      the                                                                   i=1                                                         
V  can  be  expressed as a linear combination of x1, . . . ,                                                                                                                                            
xk, A is called a                                                     following statements are equivalent: A basis is a minimal             ψibi (2.77)                                                 
                                                                                                                                                                                                        
generating  set  generating set of V . The set of all linear          generating set and a                                                  and λi                                                      
combinations of vectors in A is                                                                                                                                                                         
                                                                      maximal linearly                                                      , ψi ∈ R, bi ∈ B it follows that λi = ψi                    
span  called  the span of A. If A spans the vector space V ,                                                                                                                                            
we write V = span[A]                                                  independent set of                                                    , i = 1, . . . , k.                                         
                                                                                                                                                                                                        
or V = span[x1, . . . , xk].                                          vectors.                                                              Example 2.16                                                
                                                                                                                                                                                                        
Generating  sets  are  sets  of  vectors  that  span  vector          B is a basis of V .                                                   In R3                                                       
(sub)spaces, i.e.,                                                                                                                                                                                      
                                                                      B is a minimal generating set.                                        , the canonical/standard basis is canonical basis           
every  vector  can be represented as a linear combination of                                                                                                                                            
the vectors                                                           B  is  a  maximal linearly independent set of vectors in V ,          B =                                                         
                                                                      i.e., adding any                                                                                                                  
in  the  generating  set.  Now, we will be more specific and                                                                                                                                           
characterize the                                                      other vector to this set will make it linearly dependent.                                                                         
                                                                                                                                                                                                       
smallest generating set that spans a vector (sub)space.               Every  vector  x ∈ V is a linear combination of vectors from                                                                      
                                                                      B, and every                                                                                                                     
Definition  2.14 (Basis). Consider a vector space V = (V, +,                                                                                                                                            
·) and A ⊆                                                            linear combination is unique, i.e., with                                                                                         
                                                                                                                                                                                                        

%%%

                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                           ,                                                         
                                                                                                                                                                                                        
0                                                                     . (2.78)                                                                                                                         
                                                                                                                                                                                                        
                                                                     Different bases in R3 are                                                                                                        
                                                                                                                                                                                                        
 ,                                                                   B1 =                                                                  1                                                           
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        
 ,                                                                   0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                     , B2 =                                                      
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                      ,                                                                                                                              
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                     1                                                                     0.5                                                         
                                                                                                                                                                                                        

%%%

0.8                                                                                                                                                                                                   
                                                                                                                                                                                                        
0.4                                                                   . (2.79)                                                                                                                         
                                                                                                                                                                                                        
                                                                     The set                                                                                                                          
                                                                                                                                                                                                        
 ,                                                                   A =                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                          2                                                           
                                                                                                                                                                                                        
                                                                                                                                        −1                                                          
                                                                                                                                                                                                        
1.8                                                                                                                                      0                                                           
                                                                                                                                                                                                        
0.3                                                                                                                                        2                                                           
                                                                                                                                                                                                        
0.3                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
 ,                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                     2                                                                     ,                                                           
                                                                                                                                                                                                        
−2.2                                                                  3                                                                                                                                
                                                                                                                                                                                                        
−1.3                                                                  4                                                                                                                                
                                                                                                                                                                                                        
3.5                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                     ,                                                                     0                                                           
                                                                                                                                                                                                        

%%%

−4                                                                                                                                          elements. ♢                                                 
                                                                      If  U  ⊆  V  is  a subspace of V , then dim(U) ⩽ dim(V ) and                                                                      
                                                                     dim(U) =                                                              Remark. A basis of a subspace U = span[x1, . . . , xm] ⊆ Rn 
                                                                                                                                                                                                        
                                                                     ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          can be found                                                
                                                                      by Cambridge University Press (2020).                                                                                             
                                                                                                                                           by executing the following steps:                           
                                                                      46 Linear Algebra                                                                                                                 
                                                                                                                                           1. Write the spanning vectors as columns of a matrix A      
                                                                      dim(V ) if and only if U = V . Intuitively, the dimension of                                                                      
                                                                     a vector space                                                        2. Determine the row-echelon form of A.                     
                                                                                                                                                                                                        
                                                                   can be thought of as the number of independent directions in          3.  The  spanning  vectors associated with the pivot columns
                                                                      this vector                                                           are a basis of                                              
                                                                                                                                                                                                     
                                                                      The dimension of a space.                                             U.                                                          
(2.80)                                                                                                                                                                                                  
                                                                      vector space                                                          ♢                                                           
is  linearly  independent,  but not a generating set (and no                                                                                                                                            
basis) of R4                                                          corresponds to the                                                    Example 2.17 (Determining a Basis)                          
                                                                                                                                                                                                        
:                                                                     number of its basis                                                   For a vector subspace U ⊆ R5                                
                                                                                                                                                                                                        
For instance, the vector [1, 0, 0, 0]⊤ cannot be obtained by          vectors.                                                              , spanned by the vectors                                    
a linear combination of elements in A.                                                                                                                                                                  
                                                                      Remark.  The  dimension of a vector space is not necessarily          x1 =                                                        
Remark.  Every  vector  space  V  possesses  a  basis B. The          the number                                                                                                                        
preceding  examples  show  that there can be many bases of a                                                                                                                                           
vector space V , i.e., there is                                       of  elements in a vector. For instance, the vector space V =                                                                      
                                                                      span[                                                                                                                           
no  unique basis. However, all bases possess the same number                                                                                                                                            
of elements,                                                          0                                                                                                                                
                                                                                                                                                                                                        
the basis vectors. ♢ basis vector                                     1                                                                                                                                
                                                                                                                                                                                                        
We  only  consider  finite-dimensional  vector spaces V . In                                                                                                                                          
this case, the                                                                                                                                                                                          
                                                                      ] is                                                                                                                             
dimension  of V is the number of basis vectors of V , and we                                                                                                                                            
write dim(V ). dimension                                              one-dimensional,  although  the  basis  vector possesses two          1                                                           

%%%

                                                                                                                                                                                                        
2                                                                     2                                                                                                                                
                                                                                                                                                                                                        
−1                                                                    −2                                                                                                                               
                                                                                                                                                                                                        
−1                                                                                                                                                                                                    
                                                                                                                                                                                                        
−1                                                                                                                                                                                                    
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          , x4 =                                                      
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     , x3 =                                                                                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
, x2 =                                                                                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          −1                                                          
                                                                                                                                                                                                        
                                                                                                                                          8                                                           
                                                                                                                                                                                                        
                                                                     3                                                                     −5                                                          
                                                                                                                                                                                                        
                                                                     −4                                                                    −6                                                          
                                                                                                                                                                                                        
2                                                                     3                                                                     1                                                           
                                                                                                                                                                                                        
−1                                                                    5                                                                                                                                
                                                                                                                                                                                                        
1                                                                     −3                                                                                                                               

%%%

                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
∈ R                                                                   1 2 3 −1                                                                                                                          
                                                                                                                                            1 2 3 −1                                                    
5                                                                     2 −1 −4 8                                                                                                                         
                                                                                                                                            2 −1 −4 8                                                   
, (2.81)                                                              −1 1 3 −5                                                                                                                         
                                                                                                                                            −1 1 3 −5                                                   
we  are  interested in finding out which vectors x1, . . . ,          −1 2 5 −6                                                                                                                         
x4 are a basis for U.                                                                                                                       −1 2 5 −6                                                   
                                                                      −1 −2 −3 1                                                                                                                        
For  this,  we  need  to  check  whether  x1, . . . , x4 are                                                                                −1 −2 −3 1                                                  
linearly independent.                                                                                                                                                                                  
                                                                                                                                                                                                       
Therefore, we need to solve                                                                                                                                                                            
                                                                                                                                                                                                       
X                                                                                                                                                                                                      
                                                                                                                                                                                                       
4                                                                                                                                                                                                      
                                                                                                                                                                                                       
i=1                                                                                                                                                                                                    
                                                                                                                                                                                                       
λixi = 0 , (2.82)                                                                                                                                                                                      
                                                                                                                                                                                                       
which leads to a homogeneous system of equations with matrix          . (2.83)                                                                                                                          
                                                                                                                                            ⇝ · · · ⇝                                                   
x1, x2, x3, x4                                                        With  the  basic  transformation rules for systems of linear                                                                      
                                                                      equations, we                                                                                                                    
=                                                                                                                                                                                                       
                                                                      obtain the row-echelon form                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       

%%%

                                                                     can  only  be solved with λ1 = λ2 = λ4 = 0). Therefore, {x1,          elimination to                                              
                                                                      x2, x4} is a                                                                                                                      
                                                                                                                                           A                                                           
                                                                      basis of U.                                                                                                                       
                                                                                                                                           ⊤                                                           
                                                                      2.6.2 Rank                                                                                                                        
1 2 3 −1                                                                                                                                    .                                                           
                                                                      The  number  of linearly independent columns of a matrix A ∈                                                                      
0 1 2 −2                                                              Rm×n                                                                  For all A ∈ Rn×n                                            
                                                                                                                                                                                                        
0 0 0 1                                                               equals the number of linearly independent rows and is called          it holds that A is regular (invertible) if and only if      
                                                                      the rank rank                                                                                                                     
0 0 0 0                                                                                                                                     rk(A) = n.                                                  
                                                                      of A and is denoted by rk(A).                                                                                                     
0 0 0 0                                                                                                                                     For  all  A  ∈  Rm×n and all b ∈ Rm it holds that the linear
                                                                      Remark. The rank of a matrix has some important properties:           equation                                                    
                                                                                                                                                                                                       
                                                                      rk(A) = rk(A                                                          system  Ax = b can be solved if and only if rk(A) = rk(A|b),
                                                                                                                                           where                                                       
                                                                      ⊤                                                                                                                                 
                                                                                                                                           A|b denotes the augmented system.                           
                                                                      ), i.e., the column rank equals the row rank.                                                                                     
                                                                                                                                           For A ∈ Rm×n                                                
                                                                      The columns of A ∈ Rm×n                                                                                                           
                                                                                                                                           the  subspace  of solutions for Ax = 0 possesses dimension n
                                                                      span a subspace U ⊆ Rm with dim(U) =                                  − rk(A). Later, we will call this subspace the kernel or the
                                                                                                                                           null kernel                                                 
                                                                      rk(A).  Later we will call this subspace the image or range.                                                                      
.                                                                     A basis of                                                            null space space.                                           
                                                                                                                                                                                                        
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          U  can  be  found  by  applying Gaussian elimination to A to          A  matrix  A  ∈  Rm×n  has  full rank if its rank equals the
Feedback: https://mml-book.com.                                       identify the                                                          largest possible full rank                                  
                                                                                                                                                                                                        
2.6 Basis and Rank 47                                                 pivot columns.                                                        rank  for  a  matrix of the same dimensions. This means that
                                                                                                                                            the rank of                                                 
Since  the  pivot  columns  indicate which set of vectors is          The rows of A ∈ Rm×n                                                                                                              
linearly  independent, we see from the row-echelon form that                                                                                a  full-rank  matrix is the lesser of the number of rows and
x1,  x2,  x4 are linearly independent (because the system of          span a subspace W ⊆ Rn with dim(W) =                                  columns, i.e.,                                              
linear equations λ1x1 + λ2x2 + λ4x4 = 0                                                                                                                                                                 
                                                                      rk(A).  A  basis  of  W  can  be  found by applying Gaussian          rk(A)  = min(m, n). A matrix is said to be rank deficient if

%%%

it does not rank deficient                                            −2 −3 1                                                                                                                           
                                                                                                                                            is 2, such that rk(A) = 2.                                  
have full rank.                                                       3 5 0                                                                                                                             
                                                                                                                                            2.7 Linear Mappings                                         
♢                                                                                                                                                                                                      
                                                                                                                                            In  the  following,  we will study mappings on vector spaces
Example 2.18 (Rank)                                                    .                                                                   that preserve                                               
                                                                                                                                                                                                        
A =                                                                   We use Gaussian elimination to determine the rank:                    their  structure,  which will allow us to define the concept
                                                                                                                                            of a coordinate.                                            
                                                                                                                                                                                                      
                                                                                                                                            In  the  beginning  of the chapter, we said that vectors are
                                                                                                                                          objects that can be                                         
                                                                                                                                                                                                        
1 0 1                                                                 1 2 1                                                                 added together and multiplied by a scalar, and the resulting
                                                                                                                                            object is still                                             
0 1 1                                                                 −2 −3 1                                                                                                                           
                                                                                                                                            a  vector.  We  wish to preserve this property when applying
0 0 0                                                                 3 5 0                                                                 the mapping:                                                
                                                                                                                                                                                                        
                                                                                                                                          Consider  two  real  vector spaces V, W. A mapping Φ : V → W
                                                                                                                                            preserves                                                   
.                                                                     ⇝ · · · ⇝                                                                                                                       
                                                                                                                                            the structure of the vector space if                        
A  has two linearly independent rows/columns so that rk(A) =                                                                                                                                           
2.                                                                                                                                          Φ(x + y) = Φ(x) + Φ(y) (2.85)                               
                                                                                                                                                                                                       
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                Φ(λx) = λΦ(x) (2.86)                                        
by Cambridge University Press (2020).                                 1 2 1                                                                                                                             
                                                                                                                                            for  all  x,  y  ∈ V and λ ∈ R. We can summarize this in the
48 Linear Algebra                                                     0 1 3                                                                 following                                                   
                                                                                                                                                                                                        
A =                                                                   0 0 0                                                                 definition:                                                 
                                                                                                                                                                                                        
                                                                                                                                          Definition  2.15 (Linear Mapping). For vector spaces V, W, a
                                                                                                                                            mapping                                                     
                                                                      . (2.84)                                                                                                                        
                                                                                                                                            linear  mapping  Φ  :  V  → W is called a linear mapping (or
1 2 1                                                                 Here,  we  see  that the number of linearly independent rows          vector space homomorphism/                                  
                                                                      and columns                                                                                                                       

%%%

vector space                                                          Bijective if it is injective and surjective.                          automorphism                                                
                                                                                                                                                                                                        
homomorphism                                                          Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          automorphism in V .                                         
                                                                      Feedback: https://mml-book.com.                                                                                                   
linear transformation) if                                                                                                                   Example 2.19 (Homomorphism)                                 
                                                                      2.7 Linear Mappings 49                                                                                                            
linear                                                                                                                                      The mapping Φ : R2 → C, Φ(x) = x1 + ix2, is a homomorphism: 
                                                                      If  Φ  is  surjective,  then  every  element  in  W  can  be                                                                      
transformation                                                        “reached” from V                                                      Φ                                                           
                                                                                                                                                                                                        
∀x, y ∈ V ∀λ, ψ ∈ R : Φ(λx + ψy) = λΦ(x) + ψΦ(y). (2.87)              using Φ. A bijective Φ can be “undone”, i.e., there exists a          x1                                                        
                                                                      mapping Ψ :                                                                                                                       
It  turns  out  that  we  can  represent  linear mappings as                                                                                x2                                                          
matrices  (Section 2.7.1). Recall that we can also collect a          W  →  V  so that Ψ ◦ Φ(x) = x. This mapping Ψ is then called                                                                      
set of vectors as columns of a                                        the inverse                                                                                                                      
                                                                                                                                                                                                        
matrix.  When working with matrices, we have to keep in mind          of Φ and normally denoted by Φ                                        +                                                           
what the                                                                                                                                                                                                
                                                                      −1                                                                                                                               
matrix  represents:  a  linear  mapping  or  a collection of                                                                                                                                            
vectors. We will see                                                  .                                                                     y1                                                          
                                                                                                                                                                                                        
more about linear mappings in Chapter 4. Before we continue,          With  these  definitions, we introduce the following special          y2                                                          
we will                                                               cases of linear                                                                                                                   
                                                                                                                                             = (x1 + y1) + i(x2 + y2) = x1 + ix2 + y1 + iy2           
briefly introduce special mappings.                                   mappings between vector spaces V and W:                                                                                           
                                                                                                                                            = Φ x1                                                    
Definition 2.16 (Injective, Surjective, Bijective). Consider          isomorphism                                                                                                                       
a mapping Φ :                                                                                                                               x2                                                          
                                                                      Isomorphism: Φ : V → W linear and bijective endomorphism                                                                          
V → W, where V, W can be arbitrary sets. Then Φ is called                                                                                    + Φ y1                                                 
                                                                      Endomorphism: Φ : V → V linear automorphism                                                                                       
injective                                                                                                                                   y2                                                          
                                                                      Automorphism: Φ : V → V linear and bijective                                                                                      
Injective if ∀x, y ∈ V : Φ(x) = Φ(y) =⇒ x = y.                                                                                                                                                        
                                                                      We  define  idV  : V → V , x 7→ x as the identity mapping or                                                                      
surjective                                                            identity identity mapping                                             Φ                                                           
                                                                                                                                                                                                        
Surjective if Φ(V) = W. bijective                                     identity                                                                                                                         
                                                                                                                                                                                                        

%%%

λ                                                                     that  vector  spaces  of  the same dimension are kind of the          two sets of basis                                           
                                                                      same thing, as                                                                                                                    
                                                                                                                                           vectors. A vector x                                         
                                                                      they  can  be  transformed into each other without incurring                                                                      
x1                                                                    any loss.                                                             has different                                               
                                                                                                                                                                                                        
x2                                                                    Theorem 2.17 also gives us the justification to treat Rm×n            coordinate                                                  
                                                                                                                                                                                                        
 = λx1 + λix2 = λ(x1 + ix2) = λΦ                                    (the vector                                                           representations                                             
                                                                                                                                                                                                        
x1                                                                  space  of  m  ×  n-matrices)  and  Rmn  (the vector space of          depending on which                                          
                                                                      vectors of length                                                                                                                 
x2                                                                                                                                          coordinate system is                                        
                                                                      mn) the same, as their dimensions are mn, and there exists a                                                                      
 .                                                                  linear,  bijective  mapping  that  transforms  one  into the          chosen.                                                     
                                                                      other.                                                                                                                            
(2.88)                                                                                                                                      x x                                                         
                                                                      Remark. Consider vector spaces V, W, X. Then:                                                                                     
This  also  justifies why complex numbers can be represented                                                                                e1                                                          
as tuples in                                                          For linear mappings Φ : V → W and Ψ : W → X, the mapping                                                                          
                                                                                                                                            e2                                                          
R2                                                                    Ψ ◦ Φ : V → X is also linear.                                                                                                     
                                                                                                                                            b1                                                          
:  There  is  a  bijective  linear mapping that converts the          If Φ : V → W is an isomorphism, then Φ                                                                                            
elementwise addition of tuples in R2                                                                                                        b2                                                          
                                                                      −1                                                                                                                                
into  the  set  of  complex  numbers  with the corresponding                                                                                If  Φ  : V → W, Ψ : V → W are linear, then Φ + Ψ and λΦ, λ ∈
addition.  Note  that  we only showed linearity, but not the          : W → V is an isomorphism, too.                                       R, are                                                      
bijection.                                                                                                                                                                                              
                                                                      ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          linear, too.                                                
Theorem    2.17    (Theorem    3.59    in   Axler   (2015)).          by Cambridge University Press (2020).                                                                                             
Finite-dimensional vector                                                                                                                   ♢                                                           
                                                                      50 Linear Algebra                                                                                                                 
spaces  V  and  W  are  isomorphic  if and only if dim(V ) =                                                                                2.7.1 Matrix Representation of Linear Mappings              
dim(W).                                                               Figure 2.1 Two                                                                                                                    
                                                                                                                                            Any n-dimensional vector space is isomorphic to Rn          
Theorem  2.17  states  that there exists a linear, bijective          different coordinate                                                                                                              
mapping  between  two  vector  spaces of the same dimension.                                                                                (Theorem 2.17). We                                          
Intuitively, this means                                               systems defined by                                                                                                                
                                                                                                                                            consider a basis {b1, . . . , bn} of an n-dimensional vector

%%%

space V . In the                                                                                                                            Cartesian  coordinate  system  in  two  dimensions, which is
                                                                                                                                           spanned by the                                              
following, the order of the basis vectors will be important.                                                                                                                                            
Therefore, we                                                         α1                                                                    canonical basis vectors e1, e2. In this coordinate system, a
                                                                                                                                            vector x ∈ R2                                               
write                                                                 .                                                                                                                                 
                                                                                                                                            has  a  representation that tells us how to linearly combine
B = (b1, . . . , bn) (2.89)                                           .                                                                     e1 and e2 to                                                
                                                                                                                                                                                                        
ordered basis and call this n-tuple an ordered basis of V .           .                                                                     obtain   x.  However,  any  basis  of  R2  defines  a  valid
                                                                                                                                            coordinate system,                                          
Remark (Notation). We are at the point where notation gets a          αn                                                                                                                                
bit tricky.                                                                                                                                 and  the  same  vector  x  from  before may have a different
                                                                                                                                           coordinate  representation  in the (b1, b2) basis. In Figure
Therefore,  we  summarize  some parts here. B = (b1, . . . ,                                                                                2.1, the coordinates of x with                              
bn) is an ordered                                                                                                                                                                                      
                                                                                                                                            respect  to the standard basis (e1, e2) is [2, 2]⊤. However,
basis, B = {b1, . . . , bn} is an (unordered) basis, and B =           ∈ R                                                                 with respect to                                             
[b1, . . . , bn] is a                                                                                                                                                                                   
                                                                      n                                                                     the  basis  (b1,  b2)  the  same  vector x is represented as
matrix whose columns are the vectors b1, . . . , bn. ♢                                                                                      [1.09, 0.72]⊤, i.e.,                                        
                                                                      (2.91)                                                                                                                            
Definition 2.18 (Coordinates). Consider a vector space V and                                                                                x  =  1.09b1  +  0.72b2.  In the following sections, we will
an ordered                                                            coordinate   vector   is  the  coordinate  vector/coordinate          discover how to                                             
                                                                      representation of x with respect to the                                                                                           
basis  B = (b1, . . . , bn) of V . For any x ∈ V we obtain a                                                                                obtain this representation.                                 
unique representation (linear combination)                            coordinate                                                                                                                        
                                                                                                                                            Example 2.20                                                
x = α1b1 + . . . + αnbn (2.90)                                        representation                                                                                                                    
                                                                                                                                            Let  us  have  a  look  at  a  geometric  vector x ∈ R2 with
coordinate  of  x with respect to B. Then α1, . . . , αn are          ordered basis B.                                                      coordinates [2, 3]⊤ Figure 2.2                              
the coordinates of x with                                                                                                                                                                               
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          Different coordinate                                        
respect to B, and the vector                                          Feedback: https://mml-book.com.                                                                                                   
                                                                                                                                            representations of a                                        
α =                                                                   2.7 Linear Mappings 51                                                                                                            
                                                                                                                                            vector x, depending                                         
                                                                     A  basis  effectively  defines  a  coordinate system. We are                                                                      
                                                                      familiar with the                                                     on the choice of                                            
                                                                                                                                                                                                       

%%%

basis.                                                                                                                                                                                                  
                                                                      of V , the mapping Φ : Rn → V , Φ(ei) = bi                            AΦ(i, j) = αij , (2.93)                                     
e1                                                                                                                                                                                                      
                                                                      , i = 1, . . . , n, is linear                                         the  transformation matrix of Φ (with respect to the ordered
e2 b2                                                                                                                                       bases B of V transformation                                 
                                                                      (and because of Theorem 2.17 an isomorphism), where (e1, . .                                                                      
b1                                                                    . , en) is                                                            and C of W matrix ).                                        
                                                                                                                                                                                                        
x = −                                                                 the standard basis of Rn                                              The  coordinates of Φ(bj ) with respect to the ordered basis
                                                                                                                                            C of W                                                      
1                                                                     .                                                                                                                                 
                                                                                                                                            are  the  j-th  column  of AΦ. Consider (finite-dimensional)
2b1 +                                                                 ♢                                                                     vector spaces                                               
                                                                                                                                                                                                        
5                                                                     Now  we  are  ready  to  make an explicit connection between          V,  W with ordered bases B, C and a linear mapping Φ : V → W
                                                                      matrices and                                                          with                                                        
2b2                                                                                                                                                                                                     
                                                                      linear mappings between finite-dimensional vector spaces.             ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
x = 2e1 + 3e2                                                                                                                               by Cambridge University Press (2020).                       
                                                                      Definition  2.19  (Transformation  Matrix).  Consider vector                                                                      
with respect to the standard basis (e1, e2) of R2                     spaces V, W                                                           52 Linear Algebra                                           
                                                                                                                                                                                                        
. This means, we can write                                            with  corresponding (ordered) bases B = (b1, . . . , bn) and          transformation  matrix AΦ. If xˆ is the coordinate vector of
                                                                      C = (c1, . . . , cm).                                                 x ∈ V with                                                  
x  =  2e1  +  3e2.  However,  we  do  not have to choose the                                                                                                                                            
standard basis to                                                     Moreover,  we  consider  a linear mapping Φ : V → W. For j ∈          respect  to  B  and yˆ the coordinate vector of y = Φ(x) ∈ W
                                                                      {1, . . . , n},                                                       with respect                                                
represent  this vector. If we use the basis vectors b1 = [1,                                                                                                                                            
−1]⊤, b2 = [1, 1]⊤                                                    Φ(bj ) = α1jc1 + · · · + αmjcm =                                      to C, then                                                  
                                                                                                                                                                                                        
we will obtain the coordinates 1                                      Xm                                                                    yˆ = AΦxˆ . (2.94)                                          
                                                                                                                                                                                                        
2                                                                     i=1                                                                   This means that the transformation matrix can be used to map
                                                                                                                                            coordinates                                                 
[−1, 5]⊤ to represent the same vector with                            αijci (2.92)                                                                                                                      
                                                                                                                                            with  respect  to  an ordered basis in V to coordinates with
respect to (b1, b2) (see Figure 2.2).                                 is  the  unique  representation of Φ(bj ) with respect to C.          respect to an                                               
                                                                      Then, we call the                                                                                                                 
Remark.  For  an n-dimensional vector space V and an ordered                                                                                ordered basis in W.                                         
basis B                                                               m × n-matrix AΦ, whose elements are given by                                                                                      

%%%

Example 2.21 (Transformation Matrix)                                  −1 2 4                                                                                                                            
                                                                                                                                            and stretching.                                             
Consider a homomorphism Φ : V → W and ordered bases B =                                                                                                                                                
                                                                                                                                            (a)  Original  data.  (b) Rotation by 45◦. (c) Stretch along
(b1, . . . , b3) of V and C = (c1, . . . , c4) of W. With                                                                                  the                                                         
                                                                                                                                                                                                        
Φ(b1) = c1 − c2 + 3c3 − c4                                                                                                                 horizontal axis.                                            
                                                                                                                                                                                                        
Φ(b2) = 2c1 + c2 + 7c3 + 2c4                                                                                                               (d) General linear                                          
                                                                                                                                                                                                        
Φ(b3) = 3c2 + c3 + 4c4                                                , (2.96)                                                              mapping.                                                    
                                                                                                                                                                                                        
(2.95)                                                                where  the  αj  , j = 1, 2, 3, are the coordinate vectors of          We consider three linear transformations of a set of vectors
                                                                      Φ(bj ) with respect                                                   in R2 with                                                  
the transformation matrix                                                                                                                                                                               
                                                                      to C.                                                                 the transformation matrices                                 
P                                                                                                                                                                                                       
                                                                      Example 2.22 (Linear Transformations of Vectors)                      A1 =                                                        
AΦ with respect to B and C satisfies Φ(bk) =                                                                                                                                                            
                                                                      Figure 2.3 Three                                                                                                                 
4                                                                                                                                                                                                       
                                                                      examples of linear                                                    cos( π                                                      
i=1 αikci for k = 1, . . . , 3 and is given as                                                                                                                                                          
                                                                      transformations of                                                    4                                                           
AΦ = [α1, α2, α3] =                                                                                                                                                                                     
                                                                      the vectors shown                                                     ) − sin( π                                                  
                                                                                                                                                                                                       
                                                                      as dots in (a);                                                       4                                                           
                                                                                                                                                                                                       
                                                                      (b) Rotation by 45◦;                                                  )                                                           
                                                                                                                                                                                                       
                                                                      (c) Stretching of the                                                 sin( π                                                      
                                                                                                                                                                                                       
                                                                      horizontal                                                            4                                                           
1 2 0                                                                                                                                                                                                   
                                                                      coordinates by 2;                                                     ) cos( π                                                    
−1 1 3                                                                                                                                                                                                  
                                                                      (d) Combination of                                                    4                                                           
3 7 1                                                                                                                                                                                                   
                                                                      reflection, rotation                                                  )                                                           

%%%

                                                                                                                                            Φ ∈ Rm×n                                                    
                                                                     each  of  these  vectors,  we  obtain  the rotated square in                                                                      
                                                                      Figure 2.3(b). If we                                                  is  the corresponding transformation mapping with respect to
, A2 =                                                                                                                                      B˜ and C˜.                                                  
                                                                      apply  the  linear  mapping represented by A2, we obtain the                                                                      
                                                                     rectangle in                                                          In  the  following,  we  will  investigate  how A and A˜ are
                                                                                                                                            related, i.e., how/                                         
2 0                                                                   Figure  2.3(c)  where  each x1-coordinate is stretched by 2.                                                                      
                                                                      Figure 2.3(d)                                                         whether we can transform AΦ into A˜                         
0 1                                                                                                                                                                                                    
                                                                      shows  the  original square from Figure 2.3(a) when linearly          Φ if we choose to perform a basis                           
, A3 =                                                                transformed                                                                                                                       
                                                                                                                                            change from B, C to B, ˜ C˜.                                
1                                                                     using  A3,  which  is  a  combination  of  a  reflection,  a                                                                      
                                                                      rotation, and a stretch.                                              Remark.    We    effectively    get   different   coordinate
2                                                                                                                                           representations of the                                      
                                                                      2.7.2 Basis Change                                                                                                                
                                                                                                                                           identity  mapping  idV  . In the context of Figure 2.2, this
                                                                      In  the  following,  we  will  have  a  closer  look  at how          would mean to                                               
3 −1                                                                  transformation matrices                                                                                                           
                                                                                                                                            map  coordinates  with  respect to (e1, e2) onto coordinates
1 −1                                                                  of  a linear mapping Φ : V → W change if we change the bases          with respect to                                             
                                                                      in V and                                                                                                                          
                                                                                                                                           (b1,  b2)  without  changing  the  vector x. By changing the
                                                                      W. Consider two ordered bases                                         basis and correspondingly the representation of vectors, the
. (2.97)                                                                                                                                    transformation matrix with                                  
                                                                      B = (b1, . . . , bn), B˜ = (˜b1, . . . ,                                                                                          
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                respect  to  this  new  basis can have a particularly simple
Feedback: https://mml-book.com.                                       ˜bn) (2.98)                                                           form that allows                                            
                                                                                                                                                                                                        
2.7 Linear Mappings 53                                                of V and two ordered bases                                            for straightforward computation. ♢                          
                                                                                                                                                                                                        
Figure 2.3 gives three examples of linear transformations of          C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.99)                  Example 2.23 (Basis Change)                                 
a set of vectors. Figure 2.3(a) shows 400 vectors in R2                                                                                                                                                 
                                                                      of W. Moreover, AΦ ∈ Rm×n                                             Consider a transformation matrix                            
, each of which is represented                                                                                                                                                                          
                                                                      is the transformation matrix of the linear                            A =                                                         
by  a  dot  at  the  corresponding (x1, x2)-coordinates. The                                                                                                                                            
vectors  are  arranged in a square. When we use matrix A1 in          mapping Φ : V → W with respect to the bases B and C, and A˜                                                                      
(2.97) to linearly transform                                                                                                                                                                            

%%%

2 1                                                                                                                                         is given as                                                 
                                                                      3 0                                                                                                                               
1 2                                                                                                                                        A˜                                                          
                                                                      0 1                                                                                                                              
(2.100)                                                                                                                                     Φ = T                                                       
                                                                      (2.102)                                                                                                                           
with respect to the canonical basis in R2                                                                                                   −1AΦS . (2.105)                                             
                                                                      with respect to B, which is easier to work with than A.                                                                           
. If we define a new basis                                                                                                                  Here, S ∈ Rn×n                                              
                                                                      In  the  following,  we will look at mappings that transform                                                                      
B = (                                                                coordinate                                                            is the transformation matrix of idV that maps coordinates   
                                                                                                                                                                                                        
1                                                                     vectors  with  respect  to one basis into coordinate vectors          with respect to B˜ onto coordinates with respect to B, and T
                                                                      with respect to                                                       ∈ Rm×m is the                                               
1                                                                                                                                                                                                       
                                                                      a  different  basis. We will state our main result first and          transformation  matrix  of  idW  that  maps coordinates with
                                                                     then provide an                                                       respect to C˜ onto                                          
                                                                                                                                                                                                        
,                                                                     explanation.                                                          coordinates with respect to C.                              
                                                                                                                                                                                                        
                                                                     Theorem 2.20 (Basis Change). For a linear mapping Φ : V → W,          Proof  Following  Drumm  and  Weil  (2001), we can write the
                                                                      ordered                                                               vectors of                                                  
1                                                                                                                                                                                                       
                                                                      bases                                                                 the  new  basis B˜ of V as a linear combination of the basis
−1                                                                                                                                          vectors of B,                                               
                                                                      B = (b1, . . . , bn), B˜ = (˜b1, . . . ,                                                                                          
                                                                                                                                           such that                                                   
                                                                      ˜bn) (2.103)                                                                                                                      
) (2.101)                                                                                                                                   ˜bj = s1jb1 + · · · + snjbn =                               
                                                                      of V and                                                                                                                          
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                Xn                                                          
by Cambridge University Press (2020).                                 C = (c1, . . . , cm), C˜ = (c˜1, . . . , c˜m) (2.104)                                                                             
                                                                                                                                            i=1                                                         
54 Linear Algebra                                                     of  W, and a transformation matrix AΦ of Φ with respect to B                                                                      
                                                                      and C, the                                                            sijbi                                                       
we obtain a diagonal transformation matrix                                                                                                                                                              
                                                                      corresponding transformation matrix A˜                                , j = 1, . . . , n . (2.106)                                
A˜ =                                                                                                                                                                                                    
                                                                      Φ with respect to the bases B˜ and C˜                                 Similarly,  we  write  the  new  basis  vectors C˜ of W as a
                                                                                                                                           linear combination                                          

%%%

                                                                                                                                                                                                        
of the basis vectors of C, which yields                               mapping Φ, we get that for all j = 1, . . . , n                       summation.                                                  
                                                                                                                                                                                                        
c˜k = t1kc1 + · · · + tmkcm =                                         Φ(˜bj ) = Xm                                                          Alternatively,  when  we  express  the  ˜bj  ∈  V  as linear
                                                                                                                                            combinations of                                             
Xm                                                                    k=1                                                                                                                               
                                                                                                                                            bj ∈ V , we arrive at                                       
l=1                                                                   a˜kjc˜k                                                                                                                           
                                                                                                                                            Φ(˜bj )                                                     
tlkcl                                                                 | {z }                                                                                                                            
                                                                                                                                            (2.106) = Φ Xn                                              
, k = 1, . . . , m . (2.107)                                          ∈W                                                                                                                                
                                                                                                                                            i=1                                                         
We  define  S = ((sij )) ∈ Rn×n as the transformation matrix          (2.107) =                                                                                                                         
that maps                                                                                                                                   sijbi                                                       
                                                                      Xm                                                                                                                                
coordinates with respect to B˜ onto coordinates with respect                                                                                !                                                           
to B and                                                              k=1                                                                                                                               
                                                                                                                                            =                                                           
T  =  ((tlk))  ∈ Rm×m as the transformation matrix that maps          a˜kjXm                                                                                                                            
coordinates                                                                                                                                 Xn                                                          
                                                                      l=1                                                                                                                               
with  respect  to  C˜ onto coordinates with respect to C. In                                                                                i=1                                                         
particular, the jth                                                   tlkcl =                                                                                                                           
                                                                                                                                            sijΦ(bi) =                                                  
column  of  S  is  the coordinate representation of ˜bj with          Xm                                                                                                                                
respect to B and                                                                                                                            Xn                                                          
                                                                      l=1 Xm                                                                                                                            
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                i=1                                                         
Feedback: https://mml-book.com.                                       k=1                                                                                                                               
                                                                                                                                            sijXm                                                       
2.7 Linear Mappings 55                                                tlka˜kj!                                                                                                                          
                                                                                                                                            l=1                                                         
the  kth column of T is the coordinate representation of c˜k          cl                                                                                                                                
with respect to                                                                                                                             alicl (2.109a)                                              
                                                                      , (2.108)                                                                                                                         
C. Note that both S and T are regular.                                                                                                      =                                                           
                                                                      where  we  first  expressed the new basis vectors c˜k ∈ W as                                                                      
We  are  going  to  look  at  Φ(˜bj ) from two perspectives.          linear  combinations  of  the  basis vectors cl ∈ W and then          Xm                                                          
First, applying the                                                   swapped the order of                                                                                                              

%%%

l=1 Xn                                                                A˜                                                                                                                                
                                                                                                                                            ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
i=1                                                                   Φ = T                                                                 by Cambridge University Press (2020).                       
                                                                                                                                                                                                        
alisij!                                                               −1AΦS , (2.112)                                                       56 Linear Algebra                                           
                                                                                                                                                                                                        
cl                                                                    which proves Theorem 2.20.                                            Figure 2.2 For a                                            
                                                                                                                                                                                                        
, j = 1, . . . , n , (2.109b)                                         Theorem  2.20  tells  us that with a basis change in V (B is          homomorphism                                                
                                                                      replaced with                                                                                                                     
where we exploited the linearity of Φ. Comparing (2.108) and                                                                                Φ : V → W and                                               
(2.109b),                                                             B˜) and W (C is replaced with C˜), the transformation matrix                                                                      
                                                                      AΦ of a                                                               ordered bases B, B˜                                         
it  follows  for  all  j = 1, . . . , n and l = 1, . . . , m                                                                                                                                            
that                                                                  linear mapping Φ : V → W is replaced by an equivalent matrix          of V and C, C˜ of W                                         
                                                                      A˜                                                                                                                                
Xm                                                                                                                                          (marked in blue),                                           
                                                                      Φ with                                                                                                                            
k=1                                                                                                                                         we can express the                                          
                                                                      A˜                                                                                                                                
tlka˜kj =                                                                                                                                   mapping ΦC˜B˜ with                                          
                                                                      Φ = T                                                                                                                             
Xn                                                                                                                                          respect to the bases                                        
                                                                      −1AΦS. (2.113)                                                                                                                    
i=1                                                                                                                                         B, ˜ C˜ equivalently as                                     
                                                                      Figure   2.2   illustrates   this   relation:   Consider   a                                                                      
alisij (2.110)                                                        homomorphism Φ : V → W                                                a composition of the                                        
                                                                                                                                                                                                        
and, therefore,                                                       and ordered bases B, B˜ of V and C, C˜ of W. The mapping ΦCB          homomorphisms                                               
                                                                      is  an  instantiation  of Φ and maps basis vectors of B onto                                                                      
T A˜                                                                  linear combinations of                                                ΦC˜B˜ =                                                     
                                                                                                                                                                                                        
Φ = AΦS ∈ R                                                           basis  vectors  of C. Assume that we know the transformation          ΞCC˜ ◦ ΦCB ◦ ΨBB˜                                           
                                                                      matrix AΦ                                                                                                                         
m×n                                                                                                                                         with respect to the                                         
                                                                      of  ΦCB  with  respect  to  the  ordered bases B, C. When we                                                                      
, (2.111)                                                             perform a basis                                                       bases in the                                                
                                                                                                                                                                                                        
such that                                                             change  from  B  to  B˜  in  V and from C to C˜ in W, we can          subscripts. The                                             
                                                                      determine the                                                                                                                     

%%%

corresponding                                                         ΨBB˜ ΞCC˜ = Ξ−1 S T CC˜                                                                                                           
                                                                                                                                            that  map  vectors  onto  themselves,  but with respect to a
transformation                                                        −1                                                                    different basis.                                            
                                                                                                                                                                                                        
matrices are in red.                                                  A˜ Φ                                                                  Definition 2.21 (Equivalence). Two matrices A, A˜ ∈ Rm×n    
                                                                                                                                                                                                        
V W                                                                   AΦ                                                                    equivalent are equivalent                                   
                                                                                                                                                                                                        
B                                                                     Vector spaces                                                         if  there exist regular matrices S ∈ Rn×n and T ∈ Rm×m, such
                                                                                                                                            that                                                        
B˜ C˜                                                                 Ordered bases                                                                                                                     
                                                                                                                                            A˜ = T                                                      
C                                                                     corresponding transformation matrix A˜                                                                                            
                                                                                                                                            −1AS.                                                       
Φ                                                                     Φ  as  follows:  First, we find the matrix representation of                                                                      
                                                                      the  linear  mapping ΨBB˜ : V → V that maps coordinates with          Definition 2.22 (Similarity). Two matrices A, A˜ ∈ Rn×n     
ΦCB                                                                   respect  to  the  new basis B˜ onto the (unique) coordinates                                                                      
                                                                      with                                                                  similar are similar if                                      
ΦC˜B˜                                                                                                                                                                                                   
                                                                      respect  to  the  “old”  basis  B  (in V ). Then, we use the          there exists a regular matrix S ∈ Rn×n with A˜ = S          
ΨBB˜ S T ΞCC˜                                                         transformation  matrix  AΦ  of  ΦCB  :  V  →  W to map these                                                                      
                                                                      coordinates onto the coordinates                                      −1AS                                                        
A˜ Φ                                                                                                                                                                                                    
                                                                      with  respect  to  C  in W. Finally, we use a linear mapping          Remark.  Similar  matrices  are  always equivalent. However,
AΦ                                                                    ΞCC˜ : W → W                                                          equivalent matrices are not necessarily similar. ♢          
                                                                                                                                                                                                        
V W                                                                   to  map  the  coordinates with respect to C onto coordinates          Remark. Consider vector spaces V, W, X. From the remark that
                                                                      with respect to                                                       follows                                                     
B                                                                                                                                                                                                       
                                                                      C˜.  Therefore, we can express the linear mapping ΦC˜B˜ as a          Theorem 2.17, we already know that for linear mappings Φ : V
B˜ C˜                                                                 composition of                                                        → W                                                         
                                                                                                                                                                                                        
C                                                                     linear mappings that involve the “old” basis:                         and Ψ : W → X the mapping Ψ ◦ Φ : V → X is also linear. With
                                                                                                                                                                                                        
Φ                                                                     ΦC˜B˜ = ΞCC˜ ◦ ΦCB ◦ ΨBB˜ = Ξ−1                                       transformation  matrices  AΦ  and  AΨ  of  the corresponding
                                                                                                                                            mappings, the                                               
ΦCB                                                                   CC˜ ◦ ΦCB ◦ ΨBB˜ . (2.114)                                                                                                        
                                                                                                                                            overall transformation matrix is AΨ◦Φ = AΨAΦ. ♢             
ΦC˜B˜                                                                 Concretely,  we  use  ΨBB˜  = idV and ΞCC˜ = idW , i.e., the                                                                      
                                                                      identity mappings                                                     In  light  of this remark, we can look at basis changes from

%%%

the perspective of composing linear mappings:                         Φ : B˜ → C˜, S : B˜ → B, T : C˜ → C and                                                                                           
                                                                                                                                                                                                       
AΦ  is the transformation matrix of a linear mapping ΦCB : V          T                                                                                                                                 
→ W                                                                                                                                                                                                    
                                                                      −1                                                                                                                                
with respect to the bases B, C.                                                                                                                                                                        
                                                                      : C → C˜, and                                                                                                                     
A˜                                                                                                                                          1 2 0                                                       
                                                                      B˜ → C˜ = B˜ → B→ C → C˜ (2.115)                                                                                                  
Φ is the transformation matrix of the linear mapping ΦC˜B˜ :                                                                                −1 1 3                                                      
V → W                                                                 A˜                                                                                                                                
                                                                                                                                            3 7 1                                                       
with respect to the bases B, ˜ C˜.                                    Φ = T                                                                                                                             
                                                                                                                                            −1 2 4                                                      
S  is the transformation matrix of a linear mapping ΨBB˜ : V          −1AΦS . (2.116)                                                                                                                   
→ V                                                                                                                                                                                                    
                                                                      Note  that  the  execution order in (2.116) is from right to                                                                      
(automorphism) that represents B˜ in terms of B. Normally, Ψ          left  because  vectors are multiplied at the right-hand side                                                                     
= idV is                                                              so that x 7→ Sx 7→ AΦ(Sx) 7→                                                                                                      
                                                                                                                                                                                                       
the identity mapping in V .                                           T                                                                                                                                 
                                                                                                                                                                                                       
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          −1                                                                                                                                
Feedback: https://mml-book.com.                                                                                                             (2.117)                                                     
                                                                      AΦ(Sx)                                                                                                                            
2.7 Linear Mappings 57                                                                                                                      with respect to the standard bases                          
                                                                                                                                                                                                       
T  is the transformation matrix of a linear mapping ΞCC˜ : W                                                                                B = (                                                       
→ W                                                                   = A˜                                                                                                                              
                                                                                                                                                                                                       
(automorphism) that represents C˜ in terms of C. Normally, Ξ          Φx.                                                                                                                               
= idW is                                                                                                                                                                                               
                                                                      Example 2.24 (Basis Change)                                                                                                       
the identity mapping in W.                                                                                                                  1                                                           
                                                                      Consider  a  linear mapping Φ : R3 → R4 whose transformation                                                                      
If  we  (informally)  write down the transformations just in          matrix is                                                             0                                                           
terms of bases,                                                                                                                                                                                         
                                                                      AΦ =                                                                  0                                                           
then AΦ : B → C, A˜                                                                                                                                                                                     
                                                                                                                                                                                                      

%%%

                                                                                                                                                                                                        
 ,                                                                   0                                                                     ,                                                           
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
 ,                                                                   ,                                                                     1                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                     1                                                                     ,                                                           
                                                                                                                                                                                                        
), C = (                                                             0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1                                                                                                                                          0                                                           

%%%

                                                                                                                                                                                                        
0                                                                     1                                                                     0                                                           
                                                                                                                                                                                                        
1                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                      ,                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          ,                                                           
                                                                                                                                                                                                        
). (2.118)                                                            1                                                                                                                                
                                                                                                                                                                                                        
We seek the transformation matrix A˜                                  0                                                                                                                                
                                                                                                                                                                                                        
Φ of Φ with respect to the new bases                                  1                                                                                                                                
                                                                                                                                                                                                        
B˜ = (                                                                                                                                                                                                
                                                                                                                                                                                                        
                                                                     ) ∈ R                                                                1                                                           
                                                                                                                                                                                                        
                                                                     3                                                                     0                                                           
                                                                                                                                                                                                        
1                                                                     , C˜ = (                                                              1                                                           
                                                                                                                                                                                                        
1                                                                                                                                          0                                                           
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
 ,                                                                                                                                                                                                   
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        
                                                                     1                                                                     ,                                                           
                                                                                                                                                                                                        
0                                                                     0                                                                                                                                

%%%

                                                                                                                                                                                                        
                                                                                                                                          0 1 1 0                                                     
                                                                                                                                                                                                        
                                                                                                                                          0 0 0 1                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
0                                                                                                                                                                                                     
                                                                                                                                                                                                        
1                                                                     ). (2.119)                                                                                                                       
                                                                                                                                                                                                        
1                                                                     Then,                                                                                                                            
                                                                                                                                                                                                        
0                                                                     S =                                                                   , (2.120)                                                   
                                                                                                                                                                                                        
                                                                                                                                          where  the  ith column of S is the coordinate representation
                                                                                                                                            of ˜bi                                                      
                                                                                                                                                                                                      
                                                                                                                                            in                                                          
                                                                     1 0 1                                                                                                                             
                                                                                                                                            terms  of  the  basis  vectors of B. Since B is the standard
                                                                     1 1 0                                                                 basis,  the  coordinate representation is straightforward to
                                                                                                                                            find. For a general basis B,                                
,                                                                     0 1 1                                                                                                                             
                                                                                                                                            we  would need to solve a linear equation system to find the
                                                                                                                                          λi such that                                                
                                                                                                                                                                                                        
                                                                      , T =                                                               ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020)                        
                                                                                                                                                                                                      
                                                                                                                                            58 Linear Algebra                                           
                                                                                                                                                                                                      
                                                                                                                                            P3                                                          
1                                                                                                                                                                                                      
                                                                                                                                            i=1 λibi = ˜bj , j = 1, . . . , 3. Similarly, the jth column
0                                                                                                                                          of T is the coordinate representation of c˜j in terms of the
                                                                                                                                            basis vectors of C.                                         
0                                                                     1 1 0 1                                                                                                                           
                                                                                                                                            Therefore, we obtain                                        
1                                                                     1 0 1 0                                                                                                                           

%%%

A˜                                                                                                                                                                                                    
                                                                                                                                                                                                        
Φ = T                                                                 3 2 1                                                                                                                            
                                                                                                                                                                                                        
−1AΦS =                                                               0 4 2                                                                                                                            
                                                                                                                                                                                                        
1                                                                     10 8 4                                                                . (2.121b)                                                  
                                                                                                                                                                                                        
2                                                                     1 6 3                                                                 In  Chapter  4,  we will be able to exploit the concept of a
                                                                                                                                            basis change                                                
                                                                                                                                                                                                      
                                                                                                                                            to  find  a  basis  with respect to which the transformation
                                                                                                                                          matrix   of   an  endomorphism  has  a  particularly  simple
                                                                                                                                            (diagonal) form. In Chapter 10, we                          
                                                                                                                                                                                                      
                                                                                                                                            will   look  at  a  data  compression  problem  and  find  a
                                                                                                                                          convenient basis onto                                       
                                                                                                                                                                                                        
1 1 −1 −1                                                             (2.121a)                                                              which   we   can  project  the  data  while  minimizing  the
                                                                                                                                            compression loss.                                           
1 −1 1 −1                                                             =                                                                                                                                 
                                                                                                                                            2.7.3 Image and Kernel                                      
−1 1 1 1                                                                                                                                                                                               
                                                                                                                                            The  image  and  kernel  of  a  linear  mapping  are  vector
0 0 0 2                                                                                                                                    subspaces   with   certain   important  properties.  In  the
                                                                                                                                            following, we will characterize them                        
                                                                                                                                                                                                      
                                                                                                                                            more carefully.                                             
                                                                                                                                                                                                      
                                                                                                                                            Definition 2.23 (Image and Kernel).                         
                                                                     −4 −4 −2                                                                                                                          
                                                                                                                                            kernel For Φ : V → W, we define the kernel/null space       
                                                                     6 0 0                                                                                                                             
                                                                                                                                            null space                                                  
                                                                     4 8 4                                                                                                                             
                                                                                                                                            ker(Φ) := Φ−1                                               
                                                                     1 6 3                                                                                                                             
                                                                                                                                            (0W ) = {v ∈ V : Φ(v) = 0W } (2.122)                        
                                                                                                                                                                                                      
                                                                                                                                            image and the image/range                                   

%%%

                                                                                                                                                                                                        
range                                                                 and image of a                                                        : x1, . . . , xn ∈ R                                        
                                                                                                                                                                                                        
Im(Φ) := Φ(V ) = {w ∈ W|∃v ∈ V : Φ(v) = w} . (2.123)                  linear mapping                                                        )                                                           
                                                                                                                                                                                                        
domain  We also call V and W also the domain and codomain of          Φ : V → W.                                                            (2.124a)                                                    
Φ, respectively.                                                                                                                                                                                        
                                                                      Im(Φ)                                                                 = span[a1, . . . , an] ⊆ R                                  
codomain                                                                                                                                                                                                
                                                                      0W                                                                    m , (2.124b)                                                
Intuitively,  the  kernel is the set of vectors v ∈ V that Φ                                                                                                                                            
maps onto the                                                         ker(Φ)                                                                i.e., the image is the span of the columns of A, also called
                                                                                                                                            the column column space                                     
neutral  element 0W ∈ W. The image is the set of vectors w ∈          0V                                                                                                                                
W that                                                                                                                                      space.  Therefore, the column space (image) is a subspace of
                                                                      Φ : V → W V W                                                         Rm, where                                                   
can be “reached” by Φ from any vector in V . An illustration                                                                                                                                            
is given in                                                           Φ is injective (one-to-one) if and only if ker(Φ) = {0}.              m is the “height” of the matrix.                            
                                                                                                                                                                                                        
Figure 2.2.                                                           ♢                                                                     rk(A) = dim(Im(Φ)).                                         
                                                                                                                                                                                                        
Remark.  Consider a linear mapping Φ : V → W, where V, W are          Remark  (Null  Space  and Column Space). Let us consider A ∈          The  kernel/null space ker(Φ) is the general solution to the
vector                                                                Rm×n and                                                              homogeneous  system  of linear equations Ax = 0 and captures
                                                                                                                                            all possible                                                
spaces.                                                               a linear mapping Φ : Rn → Rm, x 7→ Ax.                                                                                            
                                                                                                                                            linear combinations of the elements in Rn                   
It  always  holds  that  Φ(0V  )  =  0W and, therefore, 0V ∈          For  A = [a1, . . . , an], where ai are the columns of A, we                                                                      
ker(Φ). In                                                            obtain                                                                that produce 0 ∈ Rm.                                        
                                                                                                                                                                                                        
particular, the null space is never empty.                            Im(Φ) = {Ax : x ∈ R                                                   The kernel is a subspace of Rn                              
                                                                                                                                                                                                        
Im(Φ)  ⊆  W is a subspace of W, and ker(Φ) ⊆ V is a subspace          n                                                                     , where n is the “width” of the matrix.                     
of V .                                                                                                                                                                                                  
                                                                      } =                                                                   The  kernel  focuses  on the relationship among the columns,
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                and we can                                                  
Feedback: https://mml-book.com.                                       (Xn                                                                                                                               
                                                                                                                                            use it to determine whether/how we can express a column as a
2.7 Linear Mappings 59                                                i=1                                                                   linear                                                      
                                                                                                                                                                                                        
Figure 2.2 Kernel                                                     xiai                                                                  combination of other columns.                               

%%%

                                                                                                                                                                                                        
♢                                                                                                                                          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
Example 2.25 (Image and Kernel of a Linear Mapping)                   1 2 −1 0                                                                                                                          
                                                                                                                                            60 Linear Algebra                                           
The mapping                                                           1 0 0 1                                                                                                                          
                                                                                                                                            = x1                                                        
Φ : R                                                                                                                                                                                                  
                                                                                                                                                                                                       
4 → R                                                                                                                                                                                                  
                                                                                                                                            1                                                           
2                                                                                                                                                                                                      
                                                                                                                                            1                                                           
,                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                     x1                                                                                                                                
                                                                                                                                            + x2                                                        
                                                                     x2                                                                                                                                
                                                                                                                                                                                                       
                                                                     x3                                                                                                                                
                                                                                                                                            2                                                           
                                                                     x4                                                                                                                                
                                                                                                                                            0                                                           
x1                                                                                                                                                                                                     
                                                                                                                                                                                                       
x2                                                                                                                                                                                                     
                                                                                                                                            + x3                                                        
x3                                                                                                                                                                                                     
                                                                                                                                                                                                       
x4                                                                     =                                                                                                                               
                                                                                                                                            −1                                                          
                                                                                                                                                                                                      
                                                                                                                                            0                                                           
                                                                     x1 + 2x2 − x3                                                                                                                     
                                                                                                                                                                                                       
                                                                     x1 + x4                                                                                                                           
                                                                                                                                            + x4                                                        
                                                                                                                                                                                                      
                                                                                                                                                                                                       
7→                                                                    (2.125a)                                                                                                                          

%%%

0                                                                                                                                           1                                                           
                                                                      ,                                                                                                                                 
1                                                                                                                                           2                                                           
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                      0                                                                                                                                 
(2.125b)                                                                                                                                    . (2.127)                                                   
                                                                      1                                                                                                                                 
is  linear.  To determine Im(Φ), we can take the span of the                                                                                This  matrix  is in reduced row-echelon form, and we can use
columns of the                                                                                                                             the  Minus1  Trick  to  compute  a  basis of the kernel (see
                                                                                                                                            Section 2.3.3). Alternatively,                              
transformation matrix and obtain                                      ] . (2.126)                                                                                                                       
                                                                                                                                            we  can  express  the non-pivot columns (columns 3 and 4) as
Im(Φ) = span[                                                        To compute the kernel (null space) of Φ, we need to solve Ax          linear  combinations of the pivot columns (columns 1 and 2).
                                                                      = 0, i.e.,                                                            The third column a3 is                                      
1                                                                                                                                                                                                       
                                                                      we  need to solve a homogeneous equation system. To do this,          equivalent to −                                             
1                                                                     we use                                                                                                                            
                                                                                                                                            1                                                           
                                                                     Gaussian elimination to transform A into reduced row-echelon                                                                      
                                                                      form:                                                                 2                                                           
,                                                                                                                                                                                                       
                                                                                                                                           times the second column a2. Therefore, 0 = a3+              
                                                                                                                                                                                                       
                                                                      1 2 −1 0                                                              1                                                           
2                                                                                                                                                                                                       
                                                                      1 0 0 1                                                              2                                                           
0                                                                                                                                                                                                       
                                                                      ⇝ · · · ⇝                                                             a2. In                                                      
                                                                                                                                                                                                       
                                                                                                                                           the same way, we see that a4 = a1−                          
,                                                                                                                                                                                                       
                                                                      1 0 0 1                                                               1                                                           
                                                                                                                                                                                                       
                                                                      0 1 −                                                                 2                                                           
−1                                                                                                                                                                                                      
                                                                      1                                                                     a2 and, therefore, 0 = a1−                                  
0                                                                                                                                                                                                       
                                                                      2 −                                                                   1                                                           
                                                                                                                                                                                                       

%%%

2                                                                                                                                                                                                      
                                                                                                                                            consequences of Theorem 2.24:                               
a2−a4.                                                                                                                                                                                                 
                                                                                                                                            If  dim(Im(Φ))  < dim(V ), then ker(Φ) is non-trivial, i.e.,
Overall, this gives us the kernel (null space) as                     −1                                                                    the kernel                                                  
                                                                                                                                                                                                        
ker(Φ) = span[                                                        1                                                                     contains more than 0V and dim(ker(Φ)) ⩾ 1.                  
                                                                                                                                                                                                        
                                                                     2                                                                     If  AΦ  is the transformation matrix of Φ with respect to an
                                                                                                                                            ordered basis                                               
                                                                     0                                                                                                                                 
                                                                                                                                            and  dim(Im(Φ))  <  dim(V  ),  then  the  system  of  linear
                                                                     1                                                                     equations AΦx =                                             
                                                                                                                                                                                                        
                                                                                                                                          0 has infinitely many solutions.                            
                                                                                                                                                                                                        
0                                                                                                                                          If   dim(V   )   =  dim(W),  then  the  following  three-way
                                                                                                                                            equivalence holds:                                          
1                                                                                                                                                                                                      
                                                                                                                                            – Φ is injective                                            
2                                                                                                                                                                                                      
                                                                                                                                            – Φ is surjective                                           
1                                                                     ] . (2.128)                                                                                                                       
                                                                                                                                            – Φ is bijective                                            
0                                                                     rank-nullity                                                                                                                      
                                                                                                                                            since Im(Φ) ⊆ W.                                            
                                                                     theorem  Theorem  2.24  (Rank-Nullity  Theorem).  For vector                                                                      
                                                                      spaces V, W and a linear mapping Φ : V → W it holds that              Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
                                                                                                                                           Feedback: https://mml-book.com.                             
                                                                      dim(ker(Φ)) + dim(Im(Φ)) = dim(V ). (2.129)                                                                                       
                                                                                                                                           2.8 Affine Spaces 61                                        
                                                                      fundamental  The rank-nullity theorem is also referred to as                                                                      
                                                                     the fundamental theorem                                               2.8 Affine Spaces                                           
                                                                                                                                                                                                        
,                                                                     theorem of linear                                                     In  the following, we will have a closer look at spaces that
                                                                                                                                            are offset from                                             
                                                                     mappings                                                                                                                          
                                                                                                                                            the   origin,   i.e.,  spaces  that  are  no  longer  vector
                                                                     of   linear   mappings  (Axler,  2015,  theorem  3.22).  The          subspaces. Moreover, we                                     
                                                                      following are direct                                                                                                              

%%%

will  briefly  discuss  properties of mappings between these          hyperplane                                                            written line                                                
affine spaces,                                                                                                                                                                                          
                                                                      Note that the definition of an affine subspace excludes 0 if          as y = x0 + λb1, where λ ∈ R and U = span[b1] ⊆ Rn          
which resemble linear mappings.                                       x0 ∈/ U.                                                                                                                          
                                                                                                                                            is a onedimensional subspace of Rn                          
Remark.  In the machine learning literature, the distinction          Therefore,  an  affine  subspace  is not a (linear) subspace                                                                      
between linear                                                        (vector subspace)                                                     .  This  means  that a line is defined by a support point x0
                                                                                                                                            and a vector b1 that defines the direction. See Figure 2.2  
and  affine  is  sometimes  not  clear  so  that we can find          of V for x0 ∈/ U.                                                                                                                 
references to affine                                                                                                                        for an illustration.                                        
                                                                      Examples  of  affine subspaces are points, lines, and planes                                                                      
spaces/mappings as linear spaces/mappings. ♢                          in R3                                                                 ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
2.8.1 Affine Subspaces                                                , which                                                                                                                           
                                                                                                                                            62 Linear Algebra                                           
Definition  2.25 (Affine Subspace). Let V be a vector space,          do not (necessarily) go through the origin.                                                                                       
x0 ∈ V and                                                                                                                                  Two-dimensional affine subspaces of Rn                      
                                                                      Remark.  Consider  two  affine subspaces L = x0 + U and L˜ =                                                                      
U ⊆ V a subspace. Then the subset                                     x˜0 + U˜ of a                                                         plane  are called planes. The parametric equation for planes
                                                                                                                                            is y = x0 + λ1b1 + λ2b2, where λ1, λ2 ∈ R                   
L = x0 + U := {x0 + u : u ∈ U} (2.130a)                               vector  space  V . Then, L ⊆ L˜ if and only if U ⊆ U˜ and x0                                                                      
                                                                      − x˜0 ∈ U˜.                                                           and U = span[b1, b2] ⊆ Rn                                   
= {v ∈ V |∃u ∈ U : v = x0 + u} ⊆ V (2.130b)                                                                                                                                                             
                                                                      Affine subspaces are often described by parameters: Consider          . This means that a plane is defined by a                   
is  called  affine  subspace  or linear manifold of V . U is          a k-dimensional affine space L = x0 + U of V . If (b1, . . .                                                                      
called direction or affine subspace                                   , bk) is an ordered basis of                                          support point x0 and two linearly independent vectors b1, b2
                                                                                                                                            that span                                                   
linear manifold                                                       U, then every element x ∈ L can be uniquely described as                                                                          
                                                                                                                                            the direction space.                                        
direction                                                             x = x0 + λ1b1 + . . . + λkbk , (2.131)                                                                                            
                                                                                                                                            In Rn                                                       
direction  space, and x0 is called support point. In Chapter          where  λ1,  .  .  .  , λk ∈ R. This representation is called                                                                      
12, we refer to                                                       parametric equation parametric equation                               hyperplane  ,  the  (n − 1)-dimensional affine subspaces are
                                                                                                                                            called hyperplanes,                                         
direction space                                                       of  L with directional vectors b1, . . . , bk and parameters                                                                      
                                                                      λ1, . . . , λk. ♢ parameters                                          and the corresponding parametric equation is y = x0 +       
support point                                                                                                                                                                                           
                                                                      Example 2.26 (Affine Subspaces)                                       Pn−1                                                        
such a subspace as a hyperplane.                                                                                                                                                                        
                                                                      One-dimensional affine subspaces are called lines and can be          i=1 λibi                                                    

%%%

                                                                                                                                            Similar  to  linear mappings between vector spaces, which we
,                                                                     y                                                                     discussed                                                   
                                                                                                                                                                                                        
where   b1,  .  .  .  ,  bn−1  form  a  basis  of  an  (n  −          L = x0 + λb1                                                          in  Section  2.7,  we can define affine mappings between two
1)-dimensional subspace                                                                                                                     affine spaces.                                              
                                                                      Remark (Inhomogeneous systems of linear equations and affine                                                                      
U of Rn                                                               subspaces).                                                           Linear  and  affine mappings are closely related. Therefore,
                                                                                                                                            many properties                                             
. This means that a hyperplane is defined by a support point          For  A  ∈  Rm×n  and  x  ∈ Rm, the solution of the system of                                                                      
                                                                      linear equations Aλ = x is either the empty set or an affine          that  we  already  know from linear mappings, e.g., that the
x0 and (n − 1) linearly independent vectors b1, . . . , bn−1          subspace of Rn of                                                     composition of                                              
that span the                                                                                                                                                                                           
                                                                      dimension  n  −  rk(A).  In  particular, the solution of the          linear  mappings  is  a linear mapping, also hold for affine
direction space. In R2                                                linear equation                                                       mappings.                                                   
                                                                                                                                                                                                        
, a line is also a hyperplane. In R3                                  λ1b1 + . . . + λnbn = x, where (λ1, . . . , λn) ̸= (0, . . .          Definition  2.26  (Affine Mapping). For two vector spaces V,
                                                                      , 0), is a hyperplane                                                 W, a linear                                                 
, a plane is also                                                                                                                                                                                       
                                                                      in Rn                                                                 Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
a hyperplane.                                                                                                                               Feedback: https://mml-book.com.                             
                                                                      .                                                                                                                                 
Figure 2.2 Lines                                                                                                                            2.9 Further Reading 63                                      
                                                                      In Rn                                                                                                                             
are affine subspaces.                                                                                                                       mapping Φ : V → W, and a ∈ W, the mapping                   
                                                                      ,  every k-dimensional affine subspace is the solution of an                                                                      
Vectors y on a line                                                   inhomogeneous  system  of linear equations Ax = b, where A ∈          ϕ : V → W (2.132)                                           
                                                                      Rm×n                                                                                                                              
x0 + λb1 lie in an                                                                                                                          x 7→ a + Φ(x) (2.133)                                       
                                                                      , b ∈                                                                                                                             
affine subspace L                                                                                                                           is an affine mapping from V to W. The vector a is called the
                                                                      Rm  and  rk(A) = n − k. Recall that for homogeneous equation          translation affine mapping                                  
with support point                                                    systems                                                                                                                           
                                                                                                                                            vector of ϕ. translation vector                             
x0 and direction b1.                                                  Ax = 0 the solution was a vector subspace, which we can also                                                                      
                                                                      think of                                                              Every  affine mapping ϕ : V → W is also the composition of a
0                                                                                                                                           linear                                                      
                                                                      as a special affine space with support point x0 = 0. ♢                                                                            
x0                                                                                                                                          mapping  Φ  :  V  → W and a translation τ : W → W in W, such
                                                                      2.8.2 Affine Mappings                                                 that                                                        
b1                                                                                                                                                                                                      

%%%

ϕ = τ ◦ Φ. The mappings Φ and τ are uniquely determined.              geometry  of a vector space. In Chapter 3, we will introduce          k = {x ∈ Z | x − k = 0 (modn)}                              
                                                                      the inner                                                                                                                         
The composition ϕ                                                                                                                           = {x ∈ Z | ∃a ∈ Z: (x − k = n · a)} .                       
                                                                      product,  which  induces  a norm. These concepts allow us to                                                                      
′ ◦ ϕ of affine mappings ϕ : V → W, ϕ                                 define angles,                                                        We  now define Z/nZ (sometimes written Zn) as the set of all
                                                                                                                                            congruence                                                  
′                                                                     lengths  and  distances,  which  we  will use for orthogonal                                                                      
                                                                      projections.  Projections turn out to be key in many machine          classes  modulo  n. Euclidean division implies that this set
: W → X is                                                            learning algorithms, such as                                          is a finite set containing n elements:                      
                                                                                                                                                                                                        
affine.                                                               linear  regression and principal component analysis, both of          Zn = {0, 1, . . . , n − 1}                                  
                                                                      which we will                                                                                                                     
Affine mappings keep the geometric structure invariant. They                                                                                For all a, b ∈ Zn, we define                                
also preserve the dimension and parallelism.                          cover in Chapters 9 and 10, respectively.                                                                                         
                                                                                                                                            a ⊕ b := a + b                                              
2.9 Further Reading                                                   ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
                                                                      by Cambridge University Press (2020).                                 a. Show that (Zn, ⊕) is a group. Is it Abelian?             
There  are  many  resources  for  learning  linear  algebra,                                                                                                                                            
including  the  textbooks  by  Strang  (2003), Golan (2007),          64 Linear Algebra                                                     b.  We  now define another operation ⊗ for all a and b in Zn
Axler (2015), and Liesen and                                                                                                                as                                                          
                                                                      Exercises                                                                                                                         
Mehrmann  (2015).  There  are  also several online resources                                                                                a ⊗ b = a × b , (2.135)                                     
that  we  mentioned  in the introduction to this chapter. We          2.1 We consider (R\{−1}, ⋆), where                                                                                                
only  covered  Gaussian elimination here, but there are many                                                                                where a × b represents the usual multiplication in Z.       
other approaches for solving systems of                               a ⋆ b := ab + a + b, a, b ∈ R\{−1} (2.134)                                                                                        
                                                                                                                                            Let  n  =  5. Draw the times table of the elements of Z5\{0}
linear  equations,  and we refer to numerical linear algebra          a. Show that (R\{−1}, ⋆) is an Abelian group.                         under ⊗, i.e.,                                              
textbooks by                                                                                                                                                                                            
                                                                      b. Solve                                                              calculate the products a ⊗ b for all a and b in Z5\{0}.     
Stoer  and  Burlirsch (2002), Golub and Van Loan (2012), and                                                                                                                                            
Horn and                                                              3 ⋆ x ⋆ x = 15                                                        Hence,  show  that  Z5\{0} is closed under ⊗ and possesses a
                                                                                                                                            neutral                                                     
Johnson (2013) for an in-depth discussion.                            in  the  Abelian  group  (R\{−1},  ⋆), where ⋆ is defined in                                                                      
                                                                      (2.134).                                                              element for ⊗. Display the inverse of all elements in Z5\{0}
In  this  book,  we distinguish between the topics of linear                                                                                under ⊗.                                                    
algebra (e.g.,                                                        2.2  Let  n  be  in  N\{0}.  Let k, x be in Z. We define the                                                                      
                                                                      congruence class k¯ of the                                            Conclude that (Z5\{0}, ⊗) is an Abelian group.              
vectors,  matrices,  linear  independence, basis) and topics                                                                                                                                            
related to the                                                        integer k as the set                                                  c. Show that (Z8\{0}, ⊗) is not a group.                    
                                                                                                                                                                                                        

%%%

d.  We  recall  that  the  B´ezout  theorem  states that two                                                                                                                                            
integers a and b are                                                                                                                                                                                  
                                                                                                                                                                                                        
relatively  prime (i.e., gcd(a, b) = 1) if and only if there                                                                                                                                          
exist two integers                                                                                                                                                                                      
                                                                                                                                                                                                      
u  and  v  such that au + bv = 1. Show that (Zn\{0}, ⊗) is a                                                                                                                                            
group if and                                                                                                                                                                                          
                                                                                                                                                                                                        
only if n ∈ N\{0} is prime.                                           x, y, z ∈ R                                                           1 1 0                                                       
                                                                                                                                                                                                        
2.3 Consider the set G of 3 × 3 matrices defined as follows:                                                                               0 1 1                                                       
                                                                                                                                                                                                        
G =                                                                                                                                        1 0 1                                                       
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     We define · as the standard matrix multiplication.                                                                               
                                                                                                                                                                                                        
                                                                     Is  (G,  ·)  a  group?  If  yes, is it Abelian? Justify your          b.                                                          
                                                                      answer.                                                                                                                           
                                                                                                                                                                                                      
                                                                      2.4 Compute the following matrix products, if possible:                                                                           
                                                                                                                                                                                                      
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
1 x z                                                                 Feedback: https://mml-book.com.                                       1 2 3                                                       
                                                                                                                                                                                                        
0 1 y                                                                 Exercises 65                                                          4 5 6                                                       
                                                                                                                                                                                                        
0 0 1                                                                 a.                                                                    7 8 9                                                       
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
 ∈ R                                                                                                                                                                                                 
                                                                                                                                                                                                        
3×3                                                                   1 2                                                                                                                              
                                                                                                                                                                                                        
                                                                     4 5                                                                                                                              
                                                                                                                                                                                                        
                                                                     7 8                                                                   1 1 0                                                       

%%%

                                                                                                                                                                                                        
0 1 1                                                                                                                                                                                                 
                                                                                                                                                                                                        
1 0 1                                                                 1 2 1 2                                                               0 3                                                         
                                                                                                                                                                                                        
                                                                     4 1 −1 −4                                                             1 −1                                                        
                                                                                                                                                                                                        
                                                                                                                                          2 1                                                         
                                                                                                                                                                                                        
c.                                                                                                                                         5 2                                                         
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 0                                                                                                                                                                                                 
                                                                                                                                                                                                        
0 1 1                                                                 0 3                                                                                                                              
                                                                                                                                                                                                        
1 0 1                                                                 1 −1                                                                                                                             
                                                                                                                                                                                                        
                                                                     2 1                                                                   1 2 1 2                                                     
                                                                                                                                                                                                        
                                                                     5 2                                                                   4 1 −1 −4                                                   
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          2.5  Find  the  set S of all solutions in x of the following
                                                                                                                                            inhomogeneous linear                                        
1 2 3                                                                                                                                                                                                  
                                                                                                                                            systems Ax = b, where A and b are defined as follows:       
4 5 6                                                                                                                                                                                                  
                                                                                                                                            a.                                                          
7 8 9                                                                 e.                                                                                                                                
                                                                                                                                            A =                                                         
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
d.                                                                                                                                                                                                     

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 −1 −1                                                                                                                                  3                                                           
                                                                                                                                                                                                        
2 5 −7 −5                                                             b.                                                                    6                                                           
                                                                                                                                                                                                        
2 −1 1 3                                                              A =                                                                   5                                                           
                                                                                                                                                                                                        
5 2 −4 2                                                                                                                                   −1                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1 −1 0 0 1                                                                                                                       
                                                                                                                                                                                                        
, b =                                                                 1 1 0 −3 0                                                            2.6  Using  Gaussian  elimination, find all solutions of the
                                                                                                                                            inhomogeneous equation system Ax = b with                   
                                                                     2 −1 0 1 −1                                                                                                                       
                                                                                                                                            A =                                                         
                                                                     −1 2 0 −2 −1                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                                                                                            0 1 0 0 1 0                                                 
1                                                                                                                                                                                                      
                                                                                                                                            0 0 0 1 1 0                                                 
−2                                                                                                                                                                                                     
                                                                                                                                            0 1 0 0 0 1                                                 
4                                                                     , b =                                                                                                                             
                                                                                                                                                                                                       
6                                                                                                                                                                                                      
                                                                                                                                             , b =                                                     
                                                                                                                                                                                                      
                                                                                                                                                                                                       

%%%

                                                                                                                                                                                                       
                                                                                                                                                                                                      
                                                                      6 4 3                                                                                                                             
2                                                                                                                                                                                                      
                                                                      6 0 9                                                                                                                             
−1                                                                                                                                                                                                     
                                                                      0 8 0                                                                                                                             
1                                                                                                                                                                                                      
                                                                                                                                                                                                       
                                                                                                                                           1 0 1 0                                                     
                                                                                                                                                                                                       
 .                                                                                                                                         0 1 1 0                                                     
                                                                      and P3                                                                                                                            
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                1 1 0 1                                                     
by Cambridge University Press (2020).                                 i=1 xi = 1.                                                                                                                       
                                                                                                                                            1 1 1 0                                                     
66 Linear Algebra                                                     2.8  Determine  the  inverses  of  the following matrices if                                                                      
                                                                      possible:                                                                                                                        
2.7 Find all solutions in x =                                                                                                                                                                           
                                                                      a.                                                                                                                               
                                                                                                                                                                                                       
                                                                      A =                                                                                                                              
                                                                                                                                                                                                       
                                                                                                                                                                                                      
x1                                                                                                                                                                                                      
                                                                                                                                           2.9 Which of the following sets are subspaces of R3         
x2                                                                                                                                                                                                      
                                                                      2 3 4                                                                 ?                                                           
x3                                                                                                                                                                                                      
                                                                      3 4 5                                                                 a. A = {(λ, λ + µ                                           
                                                                                                                                                                                                       
                                                                      4 5 6                                                                 3                                                           
 ∈ R3 of the equation system Ax = 12x,                                                                                                                                                                 
                                                                                                                                           , λ − µ                                                     
where                                                                                                                                                                                                   
                                                                                                                                           3                                                           
A =                                                                                                                                                                                                     
                                                                      b.                                                                    ) | λ, µ ∈ R}                                               
                                                                                                                                                                                                       
                                                                      A =                                                                   b. B = {(λ                                                  

%%%

                                                                                                                                                                                                        
2                                                                                                                                                                                                     
                                                                                                                                                                                                        
, −λ                                                                  1                                                                     1                                                           
                                                                                                                                                                                                        
2                                                                     1                                                                     2                                                           
                                                                                                                                                                                                        
, 0) | λ ∈ R}                                                         −2                                                                    1                                                           
                                                                                                                                                                                                        
c. Let γ be in R.                                                                                                                          0                                                           
                                                                                                                                                                                                        
C = {(ξ1, ξ2, ξ3) ∈ R3                                                 , x3 =                                                              0                                                           
                                                                                                                                                                                                        
| ξ1 − 2ξ2 + 3ξ3 = γ}                                                                                                                                                                                 
                                                                                                                                                                                                        
d. D = {(ξ1, ξ2, ξ3) ∈ R3                                                                                                                                                                             
                                                                                                                                                                                                        
| ξ2 ∈ Z}                                                             3                                                                                                                                
                                                                                                                                                                                                        
2.10 Are the following sets of vectors linearly independent?          −3                                                                                                                               
                                                                                                                                                                                                        
a.                                                                    8                                                                                                                                
                                                                                                                                                                                                        
x1 =                                                                                                                                                                                                  
                                                                                                                                                                                                        
                                                                                                                                          , x2 =                                                      
                                                                                                                                                                                                        
                                                                     b.                                                                                                                               
                                                                                                                                                                                                        
2                                                                     x1 =                                                                                                                             
                                                                                                                                                                                                        
−1                                                                                                                                                                                                    
                                                                                                                                                                                                        
3                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
 , x2 =                                                                                                                                                                                              
                                                                                                                                                                                                        
                                                                                                                                          1                                                           

%%%

                                                                                                                                                                                                        
1                                                                     1                                                                                                                                
                                                                                                                                                                                                        
0                                                                     1                                                                     1                                                           
                                                                                                                                                                                                        
1                                                                                                                                          1                                                           
                                                                                                                                                                                                        
1                                                                                                                                          1                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                           , x2 =                                                    
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     2.11 Write                                                            1                                                           
                                                                                                                                                                                                        
                                                                     y =                                                                   2                                                           
                                                                                                                                                                                                        
, x3 =                                                                                                                                     3                                                           
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1                                                                      , x3 =                                                    
                                                                                                                                                                                                        
                                                                     −2                                                                                                                               
                                                                                                                                                                                                        
                                                                     5                                                                                                                                
                                                                                                                                                                                                        
                                                                                                                                          2                                                           
                                                                                                                                                                                                        
                                                                                                                                          −1                                                          
                                                                                                                                                                                                        
1                                                                     as linear combination of                                              1                                                           
                                                                                                                                                                                                        
0                                                                     x1 =                                                                                                                             
                                                                                                                                                                                                        
0                                                                                                                                                                                                     

%%%

                                                                                                                                                                                                      
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                                                                                            
Feedback: https://mml-book.com.                                                                                                                                                                       
                                                                                                                                                                                                        
Exercises 67                                                                                                                                                                                          
                                                                                                                                                                                                        
2.12 Consider two subspaces of R4                                     2                                                                                                                                
                                                                                                                                                                                                        
:                                                                     −1                                                                    ] , U2 = span[                                              
                                                                                                                                                                                                        
U1 = span[                                                            0                                                                                                                                
                                                                                                                                                                                                        
                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                          −1                                                          
                                                                                                                                                                                                        
1                                                                                                                                          −2                                                          
                                                                                                                                                                                                        
1                                                                     ,                                                                     2                                                           
                                                                                                                                                                                                        
−3                                                                                                                                         1                                                           
                                                                                                                                                                                                        
1                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
                                                                     1                                                                     ,                                                           
                                                                                                                                                                                                        
,                                                                     −1                                                                                                                               
                                                                                                                                                                                                        
                                                                     1                                                                                                                                
                                                                                                                                                                                                        

%%%

                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
2                                                                                                                                          , A2 =                                                      
                                                                                                                                                                                                        
−2                                                                    ] .                                                                                                                              
                                                                                                                                                                                                        
0                                                                     Determine a basis of U1 ∩ U2.                                                                                                    
                                                                                                                                                                                                        
0                                                                     2.13  Consider  two  subspaces  U1  and  U2, where U1 is the                                                                     
                                                                      solution space of the                                                                                                             
                                                                                                                                                                                                      
                                                                      homogeneous  equation  system A1x = 0 and U2 is the solution                                                                      
                                                                     space of the                                                          3 −3 0                                                      
                                                                                                                                                                                                        
                                                                     homogeneous equation system A2x = 0 with                              1 2 3                                                       
                                                                                                                                                                                                        
                                                                     A1 =                                                                  7 −5 2                                                      
                                                                                                                                                                                                        
,                                                                                                                                          3 −1 2                                                      
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
                                                                     1 0 1                                                                                                                            
                                                                                                                                                                                                        
−3                                                                    1 −2 −1                                                               .                                                           
                                                                                                                                                                                                        
6                                                                     2 1 3                                                                 a. Determine the dimension of U1, U2.                       
                                                                                                                                                                                                        
−2                                                                    1 0 1                                                                 b. Determine bases of U1 and U2.                            
                                                                                                                                                                                                        
−1                                                                                                                                         c. Determine a basis of U1 ∩ U2.                            
                                                                                                                                                                                                        
                                                                                                                                          2.14  Consider  two subspaces U1 and U2, where U1 is spanned
                                                                                                                                            by the columns of                                           

%%%

                                                                                                                                                                                                        
A1 and U2 is spanned by the columns of A2 with                        1 2 3                                                                 a. Let a, b ∈ R.                                            
                                                                                                                                                                                                        
A1 =                                                                  7 −5 2                                                                Φ : L                                                       
                                                                                                                                                                                                        
                                                                     3 −1 2                                                                1                                                           
                                                                                                                                                                                                        
                                                                                                                                          ([a, b]) → R                                                
                                                                                                                                                                                                        
                                                                                                                                          f 7→ Φ(f) = Z b                                             
                                                                                                                                                                                                        
                                                                                                                                          a                                                           
                                                                                                                                                                                                        
1 0 1                                                                                                                                      f(x)dx ,                                                    
                                                                                                                                                                                                        
1 −2 −1                                                               .                                                                     where L                                                     
                                                                                                                                                                                                        
2 1 3                                                                 a. Determine the dimension of U1, U2                                  1                                                           
                                                                                                                                                                                                        
1 0 1                                                                 b. Determine bases of U1 and U2                                       ([a, b]) denotes the set of integrable functions on [a, b]. 
                                                                                                                                                                                                        
                                                                     c. Determine a basis of U1 ∩ U2                                       b.                                                          
                                                                                                                                                                                                        
                                                                     2.15 Let F = {(x, y, z) ∈ R3                                          Φ : C                                                       
                                                                                                                                                                                                        
                                                                     | x+y−z = 0} and G = {(a−b, a+b, a−3b) | a, b ∈ R}.                   1 → C                                                       
                                                                                                                                                                                                        
                                                                     a. Show that F and G are subspaces of R3                              0                                                           
                                                                                                                                                                                                        
, A2 =                                                                .                                                                     f 7→ Φ(f) = f                                               
                                                                                                                                                                                                        
                                                                     b. Calculate F ∩ G without resorting to any basis vector.             ′                                                           
                                                                                                                                                                                                        
                                                                     c.  Find  one basis for F and one for G, calculate F∩G using          ,                                                           
                                                                      the basis vectors                                                                                                                 
                                                                                                                                           where for k ⩾ 1, C                                          
                                                                      previously  found  and  check  your result with the previous                                                                      
                                                                     question.                                                             k  denotes  the  set  of k times continuously differentiable
                                                                                                                                            functions, and C                                            
3 −3 0                                                                2.16 Are the following mappings linear?                                                                                           

%%%

0 denotes the set of continuous functions.                                                                                                                                                              
                                                                                                                                            =                                                         
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                                                                            
by Cambridge University Press (2020).                                 cos(θ) sin(θ)                                                                                                                    
                                                                                                                                                                                                        
68 Linear Algebra                                                     − sin(θ) cos(θ)                                                                                                                  
                                                                                                                                                                                                        
c.                                                                                                                                                                                                    
                                                                                                                                                                                                        
Φ : R → R                                                             x                                                                                                                                
                                                                                                                                                                                                        
x 7→ Φ(x) = cos(x)                                                    2.17 Consider the linear mapping                                      3x1 + 2x2 + x3                                              
                                                                                                                                                                                                        
d.                                                                    Φ : R                                                                 x1 + x2 + x3                                                
                                                                                                                                                                                                        
Φ : R                                                                 3 → R                                                                 x1 − 3x2                                                    
                                                                                                                                                                                                        
3 → R                                                                 4                                                                     2x1 + 3x2 + x3                                              
                                                                                                                                                                                                        
2                                                                     Φ                                                                                                                                
                                                                                                                                                                                                        
x 7→                                                                                                                                                                                                  
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 2 3                                                                                                                                                                                                 
                                                                                                                                                                                                        
1 4 3                                                                                                                                     Find the transformation matrix AΦ.                          
                                                                                                                                                                                                        
x                                                                     x1                                                                    Determine rk(AΦ).                                           
                                                                                                                                                                                                        
e. Let θ be in [0, 2π[ and                                            x2                                                                    Compute  the kernel and image of Φ. What are dim(ker(Φ)) and
                                                                                                                                            dim(Im(Φ))?                                                 
Φ : R                                                                 x3                                                                                                                                
                                                                                                                                            2.18  Let  E  be  a  vector  space.  Let  f  and  g  be  two
2 → R                                                                                                                                      automorphisms on E such that                                
                                                                                                                                                                                                        
2                                                                                                                                          f  ◦ g = idE (i.e., f ◦ g is the identity mapping idE). Show
                                                                                                                                            that ker(f) =                                               
x 7→                                                                                                                                                                                                   

%%%

ker(g  ◦  f),  Im(g)  =  Im(g ◦ f) and that ker(f) ∩ Im(g) =                                                                                                                                            
{0E}.                                                                 1                                                                     1                                                           
                                                                                                                                                                                                        
2.19   Consider   an   endomorphism   Φ  :  R3  →  R3  whose                                                                               , b                                                         
transformation matrix                                                                                                                                                                                   
                                                                      ,                                                                    ′                                                           
(with respect to the standard basis in R3                                                                                                                                                               
                                                                                                                                           2                                                           
) is                                                                                                                                                                                                    
                                                                                                                                           , 4 vectors of R2                                           
AΦ =                                                                                                                                                                                                    
                                                                      1                                                                     expressed in the standard basis                             
                                                                                                                                                                                                       
                                                                      2                                                                     of R2                                                       
                                                                                                                                                                                                       
                                                                      1                                                                     as                                                          
1 1 0                                                                                                                                                                                                   
                                                                                                                                           b1 =                                                        
1 −1 0                                                                                                                                                                                                  
                                                                      ,                                                                                                                               
1 1 1                                                                                                                                                                                                   
                                                                                                                                           2                                                           
                                                                                                                                                                                                       
                                                                                                                                           1                                                           
 .                                                                                                                                                                                                     
                                                                      1                                                                                                                                
a. Determine ker(Φ) and Im(Φ).                                                                                                                                                                          
                                                                      0                                                                     , b2 =                                                      
b.  Determine the transformation matrix A˜ Φ with respect to                                                                                                                                            
the basis                                                             0                                                                                                                                
                                                                                                                                                                                                        
B = (                                                                                                                                      −1                                                          
                                                                                                                                                                                                        
                                                                     ),                                                                   −1                                                          
                                                                                                                                                                                                        
                                                                     i.e., perform a basis change toward the new basis B.                                                                             
                                                                                                                                                                                                        
1                                                                     2.20 Let us consider b1, b2, b                                        , b                                                         
                                                                                                                                                                                                        
1                                                                     ′                                                                     ′                                                           

%%%

                                                                                                                                            −1                                                          
1 =                                                                   .                                                                                                                                 
                                                                                                                                                                                                       
                                                                     Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       , c2 =                                                     
2                                                                                                                                                                                                       
                                                                      Exercises 69                                                                                                                     
−2                                                                                                                                                                                                      
                                                                      a. Show that B and B                                                                                                             
                                                                                                                                                                                                       
                                                                      ′                                                                     0                                                           
, b                                                                                                                                                                                                     
                                                                      are two bases of R2                                                   −1                                                          
′                                                                                                                                                                                                       
                                                                      and draw those basis vectors.                                         2                                                           
2 =                                                                                                                                                                                                     
                                                                      b.  Compute the matrix P 1 that performs a basis change from                                                                     
                                                                     B                                                                                                                                 
                                                                                                                                            , c3 =                                                     
1                                                                     ′                                                                                                                                 
                                                                                                                                                                                                       
1                                                                     to B.                                                                                                                             
                                                                                                                                                                                                       
                                                                     c.  We  consider  c1, c2, c3, three vectors of R3 defined in                                                                      
                                                                      the standard basis                                                    1                                                           
and let us define two ordered bases B = (b1, b2) and B                                                                                                                                                  
                                                                      of R3                                                                 0                                                           
′ = (b                                                                                                                                                                                                  
                                                                      as                                                                    −1                                                          
′                                                                                                                                                                                                       
                                                                      c1 =                                                                                                                             
1                                                                                                                                                                                                       
                                                                                                                                                                                                      
, b                                                                                                                                                                                                     
                                                                                                                                           and we define C = (c1, c2, c3).                             
′                                                                                                                                                                                                       
                                                                      1                                                                     (i) Show that C is a basis of R3                            
2                                                                                                                                                                                                       
                                                                      2                                                                     , e.g., by using determinants (see                          
) of R2                                                                                                                                                                                                 

%%%

Section 4.1).                                                         where B = (b1, b2) and C = (c1, c2, c3) are ordered bases of          .                                                           
                                                                      R2                                                                                                                                
(ii) Let us call C                                                                                                                          (i) Calculate the coordinates of x in B.                    
                                                                      and R3                                                                                                                            
′ = (c                                                                                                                                      (ii)   Based  on  that,  compute  the  coordinates  of  Φ(x)
                                                                      ,                                                                     expressed in C.                                             
′                                                                                                                                                                                                       
                                                                      respectively.                                                         (iii) Then, write Φ(x) in terms of c                        
1                                                                                                                                                                                                       
                                                                      Determine  the transformation matrix AΦ of Φ with respect to          ′                                                           
, c                                                                   the ordered bases B and C.                                                                                                        
                                                                                                                                            1                                                           
′                                                                     e. Determine A′                                                                                                                   
                                                                                                                                            , c                                                         
2                                                                     , the transformation matrix of Φ with respect to the bases                                                                        
                                                                                                                                            ′                                                           
, c                                                                   B′                                                                                                                                
                                                                                                                                            2                                                           
′                                                                     and C                                                                                                                             
                                                                                                                                            , c                                                         
3                                                                     ′                                                                                                                                 
                                                                                                                                            ′                                                           
) the standard basis of R3                                            .                                                                                                                                 
                                                                                                                                            3                                                           
. Determine                                                           f. Let us consider the vector x ∈ R2 whose coordinates in B                                                                       
                                                                                                                                            .                                                           
the matrix P 2 that performs the basis change from C to C             ′                                                                                                                                 
                                                                                                                                            (iv) Use the representation of x in B                       
′                                                                     are [2, 3]⊤.                                                                                                                      
                                                                                                                                            ′                                                           
.                                                                     In other words, x = 2b                                                                                                            
                                                                                                                                            and the matrix A′                                           
d. We consider a homomorphism Φ : R2 −→ R3                            ′                                                                                                                                 
                                                                                                                                            to find this                                                
, such that                                                           1 + 3b                                                                                                                            
                                                                                                                                            result directly.                                            
Φ(b1 + b2) = c2 + c3                                                  ′                                                                                                                                 
                                                                                                                                            ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
Φ(b1 − b2) = 2c1 − c2 + 3c3                                           2                                                                     by Cambridge University Press (2020).                       
                                                                                                                                                                                                        

%%%

3                                                                     Figure 3.1 A mind                                                     Classification                                              
                                                                                                                                                                                                        
Analytic Geometry                                                     map of the concepts                                                   induces                                                     
                                                                                                                                                                                                        
In  Chapter 2, we studied vectors, vector spaces, and linear          introduced in this                                                    70                                                          
mappings at                                                                                                                                                                                             
                                                                      chapter, along with                                                   This  material is published by Cambridge University Press as
a  general  but abstract level. In this chapter, we will add                                                                                Mathematics for Machine Learning by                         
some  geometric interpretation and intuition to all of these          when they are used                                                                                                                
concepts. In particular, we                                                                                                                 Marc  Peter  Deisenroth,  A. Aldo Faisal, and Cheng Soon Ong
                                                                      in other parts of the                                                 (2020). This version is free to view                        
will look at geometric vectors and compute their lengths and                                                                                                                                            
distances                                                             book.                                                                 and download for personal use only. Not for re-distribution,
                                                                                                                                            re-sale, or use in derivative works.                        
or  angles  between  two  vectors. To be able to do this, we          Inner product                                                                                                                     
equip  the  vector  space with an inner product that induces                                                                                ©by  M.  P.  Deisenroth,  A. A. Faisal, and C. S. Ong, 2021.
the geometry of the vector                                            Norm                                                                  https://mml-book.com.                                       
                                                                                                                                                                                                        
space.  Inner  products  and  their  corresponding norms and          Lengths Orthogonal                                                    3.1 Norms 71                                                
metrics capture                                                                                                                                                                                         
                                                                      projection Angles Rotations                                           Figure 3.1 For                                              
the  intuitive notions of similarity and distances, which we                                                                                                                                            
use to develop                                                        Chapter 4                                                             different norms, the                                        
                                                                                                                                                                                                        
the  support  vector machine in Chapter 12. We will then use          Matrix                                                                red lines indicate                                          
the concepts                                                                                                                                                                                            
                                                                      decomposition                                                         the set of vectors                                          
of  lengths and angles between vectors to discuss orthogonal                                                                                                                                            
projections,                                                          Chapter 10                                                            with norm 1. Left:                                          
                                                                                                                                                                                                        
which  will  play  a  central role when we discuss principal          Dimensionality                                                        Manhattan norm;                                             
component  analysis in Chapter 10 and regression via maximum                                                                                                                                            
likelihood estimation in                                              reduction                                                             Right: Euclidean                                            
                                                                                                                                                                                                        
Chapter  9.  Figure 3.1 gives an overview of how concepts in          Chapter 9                                                             distance.                                                   
this chapter                                                                                                                                                                                            
                                                                      Regression                                                            1                                                           
are  related and how they are connected to other chapters of                                                                                                                                            
the book.                                                             Chapter 12                                                            1 1                                                         
                                                                                                                                                                                                        

%%%

1                                                                                                                                                                                                       
                                                                      inequality.                                                           i=1                                                         
kxk1 = 1 kxk2 = 1                                                                                                                                                                                       
                                                                      a b                                                                   |xi                                                         
3.1 Norms                                                                                                                                                                                               
                                                                      c ≤ a + b                                                             | , (3.3)                                                   
When  we  think  of  geometric  vectors, i.e., directed line                                                                                                                                            
segments that start                                                   In  geometric terms, the triangle inequality states that for          where  | · | is the absolute value. The left panel of Figure
                                                                      any triangle,                                                         3.1 shows all                                               
at  the  origin,  then intuitively the length of a vector is                                                                                                                                            
the distance of the                                                   the sum of the lengths of any two sides must be greater than          vectors  x  ∈  R2  with ∥x∥1 = 1. The Manhattan norm is also
                                                                      or equal                                                              called ℓ1 ℓ1 norm                                           
“end”  of this directed line segment from the origin. In the                                                                                                                                            
following, we                                                         to  the  length of the remaining side; see Figure 3.2 for an          norm.                                                       
                                                                      illustration.                                                                                                                     
will  discuss  the notion of the length of vectors using the                                                                                ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
concept of a norm.                                                    Definition  3.1  is  in  terms  of  a general vector space V          by Cambridge University Press (2020).                       
                                                                      (Section 2.4), but                                                                                                                
Definition  3.1  (Norm).  A  norm  on  a vector space V is a                                                                                72 Analytic Geometry                                        
function norm                                                         in  this  book  we  will  only consider a finite-dimensional                                                                      
                                                                      vector space Rn                                                       Example 3.2 (Euclidean Norm)                                
∥ · ∥ : V → R , (3.1)                                                                                                                                                                                   
                                                                      .                                                                     The Euclidean norm of x ∈ Rn Euclidean norm is defined as   
x 7→ ∥x∥ , (3.2)                                                                                                                                                                                        
                                                                      Recall  that  for  a vector x ∈ Rn we denote the elements of          ∥x∥2 :=                                                     
which  assigns  each  vector x its length ∥x∥ ∈ R, such that          the vector using                                                                                                                  
for all λ ∈ R length                                                                                                                        vuutXn                                                      
                                                                      a subscript, that is, xi                                                                                                          
and x, y ∈ V the following hold:                                                                                                            i=1                                                         
                                                                      is the i                                                                                                                          
absolutely                                                                                                                                  x                                                           
                                                                      th element of the vector x.                                                                                                       
Absolutely homogeneous: ∥λx∥ = homogeneous |λ|∥x∥                                                                                           2                                                           
                                                                      Example 3.1 (Manhattan Norm)                                                                                                      
Triangle inequality: ∥x + y∥ ⩽ ∥x∥ + ∥y∥ triangle inequality                                                                                i =                                                         
                                                                      The Manhattan norm on Rn                                                                                                          
Positive  definite: positive definite ∥x∥ ⩾ 0 and ∥x∥ = 0 ⇐⇒                                                                                √                                                           
x = 0                                                                 is defined for x ∈ Rn as Manhattan norm                                                                                           
                                                                                                                                            x⊤x (3.4)                                                   
Figure 3.2 Triangle                                                   ∥x∥1 := Xn                                                                                                                        

%%%

Euclidean  distance and computes the Euclidean distance of x                                                                                                                                            
from the origin. The right panel                                      i=1                                                                   3.2 Inner Products 73                                       
                                                                                                                                                                                                        
of  Figure  3.1  shows all vectors x ∈ R2 with ∥x∥2 = 1. The          xiyi                                                                  Definition 3.2. Let V be a vector space and Ω : V × V → R be
Euclidean                                                                                                                                   a bilinear                                                  
                                                                      . (3.5)                                                                                                                           
ℓ2 norm norm is also called ℓ2 norm.                                                                                                        mapping  that  takes  two  vectors and maps them onto a real
                                                                      We  will  refer  to this particular inner product as the dot          number. Then                                                
Remark. Throughout this book, we will use the Euclidean norm          product in this                                                                                                                   
(3.4) by                                                                                                                                    Ω  is called symmetric if Ω(x, y) = Ω(y, x) for all x, y ∈ V
                                                                      book. However, inner products are more general concepts with          , i.e., the symmetric                                       
default if not stated otherwise. ♢                                    specific                                                                                                                          
                                                                                                                                            order of the arguments does not matter.                     
3.2 Inner Products                                                    properties, which we will now introduce.                                                                                          
                                                                                                                                            Ω is called positive definite if positive definite          
Inner  products  allow  for  the  introduction  of intuitive          3.2.2 General Inner Products                                                                                                      
geometrical concepts, such as the length of a vector and the                                                                                ∀x ∈ V \{0} : Ω(x, x) > 0 , Ω(0, 0) = 0 . (3.8)             
angle or distance between                                             Recall  the  linear  mapping  from Section 2.7, where we can                                                                      
                                                                      rearrange the                                                         Definition 3.3. Let V be a vector space and Ω : V × V → R be
two  vectors.  A  major  purpose  of  inner  products  is to                                                                                a bilinear                                                  
determine whether                                                     bilinear  mapping  mapping  with  respect  to  addition  and                                                                      
                                                                      multiplication  with  a  scalar.  A  bilinear mapping Ω is a          mapping  that  takes  two  vectors and maps them onto a real
vectors are orthogonal to each other.                                 mapping with two arguments, and it is linear in                       number. Then                                                
                                                                                                                                                                                                        
3.2.1 Dot Product                                                     each  argument,  i.e., when we look at a vector space V then          A positive definite, symmetric bilinear mapping Ω : V ×V → R
                                                                      it holds that                                                         is called                                                   
We  may  already be familiar with a particular type of inner                                                                                                                                            
product, the                                                          for all x, y, z ∈ V, λ, ψ ∈ R that                                    an inner product on V . We typically write ⟨x, y⟩ instead of
                                                                                                                                            Ω(x, y). inner product                                      
scalar product/dot product in Rn                                      Ω(λx + ψy, z) = λΩ(x, z) + ψΩ(y, z) (3.6)                                                                                         
                                                                                                                                            The  pair  (V,⟨·,  ·⟩)  is  called an inner product space or
scalar product , which is given by                                    Ω(x, λy + ψz) = λΩ(x, y) + ψΩ(x, z). (3.7)                            (real) vector space inner product space                     
                                                                                                                                                                                                        
dot product                                                           Here,  (3.6) asserts that Ω is linear in the first argument,          vector space with                                           
                                                                      and (3.7) asserts                                                                                                                 
x                                                                                                                                           inner product                                               
                                                                      that Ω is linear in the second argument (see also (2.87)).                                                                        
⊤y =                                                                                                                                        with  inner  product.  If  we use the dot product defined in
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          (3.5), we call                                              
Xn                                                                    Feedback: https://mml-book.com.                                                                                                   

%%%

(V,⟨·, ·⟩) a Euclidean vector space.                                  . . . , bn) of                                                                                                                    
                                                                                                                                            Xn                                                          
Euclidean vector                                                      V  . Recall from Section 2.6.1 that any vectors x, y ∈ V can                                                                      
                                                                      be written as                                                         i=1                                                         
We  will  refer  to  these spaces as inner product spaces in                                                                                                                                            
this book. space                                                      linear combinations of the basis vectors so that x =                  Xn                                                          
                                                                                                                                                                                                        
Example 3.3 (Inner Product That Is Not the Dot Product)               Pn                                                                    j=1                                                         
                                                                                                                                                                                                        
Consider V = R2                                                       i=1 ψibi ∈ V and                                                      ψi ⟨bi                                                      
                                                                                                                                                                                                        
. If we define                                                        y =                                                                   , bj ⟩ λj = xˆ                                              
                                                                                                                                                                                                        
⟨x, y⟩ := x1y1 − (x1y2 + x2y1) + 2x2y2 (3.9)                          Pn                                                                    ⊤Ayˆ , (3.10)                                               
                                                                                                                                                                                                        
then  ⟨·,  ·⟩ is an inner product but different from the dot          j=1 λjbj ∈ V for suitable ψi                                          where Aij := ⟨bi                                            
product. The proof                                                                                                                                                                                      
                                                                      , λj ∈ R. Due to the bilinearity of the                               ,  bj  ⟩  and  xˆ,  yˆ  are  the coordinates of x and y with
will be an exercise.                                                                                                                        respect                                                     
                                                                      inner product, it holds for all x, y ∈ V that                                                                                     
3.2.3 Symmetric, Positive Definite Matrices                                                                                                 to  the  basis B. This implies that the inner product ⟨·, ·⟩
                                                                      ⟨x, y⟩ =                                                              is  uniquely determined through A. The symmetry of the inner
Symmetric, positive definite matrices play an important role                                                                                product also means that A                                   
in machine                                                            *Xn                                                                                                                               
                                                                                                                                            ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
learning,  and  they  are  defined via the inner product. In          i=1                                                                   by Cambridge University Press (2020).                       
Section 4.3, we                                                                                                                                                                                         
                                                                      ψibi                                                                  74 Analytic Geometry                                        
will  return to symmetric, positive definite matrices in the                                                                                                                                            
context of matrix                                                     ,                                                                     is  symmetric. Furthermore, the positive definiteness of the
                                                                                                                                            inner product                                               
decompositions.  The idea of symmetric positive semidefinite          Xn                                                                                                                                
matrices is                                                                                                                                 implies that                                                
                                                                      j=1                                                                                                                               
key in the definition of kernels (Section 12.4).                                                                                            ∀x ∈ V \{0} : x                                             
                                                                      λjbj                                                                                                                              
Consider  an  n-dimensional  vector  space  V  with an inner                                                                                ⊤Ax > 0 . (3.11)                                            
product ⟨·, ·⟩ :                                                      +                                                                                                                                 
                                                                                                                                            Definition  3.4  (Symmetric,  Positive  Definite  Matrix). A
V × V → R (see Definition 3.3) and an ordered basis B = (b1,          =                                                                     symmetric matrix                                            

%%%

                                                                                                                                            ⊤A2x = 9x                                                   
A ∈ Rn×n                                                              x                                                                                                                                 
                                                                                                                                            2                                                           
symmetric,   positive   that   satisfies  (3.11)  is  called          ⊤A1x =                                                                                                                            
symmetric, positive definite, or                                                                                                            1 + 12x1x2 + 3x                                             
                                                                      x1 x2                                                                                                                             
definite  just positive definite. If only ⩾ holds in (3.11),                                                                                2                                                           
then A is called symmetric,                                                                                                                                                                            
                                                                                                                                            2 = (3x1 + 2x2)                                             
positive definite                                                     9 6                                                                                                                               
                                                                                                                                            2 − x                                                       
symmetric, positive                                                   6 5 x1                                                                                                                          
                                                                                                                                            2                                                           
semidefinite                                                          x2                                                                                                                                
                                                                                                                                            2                                                           
positive semidefinite.                                                                                                                                                                                 
                                                                                                                                            can be less                                                 
Example 3.4 (Symmetric, Positive Definite Matrices)                   (3.13a)                                                                                                                           
                                                                                                                                            than 0, e.g., for x = [2, −3]⊤.                             
Consider the matrices                                                 = 9x                                                                                                                              
                                                                                                                                            If A ∈ Rn×n                                                 
A1 =                                                                  2                                                                                                                                 
                                                                                                                                            is symmetric, positive definite, then                       
                                                                     1 + 12x1x2 + 5x                                                                                                                   
                                                                                                                                            ⟨x, y⟩ = xˆ                                                 
9 6                                                                   2                                                                                                                                 
                                                                                                                                            ⊤Ayˆ (3.14)                                                 
6 5                                                                  2 = (3x1 + 2x2)                                                                                                                   
                                                                                                                                            defines an inner product with respect to an ordered basis B,
, A2 =                                                                2 + x                                                                 where xˆ and                                                
                                                                                                                                                                                                        
                                                                     2                                                                     yˆ  are  the  coordinate  representations  of  x, y ∈ V with
                                                                                                                                            respect to B.                                               
9 6                                                                   2 > 0 (3.13b)                                                                                                                     
                                                                                                                                            Theorem  3.5.  For  a real-valued, finite-dimensional vector
6 3                                                                  for  all  x  ∈  V \{0}. In contrast, A2 is symmetric but not          space V and an                                              
                                                                      positive definite                                                                                                                 
. (3.12)                                                                                                                                    ordered  basis  B of V , it holds that ⟨·, ·⟩ : V × V → R is
                                                                      because x                                                             an inner product if                                         
A1 is positive definite because it is symmetric and                                                                                                                                                     

%%%

and  only  if  there  exists  a symmetric, positive definite                                                                                We can now use                                              
matrix A ∈ Rn×n with                                                  the length of a vector. Inner products and norms are closely                                                                      
                                                                      related in the                                                        an inner product to compute them using (3.16). Let us take x
⟨x, y⟩ = xˆ                                                                                                                                 = [1, 1]⊤ ∈                                                 
                                                                      sense that any inner product induces a norm Inner products                                                                        
⊤Ayˆ . (3.15)                                                                                                                               R2                                                          
                                                                      induce norms.                                                                                                                     
The following properties hold if A ∈ Rn×n                                                                                                   .  If  we  use  the  dot  product as the inner product, with
                                                                      ∥x∥ := q                                                              (3.16) we obtain                                            
is symmetric and positive                                                                                                                                                                               
                                                                      ⟨x, x⟩ (3.16)                                                         ∥x∥ =                                                       
definite:                                                                                                                                                                                               
                                                                      in  a  natural  way,  such  that  we  can compute lengths of          √                                                           
The null space (kernel) of A consists only of 0 because x             vectors  using the inner product. However, not every norm is                                                                      
                                                                      induced by an inner product. The                                      x⊤x =                                                       
⊤Ax > 0 for                                                                                                                                                                                             
                                                                      Manhattan  norm  (3.3)  is  an  example  of a norm without a          √                                                           
all x ̸= 0. This implies that Ax ̸= 0 if x ̸= 0.                      corresponding                                                                                                                     
                                                                                                                                            1                                                           
The diagonal elements aii of A are positive because aii = e           inner product. In the following, we will focus on norms that                                                                      
                                                                      are induced                                                           2 + 12 =                                                    
⊤                                                                                                                                                                                                       
                                                                      by  inner products and introduce geometric concepts, such as          √                                                           
i Aei > 0,                                                            lengths, distances, and angles.                                                                                                   
                                                                                                                                            2 (3.18)                                                    
where ei                                                              Remark  (Cauchy-Schwarz  Inequality).  For  an inner product                                                                      
                                                                      vector space                                                          as  the  length  of  x.  Let us now choose a different inner
is the ith vector of the standard basis in Rn                                                                                               product:                                                    
                                                                      (V,⟨·,   ·⟩)   the   induced   norm  ∥  ·  ∥  satisfies  the                                                                      
.                                                                     Cauchy-Schwarz inequality Cauchy-Schwarz                              ⟨x, y⟩ := x                                                 
                                                                                                                                                                                                        
Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          inequality                                                            ⊤                                                           
Feedback: https://mml-book.com.                                                                                                                                                                         
                                                                      | ⟨x, y⟩ | ⩽ ∥x∥∥y∥ . (3.17)                                                                                                     
3.3 Lengths and Distances 75                                                                                                                                                                            
                                                                      ♢                                                                     1 −                                                         
3.3 Lengths and Distances                                                                                                                                                                               
                                                                      Example 3.5 (Lengths of Vectors Using Inner Products)                 1                                                           
In  Section  3.1, we already discussed norms that we can use                                                                                                                                            
to compute                                                            In  geometry, we are often interested in lengths of vectors.          2                                                           

%%%

                                                                      √                                                                     does  not require an inner product: a norm is sufficient. If
−                                                                                                                                           we have a norm                                              
                                                                      1 = 1 , (3.20)                                                                                                                    
1                                                                                                                                           induced by an inner product, the distance may vary depending
                                                                      such  that  x is “shorter” with this inner product than with          on the                                                      
2                                                                     the dot product.                                                                                                                  
                                                                                                                                            choice of the inner product. ♢                              
1                                                                     Definition  3.6  (Distance  and  Metric).  Consider an inner                                                                      
                                                                      product space                                                         A metric d satisfies the following:                         
                                                                                                                                                                                                       
                                                                      (V,⟨·, ·⟩). Then                                                      positive  definite  1. d is positive definite, i.e., d(x, y)
y = x1y1 −                                                                                                                                  ⩾ 0 for all x, y ∈ V and d(x, y) =                          
                                                                      d(x, y) := ∥x − y∥ =                                                                                                              
1                                                                                                                                           0 ⇐⇒ x = y .                                                
                                                                      q                                                                                                                                 
2                                                                                                                                           symmetric 2. d is symmetric, i.e., d(x, y) = d(y, x) for all
                                                                      ⟨x − y, x − y⟩ (3.21)                                                 x, y ∈ V .                                                  
(x1y2 + x2y1) + x2y2 . (3.19)                                                                                                                                                                           
                                                                      is  called the distance between x and y for x, y ∈ V . If we          triangle  inequality  3. Triangle inequality: d(x, z) ⩽ d(x,
If  we compute the norm of a vector, then this inner product          use the dot distance                                                  y) + d(y, z) for all x, y, z ∈ V .                          
returns smaller                                                                                                                                                                                         
                                                                      product  as  the  inner product, then the distance is called          Remark.  At  first  glance, the lists of properties of inner
values  than the dot product if x1 and x2 have the same sign          Euclidean distance. Euclidean distance                                products   and   metrics  look  very  similar.  However,  by
(and x1x2 >                                                                                                                                 comparing Definition 3.3 with Definition 3.6 we observe that
                                                                      ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          ⟨x, y⟩ and d(x, y) behave in opposite directions.           
0);  otherwise,  it  returns  greater  values  than  the dot          by Cambridge University Press (2020).                                                                                             
product. With this                                                                                                                          Very  similar  x  and y will result in a large value for the
                                                                      76 Analytic Geometry                                                  inner product and                                           
inner product, we obtain                                                                                                                                                                                
                                                                      The mapping                                                           a small value for the metric. ♢                             
⟨x, x⟩ = x                                                                                                                                                                                              
                                                                      d : V × V → R (3.22)                                                  3.4 Angles and Orthogonality                                
2                                                                                                                                                                                                       
                                                                      (x, y) 7→ d(x, y) (3.23)                                              Figure 3.2 When                                             
1 − x1x2 + x                                                                                                                                                                                            
                                                                      metric is called a metric.                                            restricted to [0, π]                                        
2                                                                                                                                                                                                       
                                                                      Remark.  Similar  to  the  length  of a vector, the distance          then f(ω) = cos(ω)                                          
2 = 1 − 1 + 1 = 1 =⇒ ∥x∥ =                                            between vectors                                                                                                                   
                                                                                                                                            returns a unique                                            

%%%

                                                                      ⟨x, y⟩                                                                                                                            
number in the                                                                                                                               angle ω between                                             
                                                                      ∥x∥ ∥y∥                                                                                                                           
interval [−1, 1].                                                                                                                           two vectors x, y is                                         
                                                                      ⩽ 1 . (3.24)                                                                                                                      
0 π/2 π                                                                                                                                     computed using the                                          
                                                                      Therefore,  there exists a unique ω ∈ [0, π], illustrated in                                                                      
ω                                                                     Figure 3.2, with                                                      inner product.                                              
                                                                                                                                                                                                        
−1                                                                    cos ω =                                                               y                                                           
                                                                                                                                                                                                        
0                                                                     ⟨x, y⟩                                                                x                                                           
                                                                                                                                                                                                        
1                                                                     ∥x∥ ∥y∥                                                               0 1                                                         
                                                                                                                                                                                                        
cos(                                                                  . (3.25)                                                              1                                                           
                                                                                                                                                                                                        
ω                                                                     angle The number ω is the angle between the vectors x and y.          ω                                                           
                                                                      Intuitively, the                                                                                                                  
)                                                                                                                                           see  Figure  3.3,  where we use the dot product as the inner
                                                                      angle  between  two  vectors  tells  us  how  similar  their          product. Then                                               
In  addition  to  enabling  the  definition  of  lengths  of          orientations are. For                                                                                                             
vectors, as well as the                                                                                                                     we get                                                      
                                                                      example,  using the dot product, the angle between x and y =                                                                      
distance  between  two  vectors, inner products also capture          4x, i.e., y                                                           cos ω =                                                     
the geometry                                                                                                                                                                                            
                                                                      is  a  scaled  version  of x, is 0: Their orientation is the          ⟨x, y⟩                                                      
of  a  vector  space  by  defining  the  angle ω between two          same.                                                                                                                             
vectors. We use                                                                                                                             p                                                           
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
the  Cauchy-Schwarz  inequality (3.17) to define angles ω in          Feedback: https://mml-book.com.                                       ⟨x, x⟩ ⟨y, y⟩                                               
inner  product  spaces  between  two  vectors x, y, and this                                                                                                                                            
notion coincides with our                                             3.4 Angles and Orthogonality 77                                       =                                                           
                                                                                                                                                                                                        
intuition in R2 and R3                                                Example 3.6 (Angle between Vectors)                                   x                                                           
                                                                                                                                                                                                        
. Assume that x ̸= 0, y ̸= 0. Then                                    Let  us  compute  the angle between x = [1, 1]⊤ ∈ R2 and y =          ⊤y                                                          
                                                                      [1, 2]⊤ ∈ R2                                                                                                                      
−1 ⩽                                                                                                                                        p                                                           
                                                                      ; Figure 3.3 The                                                                                                                  

%%%

x⊤xy⊤y                                                                                                                                      product yields                                              
                                                                      Remark.  Orthogonality  is the generalization of the concept                                                                      
=                                                                     of perpendicularity to bilinear forms that do not have to be          an angle ω between x and y of 90◦                           
                                                                      the dot product. In our                                                                                                           
3                                                                                                                                           , such that x ⊥ y. However, if we                           
                                                                      context,  geometrically,  we can think of orthogonal vectors                                                                      
√                                                                     as having a                                                           choose the inner product                                    
                                                                                                                                                                                                        
10                                                                    right angle with respect to a specific inner product. ♢               ⟨x, y⟩ = x                                                  
                                                                                                                                                                                                        
, (3.26)                                                              Example 3.7 (Orthogonal Vectors)                                      ⊤                                                           
                                                                                                                                                                                                        
and the angle between the two vectors is arccos( √                    Figure 3.1 The                                                                                                                   
                                                                                                                                                                                                        
3                                                                     angle ω between                                                       2 0                                                         
                                                                                                                                                                                                        
10 ) ≈ 0.32 rad, which                                                two vectors x, y can                                                  0 1                                                        
                                                                                                                                                                                                        
corresponds to about 18◦                                              change depending                                                      y , (3.27)                                                  
                                                                                                                                                                                                        
.                                                                     on the inner                                                          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
                                                                                                                                            by Cambridge University Press (2020).                       
A key feature of the inner product is that it also allows us          product.                                                                                                                          
to characterize                                                                                                                             78 Analytic Geometry                                        
                                                                      y x                                                                                                                               
vectors that are orthogonal.                                                                                                                we get that the angle ω between x and y is given by         
                                                                      −1 0 1                                                                                                                            
Definition  3.7  (Orthogonality).  Two  vectors  x and y are                                                                                cos ω =                                                     
orthogonal if and orthogonal                                          1                                                                                                                                 
                                                                                                                                            ⟨x, y⟩                                                      
only  if ⟨x, y⟩ = 0, and we write x ⊥ y. If additionally ∥x∥          ω                                                                                                                                 
= 1 = ∥y∥,                                                                                                                                  ∥x∥∥y∥                                                      
                                                                      Consider two vectors x = [1, 1]⊤, y = [−1, 1]⊤ ∈ R2                                                                               
i.e.,  the  vectors  are  unit  vectors,  then  x  and y are                                                                                = −                                                         
orthonormal. orthonormal                                              ; see Figure 3.1.                                                                                                                 
                                                                                                                                            1                                                           
An  implication  of  this definition is that the 0-vector is          We  are  interested  in determining the angle ω between them                                                                      
orthogonal to                                                         using two                                                             3                                                           
                                                                                                                                                                                                        
every vector in the vector space.                                     different inner products. Using the dot product as the inner          =⇒ ω ≈ 1.91 rad ≈ 109.5                                     

%%%

                                                                                                                                                                                                        
◦                                                                     description would                                                     Moreover,  the  angle  between  any  two  vectors  x,  y, as
                                                                                                                                            measured by their                                           
, (3.28)                                                              be “orthonormal”.                                                                                                                 
                                                                                                                                            inner  product,  is also unchanged when transforming both of
and  x and y are not orthogonal. Therefore, vectors that are          Transformations  by  orthogonal matrices are special because          them using                                                  
orthogonal                                                            the length                                                                                                                        
                                                                                                                                            an  orthogonal  matrix  A.  Assuming  the dot product as the
with  respect  to  one  inner  product  do  not  have  to be          of  a  vector x is not changed when transforming it using an          inner product,                                              
orthogonal with respect to a different inner product.                 orthogonal                                                                                                                        
                                                                                                                                            the angle of the images Ax and Ay is given as               
Definition 3.8 (Orthogonal Matrix). A square matrix A ∈ Rn×n          matrix A. For the dot product, we obtain                                                                                          
                                                                                                                                            cos ω =                                                     
is an                                                                 Transformations                                                                                                                   
                                                                                                                                            (Ax)                                                        
orthogonal  matrix  orthogonal  matrix  if  and  only if its          with orthogonal                                                                                                                   
columns are orthonormal so that                                                                                                             ⊤(Ay)                                                       
                                                                      matrices preserve                                                                                                                 
AA⊤ = I = A                                                                                                                                 ∥Ax∥ ∥Ay∥                                                   
                                                                      distances and                                                                                                                     
⊤A , (3.29)                                                                                                                                 =                                                           
                                                                      angles.                                                                                                                           
which implies that                                                                                                                          x                                                           
                                                                      ∥Ax∥                                                                                                                              
A                                                                                                                                           ⊤A                                                          
                                                                      2 = (Ax)                                                                                                                          
−1 = A                                                                                                                                      ⊤Ay                                                         
                                                                      ⊤(Ax) = x                                                                                                                         
⊤                                                                                                                                           q                                                           
                                                                      ⊤A                                                                                                                                
, (3.30)                                                                                                                                    x⊤A                                                         
                                                                      ⊤Ax = x                                                                                                                           
It  is convention to i.e., the inverse is obtained by simply                                                                                ⊤Axy⊤A                                                      
transposing the matrix.                                               ⊤Ix = x                                                                                                                           
                                                                                                                                            ⊤Ay                                                         
call these matrices                                                   ⊤x = ∥x∥                                                                                                                          
                                                                                                                                            =                                                           
“orthogonal” but a                                                    2                                                                                                                                 
                                                                                                                                            x                                                           
more precise                                                          . (3.31)                                                                                                                          

%%%

⊤y                                                                                                                                                                                                      
                                                                      vector  is  1.  We  will call this basis then an orthonormal          ˜b1, . . . ,                                                
∥x∥ ∥y∥                                                               basis.                                                                                                                            
                                                                                                                                            ˜bn} of non-orthogonal and unnormalized basis vectors. We   
, (3.32)                                                              Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
                                                                      Feedback: https://mml-book.com.                                       concatenate them into a matrix B˜ = [˜b1, . . . ,           
which  gives  exactly  the angle between x and y. This means                                                                                                                                            
that orthogonal matrices A with A                                     3.6 Orthogonal Complement 79                                          ˜bn]  and apply Gaussian elimination to the augmented matrix
                                                                                                                                            (Section 2.3.2) [B˜ B˜                                      
⊤ = A                                                                 Let us introduce this more formally.                                                                                              
                                                                                                                                            ⊤                                                           
−1                                                                    Definition    3.9    (Orthonormal    Basis).   Consider   an                                                                      
                                                                      n-dimensional vector                                                  |B˜ ] to obtain an                                          
preserve both angles and distances. It                                                                                                                                                                  
                                                                      space V and a basis {b1, . . . , bn} of V . If                        orthonormal  basis.  This  constructive  way  to iteratively
turns  out  that  orthogonal matrices define transformations                                                                                build  an  orthonormal  basis {b1, . . . , bn} is called the
that  are  rotations  (with  the  possibility  of flips). In          ⟨bi                                                                   Gram-Schmidt process (Strang, 2003).                        
Section 3.9, we will discuss more                                                                                                                                                                       
                                                                      , bj ⟩ = 0 for i ̸= j (3.33)                                          Example 3.8 (Orthonormal Basis)                             
details about rotations.                                                                                                                                                                                
                                                                      ⟨bi                                                                   The canonical/standard basis for a Euclidean vector space Rn
3.5 Orthonormal Basis                                                                                                                                                                                   
                                                                      , bi⟩ = 1 (3.34)                                                      is  an orthonormal basis, where the inner product is the dot
In  Section  2.6.1,  we  characterized  properties  of basis                                                                                product of vectors.                                         
vectors and found                                                     for  all  i,  j  =  1, . . . , n then the basis is called an                                                                      
                                                                      orthonormal basis (ONB). orthonormal basis                            In R2                                                       
that  in  an  n-dimensional  vector  space,  we need n basis                                                                                                                                            
vectors, i.e., n                                                      ONB If only (3.33) is satisfied, then the basis is called an          , the vectors                                               
                                                                      orthogonal basis. Note                                                                                                            
vectors  that  are linearly independent. In Sections 3.3 and                                                                                b1 =                                                        
3.4, we used                                                          orthogonal basis that (3.34) implies that every basis vector                                                                      
                                                                      has length/norm 1.                                                    1                                                           
inner  products  to  compute  the  length of vectors and the                                                                                                                                            
angle between                                                         Recall   from   Section  2.6.1  that  we  can  use  Gaussian          √                                                           
                                                                      elimination to find a                                                                                                             
vectors.  In the following, we will discuss the special case                                                                                2                                                           
where the basis                                                       basis for a vector space spanned by a set of vectors. Assume                                                                      
                                                                      we are given                                                                                                                     
vectors are orthogonal to each other and where the length of                                                                                                                                            
each basis                                                            a set {                                                               1                                                           

%%%

                                                                      orthogonal  to  each other. This will play an important role                                                                      
1                                                                     in Chapter 10,                                                        e3                                                          
                                                                                                                                                                                                        
                                                                     when  we  discuss  linear  dimensionality  reduction  from a          e1                                                          
                                                                      geometric perspective.                                                                                                            
, b2 =                                                                                                                                      e2                                                          
                                                                      Consider a D-dimensional vector space V and an M-dimensional                                                                      
1                                                                     subspace U ⊆ V . Then its orthogonal complement U                     w                                                           
                                                                                                                                                                                                        
√                                                                     ⊥ is a (D−M)-dimensional orthogonal                                   U                                                           
                                                                                                                                                                                                        
2                                                                     complement  subspace of V and contains all vectors in V that          uniquely decomposed into                                    
                                                                      are orthogonal to every                                                                                                           
                                                                                                                                           x =                                                         
                                                                      vector in U. Furthermore, U ∩ U                                                                                                   
1                                                                                                                                           X                                                           
                                                                      ⊥ = {0} so that any vector x ∈ V can be                                                                                           
−1                                                                                                                                          M                                                           
                                                                      ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
                                                                     by Cambridge University Press (2020).                                 m=1                                                         
                                                                                                                                                                                                        
(3.35)                                                                80 Analytic Geometry                                                  λmbm +                                                      
                                                                                                                                                                                                        
form an orthonormal basis since b                                     Figure 3.1 A plane                                                    D                                                           
                                                                                                                                                                                                        
⊤                                                                     U in a                                                                X−M                                                         
                                                                                                                                                                                                        
1 b2 = 0 and ∥b1∥ = 1 = ∥b2∥.                                         three-dimensional                                                     j=1                                                         
                                                                                                                                                                                                        
We  will  exploit  the  concept  of  an orthonormal basis in          vector space can be                                                   ψjb                                                         
Chapter 12 and                                                                                                                                                                                          
                                                                      described by its                                                      ⊥                                                           
Chapter  10  when  we  discuss  support  vector machines and                                                                                                                                            
principal component analysis.                                         normal vector,                                                        j                                                           
                                                                                                                                                                                                        
3.6 Orthogonal Complement                                             which spans its                                                       , λm, ψj ∈ R , (3.36)                                       
                                                                                                                                                                                                        
Having  defined  orthogonality,  we  will now look at vector          orthogonal                                                            where (b1, . . . , bM) is a basis of U and (b               
spaces that are                                                                                                                                                                                         
                                                                      complement U⊥.                                                        ⊥                                                           

%%%

                                                                      angles  and  distances.  We  focused  on  inner  products of          Feedback: https://mml-book.com.                             
1                                                                     finite-dimensional                                                                                                                
                                                                                                                                            3.8 Orthogonal Projections 81                               
, . . . , b                                                           vectors.  In  the  following,  we will look at an example of                                                                      
                                                                      inner products of                                                     for  lower  and upper limits a, b < ∞, respectively. As with
⊥                                                                                                                                           our usual inner                                             
                                                                      a different type of vectors: inner products of functions.                                                                         
D−M) is a basis of U                                                                                                                        product, we can define norms and orthogonality by looking at
                                                                      The  inner  products  we  discussed  so far were defined for          the inner                                                   
⊥.                                                                    vectors with a                                                                                                                    
                                                                                                                                            product. If (3.37) evaluates to 0, the functions u and v are
Therefore,  the  orthogonal  complement  can also be used to          finite number of entries. We can think of a vector x ∈ Rn as          orthogonal. To                                              
describe a                                                            a function                                                                                                                        
                                                                                                                                            make  the preceding inner product mathematically precise, we
plane  U  (two-dimensional  subspace) in a three-dimensional          with  n function values. The concept of an inner product can          need to take                                                
vector space.                                                         be generalized                                                                                                                    
                                                                                                                                            care of measures and the definition of integrals, leading to
More  specifically,  the  vector  w  with  ∥w∥ = 1, which is          to  vectors  with  an  infinite number of entries (countably          the definition of                                           
orthogonal to the                                                     infinite) and also                                                                                                                
                                                                                                                                            a  Hilbert  space.  Furthermore,  unlike  inner  products on
plane U, is the basis vector of U                                     continuous-valued functions (uncountably infinite). Then the          finite-dimensional                                          
                                                                      sum over                                                                                                                          
⊥. Figure 3.1 illustrates this setting. All                                                                                                 vectors,  inner  products  on  functions  may  diverge (have
                                                                      individual  components  of  vectors  (see Equation (3.5) for          infinite value). All                                        
vectors  that are orthogonal to w must (by construction) lie          example) turns                                                                                                                    
in the plane                                                                                                                                this  requires  diving  into  some more intricate details of
                                                                      into an integral.                                                     real and functional                                         
normal vector U. The vector w is called the normal vector of                                                                                                                                            
U.                                                                    An  inner  product  of two functions u : R → R and v : R → R          analysis, which we do not cover in this book.               
                                                                      can be                                                                                                                            
Generally,  orthogonal  complements  can be used to describe                                                                                Example 3.9 (Inner Product of Functions)                    
hyperplanes                                                           defined as the definite integral                                                                                                  
                                                                                                                                            If we choose u = sin(x) and v = cos(x), the integrand f(x) =
in n-dimensional vector and affine spaces.                            ⟨u, v⟩ := Z b                                                         u(x)v(x) Figure 3.2 f(x) =                                  
                                                                                                                                                                                                        
3.7 Inner Product of Functions                                        a                                                                     sin(x) cos(x).                                              
                                                                                                                                                                                                        
Thus  far,  we  looked  at  properties  of inner products to          u(x)v(x)dx (3.37)                                                     −2.5 0.0 2.5                                                
compute lengths,                                                                                                                                                                                        
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.          x                                                           

%%%

                                                                      Fourier series. ♢                                                     will discuss                                                
−0.5                                                                                                                                                                                                    
                                                                      In  Section  6.4.6,  we will have a look at a second type of          some  of  the  fundamental  tools for data compression. More
0.0                                                                   unconventional                                                        specifically, we                                            
                                                                                                                                                                                                        
0.5                                                                   inner products: the inner product of random variables.                can  project  the  original  high-dimensional  data  onto  a
                                                                                                                                            lower-dimensional                                           
sin(                                                                  3.8 Orthogonal Projections                                                                                                        
                                                                                                                                            feature  space  and  work in this lower-dimensional space to
x) cos(                                                               Projections are an important class of linear transformations          learn more                                                  
                                                                      (besides  rotations  and  reflections) and play an important                                                                      
x                                                                     role  in  graphics,  coding  theory,  statistics and machine          about   the  dataset  and  extract  relevant  patterns.  For
                                                                      learning. In machine learning, we often deal                          example, machine                                            
)                                                                                                                                                                                                       
                                                                      with data that is high-dimensional. High-dimensional data is          ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published
of (3.37), is shown in Figure 3.2. We see that this function          often hard                                                            by Cambridge University Press (2020).                       
is odd, i.e.,                                                                                                                                                                                           
                                                                      to  analyze  or  visualize.  However,  high-dimensional data          82 Analytic Geometry                                        
f(−x) = −f(x). Therefore, the integral with limits a = −π, b          quite   often   possesses  the  property  that  only  a  few                                                                      
= π of this                                                           dimensions contain most information,                                  Figure 3.1                                                  
                                                                                                                                                                                                        
product   evaluates   to  0.  Therefore,  sin  and  cos  are          and  most other dimensions are not essential to describe key          Orthogonal                                                  
orthogonal functions.                                                 properties                                                                                                                        
                                                                                                                                            projection (orange                                          
Remark. It also holds that the collection of functions                of  the data. When we compress or visualize high-dimensional                                                                      
                                                                      data, we                                                              dots) of a                                                  
{1, cos(x), cos(2x), cos(3x), . . . } (3.38)                                                                                                                                                            
                                                                      will lose information. To minimize this compression loss, we          two-dimensional                                             
is  orthogonal  if we integrate from −π to π, i.e., any pair          ideally find                                                                                                                      
of functions are                                                                                                                            dataset (blue dots)                                         
                                                                      the most informative dimensions in the data. As discussed in                                                                      
orthogonal  to  each  other.  The collection of functions in          Chapter 1, “Feature” is a                                             onto a                                                      
(3.38) spans a                                                                                                                                                                                          
                                                                      common expression                                                     one-dimensional                                             
large  subspace  of the functions that are even and periodic                                                                                                                                            
on [−π, π), and                                                       for data                                                              subspace (straight                                          
                                                                                                                                                                                                        
projecting  functions  onto this subspace is the fundamental          representation.                                                       line).                                                      
idea behind                                                                                                                                                                                             
                                                                      data  can be represented as vectors, and in this chapter, we          −4 −2 0 2 4                                                 

%%%

                                                                      error  between  the  original  data  and  the  corresponding          dimensional  subspaces,  which are also called lines. If not
x1                                                                    projection. An illustration of such an orthogonal projection          mentioned otherwise, we assume the dot product ⟨x, y⟩ = x   
                                                                      is given in Figure 3.1. Before                                                                                                    
−2                                                                                                                                          ⊤y as the inner product.                                    
                                                                      we  detail  how  to  obtain these projections, let us define                                                                      
−1                                                                    what a projection                                                     3.8.1 Projection onto One-Dimensional Subspaces (Lines)     
                                                                                                                                                                                                        
0                                                                     actually is.                                                          Assume  we  are  given  a  line  (one-dimensional  subspace)
                                                                                                                                            through the origin with basis vector b ∈ Rn                 
1                                                                     Definition  3.10 (Projection). Let V be a vector space and U                                                                      
                                                                      ⊆ V a                                                                 . The line is a one-dimensional subspace                    
2                                                                                                                                                                                                       
                                                                      projection  subspace  of  V  . A linear mapping π : V → U is          U ⊆ Rn                                                      
x                                                                     called a projection if                                                                                                            
                                                                                                                                            spanned by b. When we project x ∈ Rn onto U, we seek the    
2                                                                     π                                                                                                                                 
                                                                                                                                            vector  πU  (x)  ∈  U  that is closest to x. Using geometric
learning  algorithms,  such  as principal component analysis          2 = π ◦ π = π.                                                        arguments, let us                                           
(PCA) by Pearson (1901) and Hotelling (1933) and deep neural                                                                                                                                            
networks (e.g., deep                                                  Since  linear  mappings  can  be expressed by transformation          Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
                                                                      matrices (see                                                         Feedback: https://mml-book.com.                             
auto-encoders (Deng et al., 2010)), heavily exploit the idea                                                                                                                                            
of dimensionality reduction. In the following, we will focus          Section  2.7), the preceding definition applies equally to a          3.8 Orthogonal Projections 83                               
on orthogonal projections,                                            special kind                                                                                                                      
                                                                                                                                            Figure 3.2                                                  
which  we  will  use in Chapter 10 for linear dimensionality          projection matrix of transformation matrices, the projection                                                                      
reduction and                                                         matrices P π, which exhibit the                                       Examples of                                                 
                                                                                                                                                                                                        
in  Chapter  12  for classification. Even linear regression,          property that P                                                       projections onto                                            
which we discuss                                                                                                                                                                                        
                                                                      2                                                                     one-dimensional                                             
in   Chapter   9,   can   be  interpreted  using  orthogonal                                                                                                                                            
projections. For a given                                              π = P π.                                                              subspaces.                                                  
                                                                                                                                                                                                        
lower-dimensional   subspace,   orthogonal   projections  of          In  the  following, we will derive orthogonal projections of          b                                                           
high-dimensional                                                      vectors in the                                                                                                                    
                                                                                                                                            x                                                           
data retain as much information as possible and minimize the          inner product space (Rn                                                                                                           
difference/                                                                                                                                 πU (x)                                                      
                                                                      ,⟨·,   ·⟩)  onto  subspaces.  We  will  start  with  oneline                                                                      

%%%

ω                                                                     coordinate of πU (x)                                                  ∥b∥                                                         
                                                                                                                                                                                                        
(a) Projection of x ∈ R2 onto a subspace U                            with respect to b.                                                    2                                                           
                                                                                                                                                                                                        
with basis vector b.                                                  The  projection  πU  (x) of x onto U must be an element of U          . (3.40)                                                    
                                                                      and,  therefore, a multiple of the basis vector b that spans                                                                      
ω cos ω                                                               U. Hence, πU (x) = λb,                                                In  the last step, we exploited the fact that inner products
                                                                                                                                            are symmetric. If we choose ⟨·, ·⟩ to be the dot product, we
sin ω                                                                 for some λ ∈ R.                                                       obtain                                                      
                                                                                                                                                                                                        
b                                                                     In the following three steps, we determine the coordinate λ,          λ =                                                         
                                                                      the projection                                                                                                                    
x                                                                                                                                           b                                                           
                                                                      πU  (x) ∈ U, and the projection matrix P π that maps any x ∈                                                                      
(b) Projection of a two-dimensional vector                            Rn onto U:                                                            ⊤                                                           
                                                                                                                                                                                                        
x with ∥x∥ = 1 onto a one-dimensional                                 1.  Finding  the  coordinate  λ. The orthogonality condition          x                                                           
                                                                      yields                                                                                                                            
subspace spanned by b.                                                                                                                      b                                                           
                                                                      ⟨x − πU (x), b⟩ = 0 πU (x)=λb ⇐⇒ ⟨x − λb, b⟩ = 0 . (3.39)                                                                         
characterize  some  properties  of  the  projection  πU  (x)                                                                                ⊤                                                           
(Figure 3.2(a) serves                                                 We  can now exploit the bilinearity of the inner product and                                                                      
                                                                      arrive at With a general inner                                        b                                                           
as an illustration):                                                                                                                                                                                    
                                                                      product, we get                                                       =                                                           
The  projection  πU  (x)  is  closest  to x, where “closest”                                                                                                                                            
implies that the                                                      λ = ⟨x, b⟩ if                                                         b                                                           
                                                                                                                                                                                                        
distance  ∥x−πU (x)∥ is minimal. It follows that the segment          ∥b∥ = 1.                                                              ⊤                                                           
πU (x)−x                                                                                                                                                                                                
                                                                      ⟨x, b⟩ − λ ⟨b, b⟩ = 0 ⇐⇒ λ =                                          x                                                           
from πU (x) to x is orthogonal to U, and therefore the basis                                                                                                                                            
vector b of                                                           ⟨x, b⟩                                                                ∥b∥                                                         
                                                                                                                                                                                                        
U.  The  orthogonality  condition yields ⟨πU (x) − x, b⟩ = 0          ⟨b, b⟩                                                                2                                                           
since angles                                                                                                                                                                                            
                                                                      =                                                                     . (3.41)                                                    
between vectors are defined via the inner product. λ is then                                                                                                                                            
the                                                                   ⟨b, x⟩                                                                If ∥b∥ = 1, then the coordinate λ of the projection is given
                                                                                                                                            by b                                                        

%%%

                                                                      ∥πU (x)∥ = ∥λb∥ = |λ| ∥b∥ . (3.43)                                    Here,  ω  is the angle between x and b. This equation should
⊤                                                                                                                                           be familiar                                                 
                                                                      Hence,  our  projection is of length |λ| times the length of                                                                      
x.                                                                    b. This also                                                          from  trigonometry:  If  ∥x∥  =  1,  then x lies on the unit
                                                                                                                                            circle. It follows                                          
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          adds  the  intuition that λ is the coordinate of πU (x) with                                                                      
by Cambridge University Press (2020).                                 respect to the                                                        The  horizontal axis that the projection onto the horizontal
                                                                                                                                            axis spanned by b is exactly                                
84 Analytic Geometry                                                  basis vector b that spans our one-dimensional subspace U.                                                                         
                                                                                                                                            is a one-dimensional                                        
2.  Finding  the projection point πU (x) ∈ U. Since πU (x) =          If we use the dot product as an inner product, we get                                                                             
λb, we immediately obtain with (3.40) that                                                                                                  subspace.                                                   
                                                                      ∥πU (x)∥                                                                                                                          
πU (x) = λb =                                                                                                                               cos  ω,  and the length of the corresponding vector πU (x) =
                                                                      (3.42) =                                                              |cos ω|. An                                                 
⟨x, b⟩                                                                                                                                                                                                  
                                                                      |b                                                                    illustration is given in Figure 3.2(b).                     
∥b∥                                                                                                                                                                                                     
                                                                      ⊤                                                                     3.  Finding  the  projection  matrix  P  π.  We  know that a
2                                                                                                                                           projection  is  a  linear  mapping  (see  Definition  3.10).
                                                                      x|                                                                    Therefore, there exists a projection                        
b =                                                                                                                                                                                                     
                                                                      ∥b∥                                                                   matrix P π, such that πU (x) = P πx. With the dot product as
b                                                                                                                                           inner                                                       
                                                                      2                                                                                                                                 
⊤                                                                                                                                           product and                                                 
                                                                      ∥b∥                                                                                                                               
x                                                                                                                                           πU (x) = λb = bλ = b                                        
                                                                      (3.25) = | cos ω| ∥x∥ ∥b∥                                                                                                         
∥b∥                                                                                                                                         b                                                           
                                                                      ∥b∥                                                                                                                               
2                                                                                                                                           ⊤                                                           
                                                                      ∥b∥                                                                                                                               
b , (3.42)                                                                                                                                  x                                                           
                                                                      2                                                                                                                                 
where  the  last equality holds for the dot product only. We                                                                                ∥b∥                                                         
can also                                                              = | cos ω| ∥x∥ .                                                                                                                  
                                                                                                                                            2                                                           
compute the length of πU (x) by means of Definition 3.1 as            (3.44)                                                                                                                            
                                                                                                                                            =                                                           

%%%

                                                                      Remark. The projection πU (x) ∈ Rn                                                                                                
bb⊤                                                                                                                                         orthogonal to both                                          
                                                                      is still an n-dimensional vector and                                                                                              
∥b∥                                                                                                                                         b1 and b2.                                                  
                                                                      not a scalar. However, we no longer require n coordinates to                                                                      
2                                                                     represent the                                                         0                                                           
                                                                                                                                                                                                        
x , (3.45)                                                            projection,  but  only a single one if we want to express it          x                                                           
                                                                      with respect to                                                                                                                   
we immediately see that                                                                                                                     b1                                                          
                                                                      the basis vector b that spans the subspace U: λ. ♢                                                                                
P π =                                                                                                                                       b2                                                          
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
bb⊤                                                                   Feedback: https://mml-book.com.                                       U                                                           
                                                                                                                                                                                                        
∥b∥                                                                   3.8 Orthogonal Projections 85                                         πU (x)                                                      
                                                                                                                                                                                                        
2                                                                     Figure 3.1                                                            x − πU (x)                                                  
                                                                                                                                                                                                        
. (3.46)                                                              Projection onto a                                                     Example 3.10 (Projection onto a Line)                       
                                                                                                                                                                                                        
Note that bb⊤                                                         two-dimensional                                                       Find  the  projection  matrix  P π onto the line through the
                                                                                                                                            origin spanned                                              
Projection  matrices (and, consequently, P π) is a symmetric          subspace U with                                                                                                                   
matrix (of rank                                                                                                                             by b =                                                      
                                                                      basis b1, b2. The                                                                                                                 
are always                                                                                                                                  1 2 2⊤                                                      
                                                                      projection πU (x) of                                                                                                              
symmetric.                                                                                                                                  . b is a direction and a basis of the one-dimensional       
                                                                      x ∈ R3 onto U can                                                                                                                 
1), and ∥b∥                                                                                                                                 subspace (line through origin).                             
                                                                      be expressed as a                                                                                                                 
2 = ⟨b, b⟩ is a scalar.                                                                                                                     With (3.46), we obtain                                      
                                                                      linear combination                                                                                                                
The  projection  matrix  P π projects any vector x ∈ Rn onto                                                                                P π =                                                       
the line through                                                      of b1, b2 and the                                                                                                                 
                                                                                                                                            bb⊤                                                         
the  origin  with  direction b (equivalently, the subspace U          displacement vector                                                                                                               
spanned by b).                                                                                                                              b                                                           
                                                                      x − πU (x) is                                                                                                                     

%%%

⊤                                                                     2 4 4                                                                                                                             
                                                                                                                                            1                                                           
b                                                                                                                                                                                                      
                                                                                                                                            1                                                           
=                                                                      . (3.47)                                                                                                                        
                                                                                                                                                                                                       
1                                                                     Let  us now choose a particular x and see whether it lies in                                                                      
                                                                      the subspace                                                           =                                                         
9                                                                                                                                                                                                       
                                                                      spanned by b. For x =                                                 1                                                           
                                                                                                                                                                                                       
                                                                      1 1 1⊤                                                                9                                                           
                                                                                                                                                                                                       
                                                                      , the projection is                                                                                                              
1                                                                                                                                                                                                       
                                                                      πU (x) = P πx =                                                                                                                  
2                                                                                                                                                                                                       
                                                                      1                                                                     5                                                           
2                                                                                                                                                                                                       
                                                                      9                                                                     10                                                          
                                                                                                                                                                                                       
                                                                                                                                           10                                                          
                                                                                                                                                                                                       
                                                                                                                                                                                                      
1 2 2                                                                                                                                                                                                   
                                                                      1 2 2                                                                  ∈ span[                                                   
=                                                                                                                                                                                                       
                                                                      2 4 4                                                                                                                            
1                                                                                                                                                                                                       
                                                                      2 4 4                                                                                                                            
9                                                                                                                                                                                                       
                                                                                                                                           1                                                           
                                                                                                                                                                                                       
                                                                                                                                           2                                                           
                                                                                                                                                                                                       
                                                                                                                                           2                                                           
1 2 2                                                                                                                                                                                                   
                                                                                                                                                                                                      
2 4 4                                                                                                                                                                                                   
                                                                      1                                                                     ] . (3.48)                                                 

%%%

                                                                                                                                                                                                        
Note  that  the application of P π to πU (x) does not change          illustration is given in Figure 3.1.                                  B = [b1, . . . , bm] ∈ R                                    
anything, i.e.,                                                                                                                                                                                         
                                                                      Assume  that  (b1, . . . , bm) is an ordered basis of U. Any          n×m, λ = [λ1, . . . , λm]                                   
P  ππU  (x)  = πU (x). This is expected because according to          projection πU (x)                                                                                                                 
Definition 3.10,                                                                                                                            ⊤ ∈ R                                                       
                                                                      onto  U  is necessarily an element of U. Therefore, they can                                                                      
we know that a projection matrix P π satisfies P                      be represented                                                        m , (3.50)                                                  
                                                                                                                                                                                                        
2                                                                     ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published          is closest to x ∈ Rn                                        
                                                                      by Cambridge University Press (2020).                                                                                             
πx = P πx for all x.                                                                                                                        . As in the 1D case, “closest” means “minimum               
                                                                      86 Analytic Geometry                                                                                                              
Remark. With the results from Chapter 4, we can show that πU                                                                                distance”,  which  implies that the vector connecting πU (x)
(x) is an                                                             as  linear  combinations of the basis vectors b1, . . . , bm          ∈ U and                                                     
                                                                      of U, such that                                                                                                                   
eigenvector  of  P π, and the corresponding eigenvalue is 1.                                                                                x  ∈  Rn  must  be  orthogonal  to  all  basis vectors of U.
♢                                                                     πU (x) = Pm                                                           Therefore, we                                               
                                                                                                                                                                                                        
3.8.2 Projection onto General Subspaces                               i=1 λibi The basis vectors .                                          obtain  m  simultaneous conditions (assuming the dot product
                                                                                                                                            as the                                                      
If U is given by a set                                                form the columns of                                                                                                               
                                                                                                                                            inner product)                                              
of spanning vectors,                                                  B ∈ Rn×m, where                                                                                                                   
                                                                                                                                            ⟨b1, x − πU (x)⟩ = b                                        
which are not a                                                       B = [b1, . . . , bm].                                                                                                             
                                                                                                                                            ⊤                                                           
basis, make sure                                                      As  in the 1D case, we follow a three-step procedure to find                                                                      
                                                                      the projection πU (x) and the projection matrix P π:                  1                                                           
you determine a                                                                                                                                                                                         
                                                                      1.  Find  the  coordinates  λ1, . . . , λm of the projection          (x − πU (x)) = 0 (3.51)                                     
basis b1, . . . , bm                                                  (with respect to the                                                                                                              
                                                                                                                                            .                                                           
before proceeding.                                                    basis of U), such that the linear combination                                                                                     
                                                                                                                                            .                                                           
In  the  following,  we  look  at  orthogonal projections of          πU (x) = Xm                                                                                                                       
vectors x ∈ Rn                                                                                                                              .                                                           
                                                                      i=1                                                                                                                               
onto lower-dimensional subspaces U ⊆ Rn with dim(U) = m ⩾ 1.                                                                                ⟨bm, x − πU (x)⟩ = b                                        
An                                                                    λibi = Bλ , (3.49)                                                                                                                

%%%

⊤                                                                     .                                                                                                                                 
                                                                                                                                            basis of U and, therefore, linearly independent, B          
m(x − πU (x)) = 0 (3.52)                                              .                                                                                                                                 
                                                                                                                                            ⊤B  ∈ Rm×m is regular and can be inverted. This allows us to
which, with πU (x) = Bλ, can be written as                            .                                                                     solve for the coefficients/                                 
                                                                                                                                                                                                        
b                                                                     b                                                                     coordinates                                                 
                                                                                                                                                                                                        
⊤                                                                     ⊤                                                                     λ = (B                                                      
                                                                                                                                                                                                        
1                                                                     m                                                                     ⊤B)                                                         
                                                                                                                                                                                                        
(x − Bλ) = 0 (3.53)                                                                                                                        −1B                                                         
                                                                                                                                                                                                        
.                                                                                                                                          ⊤                                                           
                                                                                                                                                                                                        
.                                                                                                                                          x . (3.57)                                                  
                                                                                                                                                                                                        
.                                                                                                                                          The matrix (B                                               
                                                                                                                                                                                                        
b                                                                     x − Bλ                                                               ⊤B)                                                         
                                                                                                                                                                                                        
⊤                                                                                                                                          −1B                                                         
                                                                                                                                                                                                        
m(x − Bλ) = 0 (3.54)                                                   = 0 ⇐⇒ B                                                            ⊤                                                           
                                                                                                                                                                                                        
such that we obtain a homogeneous linear equation system              ⊤                                                                     pseudo-inverse is also called the pseudo-inverse of B, which
                                                                                                                                                                                                        
                                                                     (x − Bλ) = 0 (3.55)                                                   can  be computed for non-square matrices B. It only requires
                                                                                                                                            that B                                                      
                                                                     ⇐⇒ B                                                                                                                              
                                                                                                                                            ⊤B                                                          
                                                                     ⊤Bλ = B                                                                                                                           
                                                                                                                                            is  positive  definite, which is the case if B is full rank.
b                                                                     ⊤                                                                     In  practical  applications  (e.g.,  linear  regression), we
                                                                                                                                            often add a “jitter term” ϵI to                             
⊤                                                                     x . (3.56)                                                                                                                        
                                                                                                                                            Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
1                                                                     normal   equation  The  last  expression  is  called  normal          Feedback: https://mml-book.com.                             
                                                                      equation. Since b1, . . . , bm are a                                                                                              

%%%

3.8 Orthogonal Projections 87                                         Remark.  The  solution for projecting onto general subspaces                                                                      
                                                                      includes the                                                           ,                                                         
B                                                                                                                                                                                                       
                                                                      1D case as a special case: If dim(U) = 1, then B                                                                                 
⊤B  to  guarantee increased numerical stability and positive                                                                                                                                            
definiteness.  This  “ridge” can be rigorously derived using          ⊤B ∈ R is a scalar and                                                                                                           
Bayesian inference.                                                                                                                                                                                     
                                                                      we can rewrite the projection matrix in (3.59) P π = B(B              0                                                           
See Chapter 9 for details.                                                                                                                                                                              
                                                                      ⊤B)                                                                   1                                                           
2.  Find  the  projection πU (x) ∈ U. We already established                                                                                                                                            
that πU (x) =                                                         −1B                                                                   2                                                           
                                                                                                                                                                                                        
Bλ. Therefore, with (3.57)                                            ⊤                                                                                                                                
                                                                                                                                                                                                        
πU (x) = B(B                                                          as                                                                    ] ⊆ R3 and x =                                             
                                                                                                                                                                                                        
⊤B)                                                                   P π =                                                                                                                            
                                                                                                                                                                                                        
−1B                                                                   BB⊤                                                                                                                              
                                                                                                                                                                                                        
⊤                                                                     B⊤B                                                                   6                                                           
                                                                                                                                                                                                        
x . (3.58)                                                            , which is exactly the projection matrix in (3.46). ♢                 0                                                           
                                                                                                                                                                                                        
3.  Find  the  projection  matrix  P  π. From (3.58), we can          Example 3.11 (Projection onto a Two-dimensional Subspace)             0                                                           
immediately see                                                                                                                                                                                         
                                                                      For a subspace U = span[                                                                                                         
that the projection matrix that solves P πx = πU (x) must be                                                                                                                                            
                                                                                                                                            ∈ R3 find the                                             
P π = B(B                                                                                                                                                                                               
                                                                                                                                           coordinates  λ  of  x  in  terms  of  the  subspace  U,  the
⊤B)                                                                                                                                         projection point πU (x)                                     
                                                                      1                                                                                                                                 
−1B                                                                                                                                         and the projection matrix P π.                              
                                                                      1                                                                                                                                 
⊤                                                                                                                                           First,  we  see  that  the  generating  set  of U is a basis
                                                                      1                                                                     (linear  independence) and write the basis vectors of U into
. (3.59)                                                                                                                                    a matrix B =                                                
                                                                                                                                                                                                       

%%%

                                                                     1 2                                                                   6                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1 0                                                                    =                                                                                                                              
                                                                                                                                                                                                        
1 1                                                                                                                                        .                                                           
                                                                                                                                                                                                        
1 2                                                                   3 3                                                                   (3.60)                                                      
                                                                                                                                                                                                        
                                                                     3 5                                                                  Third, we solve the normal equation B                       
                                                                                                                                                                                                        
.                                                                    , B                                                                   ⊤Bλ = B                                                     
                                                                                                                                                                                                        
Second, we compute the matrix B                                       ⊤                                                                     ⊤                                                           
                                                                                                                                                                                                        
⊤B and the vector B                                                   x =                                                                   x to find λ:                                                
                                                                                                                                                                                                        
⊤                                                                                                                                                                                                     
                                                                                                                                                                                                        
x as                                                                  1 1 1                                                                 3 3                                                         
                                                                                                                                                                                                        
B                                                                     0 1 2                                                                3 5 λ1                                                    
                                                                                                                                                                                                        
⊤B =                                                                                                                                       λ2                                                          
                                                                                                                                                                                                        
                                                                                                                                                                                                     
                                                                                                                                                                                                        
1 1 1                                                                 6                                                                     =                                                           
                                                                                                                                                                                                        
0 1 2                                                                0                                                                                                                                
                                                                                                                                                                                                        
                                                                     0                                                                     6                                                           
                                                                                                                                                                                                        
                                                                                                                                          0                                                           
                                                                                                                                                                                                        
1 0                                                                    =                                                                                                                              
                                                                                                                                                                                                        
1 1                                                                                                                                        ⇐⇒ λ =                                                      
                                                                                                                                                                                                        

%%%

                                                                     projection onto U, i.e.,                                                                                                         
                                                                                                                                                                                                        
5                                                                     is also called the                                                     . (3.64)                                                  
                                                                                                                                                                                                        
−3                                                                    reconstruction error. ∥x − πU (x)∥ =                                  To  verify  the  results,  we  can  (a)  check  whether  the
                                                                                                                                            displacement vector                                         
                                                                     1 −2 1⊤                                                                                                                           
                                                                                                                                            πU  (x) − x is orthogonal to all basis vectors of U, and (b)
. (3.61)                                                              =                                                                     verify that                                                 
                                                                                                                                                                                                        
Fourth,  the  projection  πU (x) of x onto U, i.e., into the          √                                                                     P π = P                                                     
column space of                                                                                                                                                                                         
                                                                      6 . (3.63)                                                            2                                                           
B, can be directly computed via                                                                                                                                                                         
                                                                      Fifth, the projection matrix (for any x ∈ R3                          π                                                           
πU (x) = Bλ =                                                                                                                                                                                           
                                                                      ) is given by                                                         (see Definition 3.10).                                      
                                                                                                                                                                                                       
                                                                      P π = B(B                                                             Remark.  The  projections  πU  (x)  are  still vectors in Rn
                                                                                                                                           although they lie in                                        
                                                                      ⊤B)                                                                                                                               
5                                                                                                                                           an m-dimensional subspace U ⊆ Rn                            
                                                                      −1B                                                                                                                               
2                                                                                                                                           . However, to represent a projected                         
                                                                      ⊤ =                                                                                                                               
−1                                                                                                                                          vector  we  only  need the m coordinates λ1, . . . , λm with
                                                                      1                                                                     respect to the                                              
                                                                                                                                                                                                       
                                                                      6                                                                     basis vectors b1, . . . , bm of U. ♢                        
 . (3.62)                                                                                                                                                                                              
                                                                                                                                           Remark.  In  vector  spaces  with general inner products, we
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                have to pay                                                 
by Cambridge University Press (2020).                                                                                                                                                                  
                                                                                                                                            attention  when  computing  angles  and distances, which are
88 Analytic Geometry                                                  5 2 −1                                                                defined by                                                  
                                                                                                                                                                                                        
projection  error  The corresponding projection error is the          2 2 2                                                                 means of the inner product. ♢ We can find                   
norm of the difference vector                                                                                                                                                                           
                                                                      −1 2 5                                                                approximate                                                 
The  projection  error  between  the original vector and its                                                                                                                                            

%%%

solutions to                                                                                                                                                                                            
                                                                      component analysis (Section 10.3).                                    constructively  transform  any  basis (b1, . . . , bn) of an
unsolvable linear                                                                                                                           n-dimensional vector                                        
                                                                      Remark.  We  just  looked at projections of vectors x onto a                                                                      
equation systems                                                      subspace U with                                                       space V into an orthogonal/orthonormal basis (u1, . . . ,un)
                                                                                                                                            of V . This                                                 
using projections.                                                    basis  vectors  {b1,  .  . . , bk}. If this basis is an ONB,                                                                      
                                                                      i.e., (3.33) and (3.34)                                               basis always exists (Liesen and Mehrmann, 2015) and span[b1,
Projections  allow  us to look at situations where we have a                                                                                . . . , bn] =                                               
linear system                                                         are  satisfied,  the  projection  equation (3.58) simplifies                                                                      
                                                                      greatly to                                                            span[u1,  .  .  .  ,un].  The Gram-Schmidt orthogonalization
Ax  =  b  without  a solution. Recall that this means that b                                                                                method iteratively Gram-Schmidt                             
does not lie in                                                       πU (x) = BB⊤                                                                                                                      
                                                                                                                                            orthogonalization  constructs an orthogonal basis (u1, . . .
the  span  of  A,  i.e.,  the  vector  b does not lie in the          x (3.65)                                                              ,un) from any basis (b1, . . . , bn) of                     
subspace spanned by                                                                                                                                                                                     
                                                                      since B                                                               V as follows:                                               
the  columns  of A. Given that the linear equation cannot be                                                                                                                                            
solved exactly,                                                       ⊤B = I with coordinates                                               u1 := b1 (3.67)                                             
                                                                                                                                                                                                        
we can find an approximate solution. The idea is to find the          λ = B                                                                 uk := bk − πspan[u1,...,uk−1](bk), k = 2, . . . , n . (3.68)
vector in the                                                                                                                                                                                           
                                                                      ⊤                                                                     In  (3.68),  the  kth  basis vector bk is projected onto the
subspace  spanned  by the columns of A that is closest to b,                                                                                subspace spanned                                            
i.e., we compute                                                      x . (3.66)                                                                                                                        
                                                                                                                                            by  the first k − 1 constructed orthogonal vectors u1, . . .
the  orthogonal projection of b onto the subspace spanned by          This  means  that  we  no longer have to compute the inverse          ,uk−1; see Section 3.8.2. This projection is then subtracted
the columns                                                           from (3.58),                                                          from bk and yields a vector                                 
                                                                                                                                                                                                        
of  A.  This  problem  arises  often  in  practice,  and the          which saves computation time. ♢                                       uk  that  is  orthogonal to the (k − 1)-dimensional subspace
solution is called the                                                                                                                      spanned by                                                  
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
least-squares   least-squares  solution  (assuming  the  dot          Feedback: https://mml-book.com.                                       u1,  .  .  . ,uk−1. Repeating this procedure for all n basis
product as the inner product) of                                                                                                            vectors b1, . . . , bn                                      
                                                                      3.8 Orthogonal Projections 89                                                                                                     
solution an overdetermined system. This is discussed further                                                                                yields  an  orthogonal  basis  (u1, . . . , un) of V . If we
in Section 9.4. Using                                                 3.8.3 Gram-Schmidt Orthogonalization                                  normalize the uk, we                                        
                                                                                                                                                                                                        
reconstruction  errors  (3.63)  is  one possible approach to          Projections  are at the core of the Gram-Schmidt method that          obtain an ONB where ∥uk∥ = 1 for k = 1, . . . , n.          
derive principal                                                      allows us to                                                                                                                      

%%%

Example 3.12 (Gram-Schmidt Orthogonalization)                         u1                                                                                                                               
                                                                                                                                                                                                        
Figure 3.2                                                            b2                                                                    , b2 =                                                      
                                                                                                                                                                                                        
Gram-Schmidt                                                          0 πspan[u1](b2)                                                                                                                  
                                                                                                                                                                                                        
orthogonalization.                                                    (b) First new basis vector                                            1                                                           
                                                                                                                                                                                                        
(a) non-orthogonal                                                    u1 = b1 and projection of b2                                          1                                                           
                                                                                                                                                                                                        
basis (b1, b2) of R2                                                  onto the subspace spanned by                                                                                                     
                                                                                                                                                                                                        
;                                                                     u1.                                                                   ; (3.69)                                                    
                                                                                                                                                                                                        
(b) first constructed                                                 u1                                                                    see  also  Figure  3.2(a). Using the Gram-Schmidt method, we
                                                                                                                                            construct an                                                
basis vector u1 and                                                   b2                                                                                                                                
                                                                                                                                            orthogonal  basis (u1,u2) of R2 as follows (assuming the dot
orthogonal                                                            0 πspan[u1](b2)                                                       product as                                                  
                                                                                                                                                                                                        
projection of b2                                                      u2                                                                    the inner product):                                         
                                                                                                                                                                                                        
onto span[u1];                                                        (c) Orthogonal basis vectors u1                                       u1 := b1 =                                                  
                                                                                                                                                                                                        
(c) orthogonal basis                                                  and u2 = b2 − πspan[u1]                                                                                                          
                                                                                                                                                                                                        
(u1, u2) of R2                                                        (b2).                                                                 2                                                           
                                                                                                                                                                                                        
.                                                                     Consider a basis (b1, b2) of R2                                       0                                                           
                                                                                                                                                                                                        
b1                                                                    , where                                                                                                                          
                                                                                                                                                                                                        
b2                                                                    b1 =                                                                  , (3.70)                                                    
                                                                                                                                                                                                        
0                                                                                                                                          u2 := b2 − πspan[u1](b2)                                    
                                                                                                                                                                                                        
(a) Original non-orthogonal                                           2                                                                     (3.45) = b2 −                                               
                                                                                                                                                                                                        
basis vectors b1, b2.                                                 0                                                                     u1u                                                         
                                                                                                                                                                                                        

%%%

⊤                                                                     .                                                                                                                                 
                                                                                                                                            x0                                                          
1                                                                     (3.71)                                                                                                                            
                                                                                                                                            x                                                           
∥u1∥                                                                  ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
                                                                      by Cambridge University Press (2020).                                 b2                                                          
2                                                                                                                                                                                                       
                                                                      90 Analytic Geometry                                                  0 b1                                                        
b2 =                                                                                                                                                                                                    
                                                                      Figure 3.3                                                            (a) Setting.                                                
                                                                                                                                                                                                       
                                                                      Projection onto an                                                    0 b1                                                        
1                                                                                                                                                                                                       
                                                                      affine space.                                                         x − x0                                                      
1                                                                                                                                                                                                       
                                                                      (a) original setting;                                                 U = L − x0                                                  
                                                                                                                                                                                                       
                                                                      (b) setting shifted                                                   πU(x − x0)                                                  
−                                                                                                                                                                                                       
                                                                      by −x0 so that                                                        b2                                                          
                                                                                                                                                                                                       
                                                                      x − x0 can be                                                         (b) Reduce problem to projection πU onto vector subspace.   
1 0                                                                                                                                                                                                     
                                                                      projected onto the                                                    L                                                           
0 0 1                                                                                                                                                                                                 
                                                                      direction space U;                                                    x0                                                          
1                                                                                                                                                                                                       
                                                                      (c) projection is                                                     x                                                           
                                                                                                                                                                                                       
                                                                      translated back to                                                    b2                                                          
=                                                                                                                                                                                                       
                                                                      x0 + πU (x − x0),                                                     0 b1                                                        
                                                                                                                                                                                                       
                                                                      which gives the final                                                 πL(x)                                                       
0                                                                                                                                                                                                       
                                                                      orthogonal                                                            (c) Add support point back in                               
1                                                                                                                                                                                                       
                                                                      projection πL(x).                                                     to get affine projection πL.                                
                                                                                                                                                                                                       
                                                                      L                                                                     These  steps  are  illustrated in Figures 3.2(b) and (c). We

%%%

immediately see                                                       Figure 3.3(b).                                                        about the origin. If                                        
                                                                                                                                                                                                        
that u1 and u2 are orthogonal, i.e., u                                This  projection can now be translated back into L by adding          the rotation angle is                                       
                                                                      x0, such that                                                                                                                     
⊤                                                                                                                                           positive, we rotate                                         
                                                                      we  obtain  the orthogonal projection onto an affine space L                                                                      
1 u2 = 0.                                                             as                                                                    counterclockwise.                                           
                                                                                                                                                                                                        
3.8.4 Projection onto Affine Subspaces                                πL(x) = x0 + πU (x − x0), (3.72)                                      Original                                                    
                                                                                                                                                                                                        
Thus  far,  we  discussed  how  to  project  a vector onto a          where  πU (·) is the orthogonal projection onto the subspace          Rotated by 112.5◦                                           
lower-dimensional                                                     U, i.e., the                                                                                                                      
                                                                                                                                            Figure 3.1 The                                              
subspace  U.  In  the  following,  we  provide a solution to          direction space of L; see Figure 3.3(c).                                                                                          
projecting a vector                                                                                                                         robotic arm needs to                                        
                                                                      From  Figure  3.3, it is also evident that the distance of x                                                                      
onto an affine subspace.                                              from the affine                                                       rotate its joints in                                        
                                                                                                                                                                                                        
Consider  the  setting  in  Figure  3.3(a).  We are given an          space L is identical to the distance of x − x0 from U, i.e.,          order to pick up                                            
affine space L =                                                                                                                                                                                        
                                                                      d(x, L) = ∥x − πL(x)∥ = ∥x − (x0 + πU (x − x0))∥ (3.73a)              objects or to place                                         
x0  +  U,  where b1, b2 are basis vectors of U. To determine                                                                                                                                            
the orthogonal                                                        = d(x − x0, πU (x − x0)) = d(x − x0, U). (3.73b)                      them correctly.                                             
                                                                                                                                                                                                        
projection  πL(x) of x onto L, we transform the problem into          We  will  use  projections onto an affine subspace to derive          Figure taken                                                
a problem                                                             the concept of                                                                                                                    
                                                                                                                                            from (Deisenroth                                            
that  we  know  how  to  solve: the projection onto a vector          a separating hyperplane in Section 12.1.                                                                                          
subspace. In                                                                                                                                et al., 2015).                                              
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
order  to get there, we subtract the support point x0 from x          Feedback: https://mml-book.com.                                       3.9 Rotations                                               
and from L,                                                                                                                                                                                             
                                                                      3.9 Rotations 91                                                      Length  and angle preservation, as discussed in Section 3.4,
so  that L − x0 = U is exactly the vector subspace U. We can                                                                                are the two                                                 
now use the                                                           Figure 3.2 A                                                                                                                      
                                                                                                                                            characteristics   of   linear   mappings   with   orthogonal
orthogonal  projections  onto  a  subspace  we  discussed in          rotation rotates                                                      transformation  matrices.  In  the following, we will have a
Section 3.8.2 and                                                                                                                           closer look at specific orthogonal                          
                                                                      objects in a plane                                                                                                                
obtain  the  projection πU (x − x0), which is illustrated in                                                                                transformation matrices, which describe rotations.          

%%%

                                                                                                                                                                                                        
A  rotation  is  a  linear  mapping  (more  specifically, an          of the standard basis                                                                                                            
automorphism of rotation                                                                                                                                                                                
                                                                      in R2 by an angle θ.                                                  , e2 =                                                      
a Euclidean vector space) that rotates a plane by an angle θ                                                                                                                                            
about the                                                             e1                                                                                                                               
                                                                                                                                                                                                        
origin,  i.e.,  the  origin is a fixed point. For a positive          e2                                                                    0                                                           
angle   θ  >  0,  by  common  convention,  we  rotate  in  a                                                                                                                                            
counterclockwise direction. An example is                             θ                                                                     1                                                           
                                                                                                                                                                                                        
shown in Figure 3.2, where the transformation matrix is               θ                                                                      of R2                                                    
                                                                                                                                                                                                        
R =                                                                   Φ(e2) = [− sin θ, cos θ]                                              , which defines                                             
                                                                                                                                                                                                        
                                                                     ⊤                                                                     the standard coordinate system in R2                        
                                                                                                                                                                                                        
−0.38 −0.92                                                           Φ(e1) = [cos θ,sin θ]                                                 . We aim to rotate this coordinate                          
                                                                                                                                                                                                        
0.92 −0.38                                                           ⊤                                                                     system by an angle θ as illustrated in Figure 3.2. Note that
                                                                                                                                            the rotated                                                 
. (3.74)                                                              cos θ                                                                                                                             
                                                                                                                                            vectors are still linearly independent and, therefore, are a
Important  application  areas  of rotations include computer          sin θ                                                                 basis of R2                                                 
graphics and                                                                                                                                                                                            
                                                                      − sin θ                                                               .                                                           
robotics. For example, in robotics, it is often important to                                                                                                                                            
know how to                                                           cos θ                                                                 This means that the rotation performs a basis change.       
                                                                                                                                                                                                        
rotate  the  joints  of a robotic arm in order to pick up or          3.9.1 Rotations in R2                                                 Rotations  Φ are linear mappings so that we can express them
place an object,                                                                                                                            by a                                                        
                                                                      Consider the standard basis                                                                                                      
see Figure 3.1.                                                                                                                             rotation  matrix  rotation  matrix  R(θ).  Trigonometry (see
                                                                      e1 =                                                                  Figure  3.2)  allows  us to determine the coordinates of the
©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                                rotated axes (the image of Φ) with respect to               
by Cambridge University Press (2020).                                                                                                                                                                  
                                                                                                                                            the standard basis in R2                                    
92 Analytic Geometry                                                  1                                                                                                                                 
                                                                                                                                            . We obtain                                                 
Figure 3.2 Rotation                                                   0                                                                                                                                 

%%%

Φ(e1) =                                                                                                                                                                                                
                                                                      In contrast to the R2                                                 of a vector (gray) in                                       
cos θ                                                                                                                                                                                                   
                                                                      case, in R3 we can rotate any two-dimensional plane                   R3 by an angle θ                                            
sin θ                                                                                                                                                                                                   
                                                                      about a one-dimensional axis. The easiest way to specify the          about the e3-axis.                                          
                                                                     general  rotation matrix is to specify how the images of the                                                                      
                                                                      standard basis e1, e2, e3 are                                         The rotated vector is                                       
, Φ(e2) =                                                                                                                                                                                              
                                                                      supposed  to  be  rotated, and making sure these images Re1,          shown in blue.                                              
− sin θ                                                               Re2, Re3 are                                                                                                                      
                                                                                                                                            e1                                                          
cos θ                                                                 orthonormal  to  each  other.  We  can then obtain a general                                                                      
                                                                      rotation matrix                                                       e2                                                          
                                                                                                                                                                                                       
                                                                      R by combining the images of the standard basis.                      e3                                                          
. (3.75)                                                                                                                                                                                                
                                                                      To  have a meaningful rotation angle, we have to define what          θ                                                           
Therefore,  the  rotation  matrix  that  performs  the basis          “counterclockwise”  means  when  we operate in more than two                                                                      
change into the                                                       dimensions. We                                                        Rotation about the e1-axis                                  
                                                                                                                                                                                                        
rotated coordinates R(θ) is given as                                  use   the  convention  that  a  “counterclockwise”  (planar)          R1(θ) =                                                     
                                                                      rotation about an                                                                                                                 
R(θ) =                                                                                                                                      Φ(e1) Φ(e2) Φ(e3)                                           
                                                                      axis  refers to a rotation about an axis when we look at the                                                                      
Φ(e1) Φ(e2)                                                           axis “head on,                                                        =                                                           
                                                                                                                                                                                                        
=                                                                     from the end toward the origin”. In R3                                                                                           
                                                                                                                                                                                                        
                                                                     , there are therefore three (planar)                                                                                             
                                                                                                                                                                                                        
cos θ − sin θ                                                         rotations about the three standard basis vectors (see Figure          1 0 0                                                       
                                                                      3.2):                                                                                                                             
sin θ cos θ                                                                                                                                 0 cos θ − sin θ                                             
                                                                      Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.                                                                      
                                                                     Feedback: https://mml-book.com.                                       0 sin θ cos θ                                               
                                                                                                                                                                                                        
. (3.76)                                                              3.9 Rotations 93                                                                                                                 
                                                                                                                                                                                                        
3.9.2 Rotations in R3                                                 Figure 3.2 Rotation                                                    . (3.77)                                                  

%%%

                                                                                                                                                                                                        
Here,  the  e1 coordinate is fixed, and the counterclockwise          0 0 1                                                                                                                            
rotation is                                                                                                                                                                                             
                                                                                                                                                                                                      
performed in the e2e3 plane.                                                                                                                                                                            
                                                                       . (3.79)                                                                                                                       
Rotation about the e2-axis                                                                                                                                                                              
                                                                      Figure 3.2 illustrates this.                                                                                                     
R2(θ) =                                                                                                                                                                                                 
                                                                      3.9.3 Rotations in n Dimensions                                       Ii−1 0 · · · · · · 0                                        
                                                                                                                                                                                                       
                                                                      The   generalization   of   rotations  from  2D  and  3D  to          0 cos θ 0 − sin θ 0                                         
                                                                     n-dimensional  Euclidean  vector  spaces  can be intuitively                                                                      
                                                                      described  as  fixing  n  −  2  dimensions  and restrict the          0 0 Ij−i−1 0 0                                              
cos θ 0 sin θ                                                         rotation  to  a  two-dimensional  plane in the n-dimensional                                                                      
                                                                      space.  As  in the three-dimensional case, we can rotate any          0 sin θ 0 cos θ 0                                           
0 1 0                                                                 plane                                                                                                                             
                                                                                                                                            0 · · · · · · 0 In−j                                        
− sin θ 0 cos θ                                                       (two-dimensional subspace of Rn                                                                                                   
                                                                                                                                                                                                       
                                                                     ).                                                                                                                                
                                                                                                                                                                                                       
 . (3.78)                                                            Definition 3.11 (Givens Rotation). Let V be an n-dimensional                                                                      
                                                                      Euclidean                                                                                                                        
If  we  rotate  the e1e3 plane about the e2 axis, we need to                                                                                                                                            
look at the e2                                                        vector   space   and   Φ  :  V  →  V  an  automorphism  with                                                                     
                                                                      transformation ma-                                                                                                                
axis from its “tip” toward the origin.                                                                                                                                                                 
                                                                      ©2021  M.  P. Deisenroth, A. A. Faisal, C. S. Ong. Published                                                                      
Rotation about the e3-axis                                            by Cambridge University Press (2020).                                                                                            
                                                                                                                                                                                                        
R3(θ) =                                                               94 Analytic Geometry                                                  ∈ R                                                         
                                                                                                                                                                                                        
                                                                     trix                                                                  n×n                                                         
                                                                                                                                                                                                        
                                                                     Rij (θ) :=                                                            , (3.80)                                                    
                                                                                                                                                                                                        
cos θ − sin θ 0                                                                                                                            Givens rotation for 1 ⩽ i < j ⩽ n and θ ∈ R. Then Rij (θ) is
                                                                                                                                            called a Givens rotation.                                   
sin θ cos θ 0                                                                                                                                                                                          

%%%

Essentially, Rij (θ) is the identity matrix In with                                                                                                                                                     
                                                                      they rotate about the same point (e.g., the origin).                  Draft  (2023-02-15)  of  “Mathematics for Machine Learning”.
rii  =  cos  θ , rij = − sin θ , rji = sin θ , rjj = cos θ .                                                                                Feedback: https://mml-book.com.                             
(3.81)                                                                3.10 Further Reading                                                                                                              
                                                                                                                                            3.10 Further Reading 95                                     
In  two  dimensions  (i.e.,  n  =  2), we obtain (3.76) as a          In  this  chapter,  we  gave a brief overview of some of the                                                                      
special case.                                                         important concepts                                                    kernel  methods  (Scholkopf  and  Smola  ¨  ,  2002). Kernel
                                                                                                                                            methods exploit the                                         
3.9.4 Properties of Rotations                                         of analytic geometry, which we will use in later chapters of                                                                      
                                                                      the book.                                                             fact  that many linear algorithms can be expressed purely by
Rotations  exhibit  a number of useful properties, which can                                                                                inner  product computations. Then, the “kernel trick” allows
be derived by                                                         For  a  broader  and  more  in-depth overview of some of the          us to compute these                                         
                                                                      concepts we                                                                                                                       
considering them as orthogonal matrices (Definition 3.8):                                                                                   inner     products     implicitly    in    a    (potentially
                                                                      presented,  we refer to the following excellent books: Axler          infinite-dimensional) feature                               
Rotations  preserve  distances, i.e., ∥x−y∥ = ∥Rθ(x)−Rθ(y)∥.          (2015) and                                                                                                                        
In other                                                                                                                                    space,  without  even knowing this feature space explicitly.
                                                                      Boyd and Vandenberghe (2018).                                         This allowed the                                            
words,  rotations  leave the distance between any two points                                                                                                                                            
unchanged                                                             Inner  products  allow  us  to  determine  specific bases of          “non-linearization”  of  many  algorithms  used  in  machine
                                                                      vector (sub)spaces,                                                   learning, such as                                           
after the transformation.                                                                                                                                                                               
                                                                      where  each  vector  is orthogonal to all others (orthogonal          kernel-PCA  (Scholkopf  et  al. ¨ , 1997) for dimensionality
Rotations  preserve  angles, i.e., the angle between Rθx and          bases) using the                                                      reduction. Gaussian processes (Rasmussen and Williams, 2006)
Rθy equals                                                                                                                                  also fall into the category                                 
                                                                      Gram-Schmidt   method.   These   bases   are   important  in                                                                      
the angle between x and y.                                            optimization and                                                      of  kernel  methods  and are the current state of the art in
                                                                                                                                            probabilistic  regression  (fitting  curves to data points).
Rotations  in  three  (or more) dimensions are generally not          numerical  algorithms  for  solving linear equation systems.          The idea of kernels is explored                             
commutative.  Therefore,  the  order  in which rotations are          For instance,                                                                                                                     
applied is important,                                                                                                                       further in Chapter 12.                                      
                                                                      Krylov  subspace methods, such as conjugate gradients or the                                                                      
even  if  they  rotate  about  the  same  point. Only in two          generalized                                                           Projections  are  often  used in computer graphics, e.g., to
dimensions vector                                                                                                                           generate  shadows.  In  optimization, orthogonal projections
                                                                      minimal  residual  method  (GMRES), minimize residual errors          are often used to (iteratively)                             
rotations are commutative, such that R(ϕ)R(θ) = R(θ)R(ϕ) for          that  are  orthogonal  to  each  other (Stoer and Burlirsch,                                                                      
all                                                                   2002).                                                                minimize  residual  errors.  This  also  has applications in
                                                                                                                                            machine learning,                                           
ϕ,   θ   ∈  [0,  2π).  They  form  an  Abelian  group  (with          In  machine  learning,  inner  products are important in the                                                                      
multiplication) only if                                               context of                                                            e.g.,  in linear regression where we want to find a (linear)

%%%

function that                                               
                                                            
minimizes  the  residual  errors,  i.e.,  the lengths of the
orthogonal  projections of the data onto the linear function
(Bishop,  2006). We will investigate this further in Chapter
9. PCA (Pearson, 1901; Hotelling, 1933) also                
                                                            
uses   projections   to   reduce   the   dimensionality   of
high-dimensional data.                                      
                                                            
We will discuss this in more detail in Chapter 10.
